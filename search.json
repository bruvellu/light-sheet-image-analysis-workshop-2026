[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Compilation of tools and resources for image analysis focused on what we will use.\n\n\n\nhttps://zenodo.org/communities/lsmunimayor\n\n\n\n\n\nThe instructions to generate the course folder in Windows using powershell are here.\n\n\n\n\nDesktop software for image analysis.\n\nhttps://fiji.sc/\nhttps://napari.org/\n\n\n\n\nSpaces to interact and discuss about image analysis.\n\nhttps://forum.image.sc/\nhttps://imagesc.zulipchat.com/\n\n\n\n\nOther curated lists of tools and resources for scientific image analysis.\n\nhttps://github.com/epfl-center-for-imaging/awesome-scientific-image-analysis/\nhttps://github.com/hallvaaw/awesome-biological-image-analysis/\n\n\n\n\n\nTribolium z-stack https://git.mpi-cbg.de/rhaase/clij2_example_data/blob/master/lund1051_resampled.tif",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#course-material",
    "href": "resources.html#course-material",
    "title": "Resources",
    "section": "",
    "text": "https://zenodo.org/communities/lsmunimayor",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#workshops",
    "href": "resources.html#workshops",
    "title": "Resources",
    "section": "",
    "text": "The instructions to generate the course folder in Windows using powershell are here.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#general",
    "href": "resources.html#general",
    "title": "Resources",
    "section": "",
    "text": "Desktop software for image analysis.\n\nhttps://fiji.sc/\nhttps://napari.org/",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#community",
    "href": "resources.html#community",
    "title": "Resources",
    "section": "",
    "text": "Spaces to interact and discuss about image analysis.\n\nhttps://forum.image.sc/\nhttps://imagesc.zulipchat.com/",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#compilations",
    "href": "resources.html#compilations",
    "title": "Resources",
    "section": "",
    "text": "Other curated lists of tools and resources for scientific image analysis.\n\nhttps://github.com/epfl-center-for-imaging/awesome-scientific-image-analysis/\nhttps://github.com/hallvaaw/awesome-biological-image-analysis/",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#additional-datasets",
    "href": "resources.html#additional-datasets",
    "title": "Resources",
    "section": "",
    "text": "Tribolium z-stack https://git.mpi-cbg.de/rhaase/clij2_example_data/blob/master/lund1051_resampled.tif",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html",
    "href": "practicals/practical_tracking/index.html",
    "title": "Cell tracking using Mastodon",
    "section": "",
    "text": "This is a basic tutorial on how to track cells in Fiji (Schindelin et al. 2012) using Mastodon (Girstmair et al. 2025). The goals are to learn how to load and create Mastodon datasets, get familiar with navigating the BigDataViewer and TrackScheme windows, perform manual cell tracking with cell divisions and simple editing of lineages, perform semi-automated detection and tracking, and try some lineage analysis. We will use a demo Mastodon dataset (Girstmair 2024) generated with lightsheet data from the isopod Parhyale hawaiensis (Wolff et al. 2018). For more information and detailed instructions, please check Mastodon’s official documentation.",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-summary",
    "href": "practicals/practical_tracking/index.html#sec-summary",
    "title": "Cell tracking using Mastodon",
    "section": "",
    "text": "This is a basic tutorial on how to track cells in Fiji (Schindelin et al. 2012) using Mastodon (Girstmair et al. 2025). The goals are to learn how to load and create Mastodon datasets, get familiar with navigating the BigDataViewer and TrackScheme windows, perform manual cell tracking with cell divisions and simple editing of lineages, perform semi-automated detection and tracking, and try some lineage analysis. We will use a demo Mastodon dataset (Girstmair 2024) generated with lightsheet data from the isopod Parhyale hawaiensis (Wolff et al. 2018). For more information and detailed instructions, please check Mastodon’s official documentation.",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-requirements",
    "href": "practicals/practical_tracking/index.html#sec-requirements",
    "title": "Cell tracking using Mastodon",
    "section": "Requirements",
    "text": "Requirements\n\nFiji (Schindelin et al. 2012)\nMastodon plugin (Girstmair et al. 2025)\nMastodon_Auto-Tracking_Demo_Ph-limb-dev.zip dataset (Girstmair 2024)",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-setup",
    "href": "practicals/practical_tracking/index.html#sec-setup",
    "title": "Cell tracking using Mastodon",
    "section": "Setup",
    "text": "Setup\n\nDownload Dataset\n\nDownload Mastodon_Auto-Tracking_Demo_Ph-limb-dev.zip dataset from this Zenodo repository (Girstmair 2024). The direct link to the file is here (4.3GB).\nMove the ZIP file to a working directory.\nUnzip Mastodon_Auto-Tracking_Demo_Ph-limb-dev.zip.\n\n\n\n\n\n\n\nWorking directory after downloading and unzipping the dataset file.\n\n\n\n\n\n\n\nContents of the dataset directory.\n\n\n\n\n\nAfter unzipping, the contents will occupy 23GB of disk space.\n\n\nDownload Fiji\n\nGo to https://fiji.sc, choose Distribution: Stable, and click the download button.\nCopy the downloaded archive to the working directory and unzip it.\nOpen the Fiji.app directory and double-click on the launcher.\nThe main window of Fiji will open.\n\n\n\n\nInstall Mastodon\n\nClick on Help &gt; Update....\n\n\n\nThe updater will open and say Fiji is up-to-date.\nClick Manage Update Sites.\n\n\n\n\n\n\n\n\n\n\n\n\nA window will open with a list of plugins available to install in Fiji.\n\n\n\nSearch for “mastodon”.\n\n\n\nSeveral Mastodon-related plugins will appear.\nClick on the checkbox for Mastodon.\n\n\n\nClick Apply and Close and then Apply Changes.\n\n\n\nWait… until the downloads are finished. Then, click OK.\n\n\n\n\n\n\n\n\n\n\n\n\nRestart Fiji (close window and double-click the launcher).",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-open-project",
    "href": "practicals/practical_tracking/index.html#sec-open-project",
    "title": "Cell tracking using Mastodon",
    "section": "Open Mastodon Project",
    "text": "Open Mastodon Project\n\nIn Fiji click on Plugins &gt; Tracking &gt; Mastodon &gt; Mastodon Launcher.\n\n\nThe Mastodon Launcher window will open.\n\nClick on open Mastodon project (top left) and Open another project (bottom right).\n\n\n\n\n\n\n\n\n\n\n\n\nNavigate to the directory Mastodon_Auto-Tracking_Demo_Ph-limb-dev/.\nSelect the file Parhyale_LimbDev_30tps.mastodon.\n\n\nSeveral new windows will open (Console, Mastodon, BigDataViewer, TrackScheme, Data table).",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-inspect-dataset",
    "href": "practicals/practical_tracking/index.html#sec-inspect-dataset",
    "title": "Cell tracking using Mastodon",
    "section": "Inspect the Dataset",
    "text": "Inspect the Dataset\nLet’s focus on the Mastodon window.\n\nClose Console, BigDataViewer, TrackScheme, and Data table windows.\n\n\nThis is the main project menu from where we can open windows, set options, process data and save the project. The most important buttons for this tutorial are bdv (BigDataViewer) and trackscheme.\n\nClick on bdv and make the window larger.\n\n\n\nDrag the timepoint slider at the bottom to see cells moving and dividing.\nUsing our acquired BigDataViewer skills, focus on the surface of the embryo.\nIf you get lost, press Shift+Z to re-orient the embryo.\nFind a cell that divides before timepoint 15 and looks trackable and zoom on it using Ctrl+Shift+Scroll.\nThen center the view on it by holding the right button and dragging the mouse.\nUse Shift+Scroll to navigate through Z and M/N to go through time.\n\n\n\n\n\n\n\n\n\n\n\nWe are ready to track.",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-manual-tracking",
    "href": "practicals/practical_tracking/index.html#sec-manual-tracking",
    "title": "Cell tracking using Mastodon",
    "section": "Manual Tracking",
    "text": "Manual Tracking\n\nOn the Mastodon project window, click on the trackscheme button.\n\n\nThe TrackScheme window will appear.\n\nResize the BigDataViewer window to be side-by-side with the TrackScheme.\nClick on the BigDataViewer window, set the timepoint to some frames before mitosis, Shift+Scroll to find the center of the nucleus, put the mouse pointer there and press A.\n\n\nA round magenta circle will appear over the nucleus and in the TrackScheme.\n\nWith the mouse over the circle, use Shift+Q and Shift+E to adjust the size of the spot to roughly the nucleus diameter.\n\n\n\nZoom in on the new spot in the TrackScheme, hover and click on it and watch what happens in the BigDataViewer window.\n\n\n\nGo back to the BigDataViewer, hover the pointer over the circle and hold the spacebar to adjust the position of the circle and the nucleus.\n\n\nNow let’s add a second spot.\n\nHover the mouse inside the circle and hold A. This will advance to the next frame showing the first spot in white dashed line and the second spot in white solid line with a white solid link between the two.\n\n\n\nStill holding A, position the second spot, then release A to create the new linked spot.\n\n\nCheck how the second spot and a link were created in the TrackScheme automatically.\n\nContinue to track the nucleus for a few more frames, until the frame immediately before division.\n\nNote that when clicking on a spot in the BigDataViewer window, the corresponding spot is highlighted in the TrackScheme window.\n\nNow click on the spots in the TrackScheme and see what happens to the BigDataViewer (nothing will happen).\n\nLet’s change that and link the BigDataViewer and TrackScheme windows. In the menu bar of BigDataViewer and TrackScheme windows there are lock symbols 1, 2, 3.\n\nClick on Lock 1 in both windows.\n\n\n\nNow click through spots in the TrackScheme.\n\n\nThe view in the BigDataViewer will change to show the selected spot at the center.\nBefore we continue tracking the cell division, let’s check one of the amazing Mastodon features.\n\nClick on the BigDataViewer button in the Mastodon project window and another BigDataViewer window will open.\n\n\n\nNow activate Lock 1 and click on one of the TrackScheme spots.\n\n\nBoth windows will be synchronized!\nWhy is this useful for manual tracking?\n\nAdjust the view to center the spot in the second BigDataViewer window, press Shift+Y, and select a spot from the TrackScheme.\n\n\nWe can have both XY and ZY views of the same nucleus! This is great for tracking in 3D. We can check, for instance, that our spot is well centered in Z and adjust it in this window.\n\nContinue tracking one of the daughter cells.\n\nSelect the last spot in the TrackScheme, go to the XY BigDataViewer, hover the mouse over the circle and hold A, move the spot, and release A to add it.\nDo it for a few frames.\nThen go back to the pre-division spot and add a linked spot corresponding to the other daughter cell.\n\n\nThis will create the first branch of the lineage tree.\n\nContinue tracking the second daughter cell for a few frames.\n\n\n\nZoom out the TrackScheme view to see the full branched tree.",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-semiauto-tracking",
    "href": "practicals/practical_tracking/index.html#sec-semiauto-tracking",
    "title": "Cell tracking using Mastodon",
    "section": "Semi-Automated Tracking",
    "text": "Semi-Automated Tracking\nThis mode will try to guess where the next nucleus is and automatically create the spots and links.\n\nTo start, choose a different nucleus to track and press A to add a new spot.\n\n\n\nNow, hovering the pointer above the spot press Ctrl+T.\n\n\nA lineage will appear in the TrackScheme.\nCheck how accurate it is by clicking on the spots and watching their position relative to the nucleus in the BigDataViewer windows. Try going further with the semi-automated tracking.\n\nHover a spot and press Ctrl+T to continue the semi-automated tracking.\n\n\nSee how long you can go, how it behaves with cell divisions, and which cells work well with it and which don’t. There are many parameters that can be adjusted to tweak the semi-automated tracking behavior. Please check Mastodon’s documentation for more details.",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-auto-tracking",
    "href": "practicals/practical_tracking/index.html#sec-auto-tracking",
    "title": "Cell tracking using Mastodon",
    "section": "Automated Tracking",
    "text": "Automated Tracking\nThe final part of this tutorial is to try automatic detection and linking of spots. This is the dream: loading data and getting out a full lineage. However, in practice, it’s a lot messier. Cleaning, fixing, and curating the data is required to get a nice informative lineage. We will use a simplified version of the protocol included in this demo dataset. It is described in the file Protocol_Auto-Detection_Auto-Linking.docx.\n\n\nDetection\n\nIn the Mastodon window, go to Plugins &gt; Tracking &gt; Detection....\n\n\n\nPress Next twice (leave options as is).\n\n\n\n\n\n\n\n\n\n\n\n\nChoose Advanced DoG detector and press Next.\nKeep Detect: bright blobs.\nChange Estimated diameter to 35px.\nChange Quality threshold to 100.\nKeep Behavior as Add.\nThen, press Next.\n\n\n\n\n\n\n\n\n\n\n\n\nClick Preview and see how well the detection will work by exploring a new BigDataViewer window.\n\n\nAre there too many false positives? Try changing the diameter, for example, and run the preview again to see what happens to the detected spots.\n\nOnce satisfied, press Next and wait.\n\n\n\nWhen the detection is done, press Finish.\n\n\nThe BigDataViewer windows and the TrackScheme will be showing a lot of new spots.\n\nExplore the spots in the BigDataViewer and TrackScheme windows.\nZoom in to see the unlinked, individual spots per frame.\n\n\n\n\nCleaning\nBefore we try to automatically link these spots, let’s remove low quality detections.\n\nOn the Mastodon window click on Table, resize it to have more space, and resize the column Detection q... to show Detection quality.\n\n\n\n\n\n\n\n\n\n\n\n\nClick on Detection quality to sort the table.\n\n\n\nClick on the first row to select it. Select all rows where Detection quality is &lt;400.\nThen click Edit &gt; Delete Selection.\n\n\n\nClose the table.\n\nWe can also manually delete obviously wrong spots by hovering and pressing D.\n\nGive it a try by cleaning up the spots outside the embryo.\n\n\n\nLinking\nNow let’s try linking spots.\n\nIn the main Mastodon window click on Plugins &gt; Tracking &gt; Linking....\nKeep All spots selected for all timepoints (0-29) and press Next.\nChoose Lap linker and click Next.\n\n\n\n\n\n\n\n\n\n\n\n\nChange the parameters to:\n\nFrame to frame linking: Max distance to 40px.\nGap closing: Max distance to 60px (keep others as is).\nTrack division: Check Allow track division, set Max distance to 40px, and press + to add a Feature penalty and set it to Center ch1 to 0.3.\n\nPress Next to start linking and wait… then press Finish.\n\n\n\n\n\n\n\n\n\n\n\nNote that there are now tracks in the BigDataViewer and TrackScheme windows.\n\nExplore them a bit.\n\n\nMastodon can calculate features (position, displacement, velocity, etc.) of individual spots, links and branches. Let’s do that.\n\nIn the main Mastodon window press compute features.\n\n\nA Feature calculation window will open.\n\nPress compute and wait… when it’s done, close it.\n\n\nNote that now the tracks in the BigDataViewer are showing colored links.\n\nOpen the table window from the main window.\n\n\nIt’ll be filled with computed features.",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-feature-visualization",
    "href": "practicals/practical_tracking/index.html#sec-feature-visualization",
    "title": "Cell tracking using Mastodon",
    "section": "Basic Feature Visualization",
    "text": "Basic Feature Visualization\nFinally, let’s visualize the computed features that might be interesting or useful.\n\nIn the BigDataViewer window press File &gt; Preferences to open the feature color coding visualization parameters.\n\n\n\nOn Settings &gt; Feature Color Modes click on Duplicate (it’ll generate a Number of links (2)) and then Rename.\nRename it to Velocity.\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Coloring Spots change Read spot color from to Incoming link and change Feature to Link velocity. Then click on autoscale in the range.\nOn the Coloring Links change Read link color from to Link and change Feature to Link velocity. Then also click on autoscale.\nClick Apply (nothing will happen), then OK.\n\n\n\nOn the BigDataViewer window press View &gt; Coloring &gt; Velocity.\n\n\nThe spots and links in the BigDataViewer window will change colors.\n\n\nDo the same for the other BigDataViewer window and the TrackScheme.\n\n\nThis gives a visual representation of cells which have a high displacement per frame. These might be artifacts in linking unrelated spots or, in a good processed lineage, reveal some biological process like cell migration.",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-graph-plotting",
    "href": "practicals/practical_tracking/index.html#sec-graph-plotting",
    "title": "Cell tracking using Mastodon",
    "section": "Graph Plotting",
    "text": "Graph Plotting\nTo finalize, a simple example of plotting the lineage data.\n\nClick on grapher in the main Mastodon window, a plot window will open.\n\n\n\nPress the Lock 1 to lock the windows, select Link velocity - outgoing for X axis and Detection quality for Y axis and press Plot.\n\n\n\nFind out if the spots with the highest link velocity are properly linked or if it is an artifact.",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-citation",
    "href": "practicals/practical_tracking/index.html#sec-citation",
    "title": "Cell tracking using Mastodon",
    "section": "Citation",
    "text": "Citation\nVellutini, B. C. (2026). Cell tracking in Fiji using Mastodon. Zenodo. https://doi.org/10.5281/zenodo.18090897",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-license",
    "href": "practicals/practical_tracking/index.html#sec-license",
    "title": "Cell tracking using Mastodon",
    "section": "License",
    "text": "License\nThis tutorial is available under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-references",
    "href": "practicals/practical_tracking/index.html#sec-references",
    "title": "Cell tracking using Mastodon",
    "section": "References",
    "text": "References\n\n\nGirstmair, Johannes. 2024. Mastodon Auto-Tracking Demo on Parhyale Hawaiensis Limb Development. Zenodo. https://doi.org/10.5281/ZENODO.13944688.\n\n\nGirstmair, Johannes, Tobias Pietzsch, Vladimir Ulman, Stefan Hahmann, Matthias Arzt, Mette Handberg-Thorsager, Ko Sugawara, et al. 2025. “Mastodon: The Command Center for Large-Scale Lineage-Tracing Microscopy Datasets.” bioRxiv, December, 2025.12.10.693416. https://doi.org/10.64898/2025.12.10.693416.\n\n\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. “Fiji: An Open-Source Platform for Biological-Image Analysis.” Nat. Methods 9 (June): 676–82. https://doi.org/10.1038/nmeth.2019.\n\n\nWolff, Carsten, Jean-Yves Tinevez, Tobias Pietzsch, Evangelia Stamataki, Benjamin Harich, Léo Guignard, Stephan Preibisch, et al. 2018. “Multi-View Light-Sheet Imaging and Tracking with the MaMuT Software Reveals the Cell Lineage of a Direct Developing Arthropod Limb.” Elife 7 (March). https://doi.org/10.7554/eLife.34410.",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon"
    ]
  },
  {
    "objectID": "practicals/practical_object_classification/index.html",
    "href": "practicals/practical_object_classification/index.html",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "Pixel classification, dimensionality reduction, and automatic clustering are common data-driven approaches used to analyze complex image data. Pixel classification assigns each pixel to a class based on its intensity or multi-channel feature values, enabling tasks such as segmentation or tissue labeling without explicit object detection. Dimensionality reduction methods (such as PCA, UMAP, or t-SNE) compress high-dimensional feature spaces into fewer dimensions while preserving key patterns or relationships, making large imaging datasets easier to visualize and interpret. Automatic clustering methods (e.g. k-means or similar algorithms) then group pixels or objects into clusters based on feature similarity, allowing unbiased identification of recurring patterns or phenotypes without predefined labels.\n\n\nBy the end of this tutorial, participants will be able to:\n\nTrain an object classifier with Napari\nFilter labels\nExtract morphological and intensity based parameters\nPlot parameters in 2D\nApply a dimensionality reduction method\nVisualize the results\nCluster subgroups of objects using K-means\n\nThese steps form a complete workflow: raw image → segmentation → cleanup → labeling → measurement → data viz\nWe’ll use Lund.tif as the example image https://zenodo.org/records/17986091\nRequirements: - Everything you need is in the toml file in the Pixi/napari-devbio folder https://github.com/cuenca-mb/pixi-napari-devbio\n–\n\n\n\nIn the terminal, go to the directory Pixi/napari-devbio and run:\npixi run napari\n\n\n\nDrag and drop the file or\nFile → Open File\n\n\n\nLoad image and generate labels layer\n\n\nWe will be able to see and explore the stack, and we can change to a 3D rendering with the option Toogle 2D/3D view in the lower left button pannel. We will create a labels layer to anotate some ground truth for our object classifier.\nIn the left pannel, select the brush tool and make some background anotations with the brush. Then change to label 2 and make some object annotations (cells).\n\n\n\nDraw the labels\n\n\n\n\n\nGo to\nTools → Segmentation / labeling → Object segmentation (APOC)\n\n\n\nBrightness/Contrast histogram\n\n\nSelect the raw data for training and the labels layer for ground truth. Then click Train. Observe the results in 3D. For better visualization you can hide the labels layer.\n\n\n\nTrain\n\n\n\n\n\nObserve labels result\n\n\nFrom this first round of training, very likely the result is not perfect. This is because our ground truth anotation was done on a single z slice, so the classifier will perform well only around that depth. We can do several rounds of training.\nSelect again the labels image, and in the 2D mode go to a different z. Perform new anotations, preserving the label orders for background and object. Then just click Train again and observe the new results. Repeat as needed. Note that a classifier file will be produced and overwritten after each training. This can be used for batch segmentation of data from the same experiment/microscope.\n\n\n\nModify labels layer\n\n\n\n\n\nRe-train\n\n\n\n\n\nNow there might be some small objects that we want to filter. Go to\nTools → Segmentation post-processing → Exclude small labels\nThen select the layer Result of ObjectSegmentation and impose a minimum size of 20 (it will say maximum size in napari, but it is a mistake from the developers). Run it and observe the final labels (hide the old labels for better visualization.\n\n\n\nExclude small objects\n\n\n\n\n\nObserve filtered labels\n\n\nNow that we have a nice set of labels in 3D, we will quantify intensity and morphology based parameters for further analysis.\nGo to\nTools → Measurement tables → Regionprops\nSelect all parameters except moments and click run.\n\n\n\nRegionprops\n\n\n\n\n\nTable\n\n\nWe can export this table or visualize some of the data using the cluster plotter.\n\n\n\nGo to\nPlugins → napari-clusters-plotter → Plotter widget\nIn here you can select any measurement (column) from the region props table and make scatter plots.\n\n\n\nPlotter widget\n\n\n\n\n\nScatter plot\n\n\nA very useful tool is that we can draw in the plot closed curves and the objects falling inside will be automatically colorcoded and visualized in the 3D rendered image. I recommend hiding all the other layers except the raw image.\nBy pressing shift, we can draw further subgroups.\n\n\n\nLabel IDs\n\n\n\nThis manual selection generates a new column in the regionprops table with the identifier for us to export and further analyse in other software.\n\n\n\nGo to\nPlugins → napari-clusters-plotter → Dimensionality Reduction Widget\nThis is a very useful tool in which we can apply a variety of dimensionality reduction methods to our list of parameters. In this example I used UMAPS. We can select how many components we want and after hitting run they will be added to our regionprops table.\n\n\n\nDimensionality Reduction\n\n\n\n\n\nUMAP\n\n\nAfter doing this, we can come back to the Plotter Widget and plot this two components of the UMAP and generate more complex clustering of the data manually.\n\n\n\nUMAP viz\n\n\n\n\n\nManual Cluster ID\n\n\n\n\n\nBut what if we don’t want to be biased when clustering the data in subgroups? We can use unsupervised methods includded in this plugin. Go to\nPlugins → napari-clusters-plotter → Clustering Widget\nSelect the K-MEANS method, impose 3 clusters and run. This will generate a new column with the cluster ID for each object.\n\n\n\nClustering methods\n\n\n\n\n\nK-MEANS\n\n\nNow going back to the Plotter Widget, we can select the UMAP axis AND the K-MEAN cluster IDs to automatically colorcode the objects belonging to different classes.\n\n\n\nAutomatic clustering",
    "crumbs": [
      "Practicals",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_object_classification/index.html#learning-outcomes",
    "href": "practicals/practical_object_classification/index.html#learning-outcomes",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "By the end of this tutorial, participants will be able to:\n\nTrain an object classifier with Napari\nFilter labels\nExtract morphological and intensity based parameters\nPlot parameters in 2D\nApply a dimensionality reduction method\nVisualize the results\nCluster subgroups of objects using K-means\n\nThese steps form a complete workflow: raw image → segmentation → cleanup → labeling → measurement → data viz\nWe’ll use Lund.tif as the example image https://zenodo.org/records/17986091\nRequirements: - Everything you need is in the toml file in the Pixi/napari-devbio folder https://github.com/cuenca-mb/pixi-napari-devbio\n–",
    "crumbs": [
      "Practicals",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_object_classification/index.html#open-the-image-and-inspect-the-histogram",
    "href": "practicals/practical_object_classification/index.html#open-the-image-and-inspect-the-histogram",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "In the terminal, go to the directory Pixi/napari-devbio and run:\npixi run napari",
    "crumbs": [
      "Practicals",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_object_classification/index.html#open-the-image-and-create-manual-anotations",
    "href": "practicals/practical_object_classification/index.html#open-the-image-and-create-manual-anotations",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "Drag and drop the file or\nFile → Open File\n\n\n\nLoad image and generate labels layer\n\n\nWe will be able to see and explore the stack, and we can change to a 3D rendering with the option Toogle 2D/3D view in the lower left button pannel. We will create a labels layer to anotate some ground truth for our object classifier.\nIn the left pannel, select the brush tool and make some background anotations with the brush. Then change to label 2 and make some object annotations (cells).\n\n\n\nDraw the labels",
    "crumbs": [
      "Practicals",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_object_classification/index.html#train-the-object-classifier",
    "href": "practicals/practical_object_classification/index.html#train-the-object-classifier",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "Go to\nTools → Segmentation / labeling → Object segmentation (APOC)\n\n\n\nBrightness/Contrast histogram\n\n\nSelect the raw data for training and the labels layer for ground truth. Then click Train. Observe the results in 3D. For better visualization you can hide the labels layer.\n\n\n\nTrain\n\n\n\n\n\nObserve labels result\n\n\nFrom this first round of training, very likely the result is not perfect. This is because our ground truth anotation was done on a single z slice, so the classifier will perform well only around that depth. We can do several rounds of training.\nSelect again the labels image, and in the 2D mode go to a different z. Perform new anotations, preserving the label orders for background and object. Then just click Train again and observe the new results. Repeat as needed. Note that a classifier file will be produced and overwritten after each training. This can be used for batch segmentation of data from the same experiment/microscope.\n\n\n\nModify labels layer\n\n\n\n\n\nRe-train",
    "crumbs": [
      "Practicals",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_object_classification/index.html#filter-small-objects-and-quantify-parameters",
    "href": "practicals/practical_object_classification/index.html#filter-small-objects-and-quantify-parameters",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "Now there might be some small objects that we want to filter. Go to\nTools → Segmentation post-processing → Exclude small labels\nThen select the layer Result of ObjectSegmentation and impose a minimum size of 20 (it will say maximum size in napari, but it is a mistake from the developers). Run it and observe the final labels (hide the old labels for better visualization.\n\n\n\nExclude small objects\n\n\n\n\n\nObserve filtered labels\n\n\nNow that we have a nice set of labels in 3D, we will quantify intensity and morphology based parameters for further analysis.\nGo to\nTools → Measurement tables → Regionprops\nSelect all parameters except moments and click run.\n\n\n\nRegionprops\n\n\n\n\n\nTable\n\n\nWe can export this table or visualize some of the data using the cluster plotter.",
    "crumbs": [
      "Practicals",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_object_classification/index.html#data-visualization-with-clusters-plotter",
    "href": "practicals/practical_object_classification/index.html#data-visualization-with-clusters-plotter",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "Go to\nPlugins → napari-clusters-plotter → Plotter widget\nIn here you can select any measurement (column) from the region props table and make scatter plots.\n\n\n\nPlotter widget\n\n\n\n\n\nScatter plot\n\n\nA very useful tool is that we can draw in the plot closed curves and the objects falling inside will be automatically colorcoded and visualized in the 3D rendered image. I recommend hiding all the other layers except the raw image.\nBy pressing shift, we can draw further subgroups.\n\n\n\nLabel IDs\n\n\n\nThis manual selection generates a new column in the regionprops table with the identifier for us to export and further analyse in other software.",
    "crumbs": [
      "Practicals",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_object_classification/index.html#dimensionality-reduction",
    "href": "practicals/practical_object_classification/index.html#dimensionality-reduction",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "Go to\nPlugins → napari-clusters-plotter → Dimensionality Reduction Widget\nThis is a very useful tool in which we can apply a variety of dimensionality reduction methods to our list of parameters. In this example I used UMAPS. We can select how many components we want and after hitting run they will be added to our regionprops table.\n\n\n\nDimensionality Reduction\n\n\n\n\n\nUMAP\n\n\nAfter doing this, we can come back to the Plotter Widget and plot this two components of the UMAP and generate more complex clustering of the data manually.\n\n\n\nUMAP viz\n\n\n\n\n\nManual Cluster ID",
    "crumbs": [
      "Practicals",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_object_classification/index.html#unsupervised-clustering",
    "href": "practicals/practical_object_classification/index.html#unsupervised-clustering",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "But what if we don’t want to be biased when clustering the data in subgroups? We can use unsupervised methods includded in this plugin. Go to\nPlugins → napari-clusters-plotter → Clustering Widget\nSelect the K-MEANS method, impose 3 clusters and run. This will generate a new column with the cluster ID for each object.\n\n\n\nClustering methods\n\n\n\n\n\nK-MEANS\n\n\nNow going back to the Plotter Widget, we can select the UMAP axis AND the K-MEAN cluster IDs to automatically colorcode the objects belonging to different classes.\n\n\n\nAutomatic clustering",
    "crumbs": [
      "Practicals",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_macros/index.html",
    "href": "practicals/practical_macros/index.html",
    "title": "Macro programming in Fiji",
    "section": "",
    "text": "Fiji includes a built-in macro language that allows you to automate repetitive image-processing tasks, ensure reproducibility, and scale analyses to large datasets. Macros can record actions performed in the graphical user interface (GUI) and translate them into editable code, making them an excellent entry point into scripting.\nThis tutorial introduces the basics of working with macros in Fiji.\n\n\nBy the end of this tutorial, participants will be able to:\n\nUse the Fiji Macro Editor and Macro Recorder to create and modify ImageJ macros.\nBuild batch-processing macros to analyze large image datasets automatically.\nDesign interactive macros that accept user-defined parameters via dialogs.\nWrite clear, reusable macro code using variables, loops, and functions.\nImplement reproducible image-analysis workflows that can be shared and rerun.\nAdapt existing macros to new datasets and experimental conditions.\n\n\n\n\n\n\n\nTo write or edit macros in Fiji, you will use the Script Editor, which includes support for the ImageJ Macro language.\n\nOpen Fiji.\nFrom the top menu bar, go to\nFile → New → Script… A new window will open. This is the Macro Editor, where you can write, edit, and run macro code.\nFrom the top menu bar, go to\nLanguage → ImageJ Macro\n\nOnce the editor is open, you can start writing macros or paste code generated by the Macro Recorder.\n\n\n\n\nTo write comments that won’t run in your script use //.\nVariables hold values and can be defined by operators.\nWe need to explicit the end of a line of code using semicolons.\nStrings are chain of characters inside \"\".\nWe can print variables or strings with the function print()\n\n\n\n\n\n\n\nTypes of variables\n\n\n\n\n\n\n\n\n\n\n\nMacro code contains colorcoded text. Pink = strings Yellow = commands Green = comments\nIf something is off, likely you made a mistake in the code. And if so, when you run the code you’ll have a pop up window with a very broad description of what might have gone wrong and where (in general not very helpful).\n\n\n\nThe life changing tool that will make you grow fond of Fiji. Go to Plugins → Macro → Record...\n\n\n\nRecorder\n\n\nNow everything we do will be “recorded” and we can turn it into reproducible image analysis pipelines that run automatically.\n\n\n\n\n\nNow, with the recorder open we will create a Macro that changes automatically Brightness and contrast features of a multi channel image and creates a png figure with scale bar.\n\n\n\nOpen Fiji.\nFrom the menu bar, go to:\nFile → Open Samples → Fluorescent Cells\n\nThis will load a multichannel fluorescence image provided with Fiji.\n\n\n\n\nWith the image window active, go to:\nImage → Color → Split Channels\n\nFiji will create separate grayscale images for each fluorescence channel (e.g. C1, C2, C3).\n\n\n\nAdjust the display range of each channel individually.\n\nGo to:\nImage → Adjust → Brightness/Contrast…\nChange the minimum or maximum values. Do it arbitrarily, it’s just a test!\nClick Set or Apply.\nRepeat for the other channels\n\n\nNote: These values affect only the display, not the underlying pixel data.\n\n\n\n\n\nGo to:\nImage → Color → Merge Channels…\nClick OK.\n\nA merged multichannel image will be created.\n\n\n\n\nSelect the merged image window.\nGo to:\nAnalyze → Tools → Scale Bar… Ideally, the scale bar will be in microns but in this sample image it will be in pixels.\nClick OK.\n\n\n\n\n\nWith the merged image selected, go to:\nImage → Type → RGB Color\n\nThis step is required before saving the image in common formats such as PNG.\n\n\n\n\nGo to:\nFile → Save As → PNG…\nChoose a destination folder.\nName the file (e.g. FluorescentCells.png).\nClick Save.\n\nYou have now manually completed the same workflow that will later be automated using a Fiji macro. Go to the recorder and click Create. Now let’s run it! You will obtain the same result of your manual pipeline and you can apply this to other images from the same experiment to compare visually using the same B&C parameters and get png images ready for publications.\n\n\n\nRaw macro\n\n\nNote that the command run contains the instructions to run the specific functions in string format with a sintaxis that is very specific. Minor variations might occurr from MacOS/Ubuntu to Windows. Other commands such as saveAs include the specific title of the image we are processing, or directory where is being saved. We will pay attention to these when running the analysis in another image or directory.\n\n\n\nLet’s asume we would like to run this code in an image we previously opened. We can then remove the first line of code that opens the sample image.\nNext, the title of the opened image is being used throughout the code, it’s includded by default in the title of each channel after splitting, in the merge command and in the saveAs function. A good way to generalize the code is to create a variable with the title of the open image, whichever that is. For this wew use the function getTitle(). We can then replace VERY carefully, the title of the image by our variable.\nReaching the end of the code, we would like to have an interactive way to select the directory where the image will be saved. We can use the function getDirectory() to browse the folder we want and save that path as a variable to insert in the function. We usually would like to put this at the beginning of the code since it requires user input and if the analysis is long we would need to wait to select the saving directory. Now we can replace the dir and title variables in the saveAs function.\nTest and save the macro.\n\n\n\n\n\n\nRaw macro\n\n\n\n\n\n\n\nEdited macro\n\n\n\n\n\nFinal version of the code:\n//This macro process a 3 channel image with stainings A, B and C. Adjusts B&C, \n//Creates a png with scale bar and saves\n//Marina Cuenca 2026\n\ndir = getDirectory(\"Saving directory\");\n\ntitle = getTitle();\n\nrun(\"Split Channels\");\n\nselectImage(\"C1-\" + title);\nsetMinAndMax(0, 182);\n\nselectImage(\"C2-\" + title);\nsetMinAndMax(0, 182);\n\nselectImage(\"C3-\" + title);\nsetMinAndMax(0, 157);\n\nrun(\"Merge Channels...\", \"c1=C1-\" + title + \" c2=C2-\" + title + \" c3=C3-\" + title + \" create\");\n\nrun(\"Scale Bar...\", \"width=50 height=50 location=[Upper Right] bold overlay\");\n\nrun(\"RGB Color\"); \n\nsaveAs(\"PNG\", dir + \"/\" + title);\n\n\n\n\n\nNow we will build step by step a script to process images belonging to the same experiment stored in the same folder, from which we want to quantify morphology of the cells. We will have to automatically search what is inside the folder and loop through the contents, applying the pipeline to each image, storing the results, and closing the open windows before moving to the next image. This might sound overwheelming, but the strategy is to make it work for one image, generalize de code and then wrap it in a loop.\nWe will use the dataset in folder Drosophila-CartographicProjection (https://zenodo.org/records/18020241)\n\n\nThis section describes how to manually perform image segmentation and region-based morphometric analysis in Fiji, following the same steps that will later be automated using a macro.\nMake sure the recorder is clear.\n\n\n\nOpen Fiji.\nDrop the file or Go to:\nFile → Open… and open one of the images in the folder. Any is fine.\n\n\n\n\n\nMake sure the image window is active.\nGo to:\nImage → Type → 8-bit\n\nThis step converts the image to 8-bit grayscale, which is required by many thresholding and segmentation algorithms.\n\n\n\n\nWith the image selected, go to:\nImage → Adjust → Auto Local Threshold…\nIn the dialog, set:\n\nMethod: Otsu\nRadius: 15\nLeave other parameters at their default values\n\nClick OK\n\nThis will generate a binary image separating foreground objects from the background.\n\n\n\n\nWith the thresholded (binary) image active, go to:\nPlugins → MorphoLibJ → Binary Images → Connected Components Labeling\nSet:\n\nConnectivity: 4\nOutput type: 16 bits\n\nClick OK\n\nEach connected object in the image will be assigned a unique label.\n\n\n\n\nSelect the labeled image.\nGo to:\nPlugins → MorphoLibJ → Label Images → Remove Border Labels\nEnsure all borders are selected:\n\nLeft\nRight\nTop\nBottom\n\nClick OK\n\nObjects touching the image borders will be removed from the label image.\n\n\n\n\nWith the cleaned label image selected, go to:\nPlugins → MorphoLibJ → Analyze → Analyze Regions\nSelect the following measurements:\n\nArea\nPerimeter\nCircularity\n\nClick OK\n\nA Results table containing the selected morphometric measurements will appear.\n\n\n\n\nClick on the Results window.\nGo to:\nFile → Save As…\nSave the table as a CSV file, for example:\nBtd-cp000-Morphometry.csv*\n\n\n\n\n\nSelect the labeled image window.\nGo to:\nFile → Save As → Tiff…\nSave the image, for example as:\nBtd-cp000-lbl-killBorders.tif\n\nYou have now manually completed a full workflow including image preprocessing, segmentation, object filtering, and quantitative morphometric analysis.\nWhen clicking Create in the macro recorder, you will see a script like this:\n\n\n\nManual macro\n\n\n\n\n\n\n\nThe first line of code is to open the image, we will ignore this until the loop comes into place.\nLet’s get the title of the image to generalize the rest of the code. This will need to happen once the image is open.\n\n\n\n\n\n\nBefore\n\n\n\n\n\n\n\nAfter\n\n\n\n\n\nTry the macro. As result we have several open windows we would like to close before opening the next image in the folder. For this, we will use the command close. We can specify the name of the window we want to close by close('name'), close the current selected window close or close everything close(*), which is what we want in this case. Sadly, the Result windows are special windows that do not respond to this command, we do have to specify its name.\n\n\n\n\n\n\nBefore\n\n\n\n\n\n\n\nAfter\n\n\n\n\n\nTry the macro again. As result we get the processing done and no image open. Data will be saved in whichever directory we specified at the beginning.\n\n\n\n\n\nFirst we need to get the name of the source directory and get the file names inside. For this we will use the getDirectory and getFileList functions. I do recommend at this point to print the contents of the folder to make sure we are in the right place and for future debugging. We have to do this with a for loop, through all the values inside out file list:\n`sourceDir = getDirectory(“Source directory”); //select where images are\nfileList = getFileList(sourceDir); //list of files inside the sourceDir\nfor (i = 0; i &lt; fileList.length; i++) { //fileList.lenght is an integer print(fileList[i]); //element i inside fileList (starts counting from 0) }`\n\n\n\n\n\n\nBefore\n\n\n\n\n\n\n\nAfter\n\n\n\n\n\nRun the macro.\n\n\n\nRight now our log returns all the files inside the source directory, which now contains also the result of our analysis. It is best practice to create an Analysis folder, where data will be stored. In this way our raw data and our processed data remain separate. The easiest way is to create it ourselved by hand and use the getDirectory function, but you can also add an option in the code to create it if it does not exists already (safest).\noutputDir = sourceDir + \"/Analysis\"; //name of the saving folder within the iamge containing folder if (!File.exists(outputDir)) {      File.makeDirectory(outputDir); //creates folder if it does not exist already }\n\n\n\n\n\n\nBefore\n\n\n\n\n\n\n\nAfter\n\n\n\n\n\n\n\n\nNow let’s loop through the files ending with .tif and introduce the sourceDir, fileList and outputDir variables. Note that the fileName elements are identical to the titles, so we can change that variable.\n\n\n\n\n\n\nBefore\n\n\n\n\n\n\n\nAfter\n\n\n\n\n\nRun the code. At this point you should be seeing each image open individually, processing, and closing. In the Analysis folder you should get the output of the analysis.\nIf images are big and take time to open, we can activate BatchMode by includding the line of code setBatchmode(true) anywhere in the code (before looping through files and opening them).\nThe final version of the code is here:\n//Opens individual 2D membrane images from Drosophila-CarographicProjections\n//Segments, labels and quantifies area, perimeter and circularity of the cells\n//Saves labels and csv file in Analysis folder\n//Marina Cuenca 2026\n\nsourceDir = getDirectory(\"Source directory\"); //select where images are\n\nfileList = getFileList(sourceDir); //list of files inside the sourceDir\n\nfor (i = 0; i &lt; fileList.length; i++) { //fileList.lenght is an integer\n    print(fileList[i]); //element i inside fileList (starts counting from 0)\n}\n\noutputDir = sourceDir + \"/Analysis\"; //name of the saving folder within the iamge containing folder\nif (!File.exists(outputDir)) { \n    File.makeDirectory(outputDir); //creates folder if it does not exist already\n}\n\nsetBatchMode(true); //Won't show the images when open\n\nfor (i = 0; i &lt; fileList.length; i++) { //Loop through images\n    \n    title = fileList[i];\n    \n    if (!endsWith(title, \".tif\")) //only runs the code if file is an image .tif\n        continue;\n\n    open(sourceDir + \"/\" + title);\n    \n    selectImage(title);\n    \n    run(\"8-bit\");\n    run(\"Auto Local Threshold\", \"method=Otsu radius=15 parameter_1=0 parameter_2=0\");\n    run(\"Connected Components Labeling\", \"connectivity=4 type=[16 bits]\");\n    run(\"Remove Border Labels\", \"left right top bottom\");\n    run(\"Analyze Regions\", \"area perimeter circularity\");\n    \n    saveAs(\"Results\", outputDir + \"/\" + title + \"-Morphometry.csv\");\n    saveAs(\"Tiff\", outputDir + \"/\" + title + \"-lbl-killBorders.tif\");\n    \n    close(\"*\");\n    close( title + \"-Morphometry.csv\");\n    \n}\n\nprint(\"Done\");",
    "crumbs": [
      "Practicals",
      "Macro programming in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_macros/index.html#learning-outcomes",
    "href": "practicals/practical_macros/index.html#learning-outcomes",
    "title": "Macro programming in Fiji",
    "section": "",
    "text": "By the end of this tutorial, participants will be able to:\n\nUse the Fiji Macro Editor and Macro Recorder to create and modify ImageJ macros.\nBuild batch-processing macros to analyze large image datasets automatically.\nDesign interactive macros that accept user-defined parameters via dialogs.\nWrite clear, reusable macro code using variables, loops, and functions.\nImplement reproducible image-analysis workflows that can be shared and rerun.\nAdapt existing macros to new datasets and experimental conditions.",
    "crumbs": [
      "Practicals",
      "Macro programming in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_macros/index.html#basic-macro-concepts",
    "href": "practicals/practical_macros/index.html#basic-macro-concepts",
    "title": "Macro programming in Fiji",
    "section": "",
    "text": "To write or edit macros in Fiji, you will use the Script Editor, which includes support for the ImageJ Macro language.\n\nOpen Fiji.\nFrom the top menu bar, go to\nFile → New → Script… A new window will open. This is the Macro Editor, where you can write, edit, and run macro code.\nFrom the top menu bar, go to\nLanguage → ImageJ Macro\n\nOnce the editor is open, you can start writing macros or paste code generated by the Macro Recorder.\n\n\n\n\nTo write comments that won’t run in your script use //.\nVariables hold values and can be defined by operators.\nWe need to explicit the end of a line of code using semicolons.\nStrings are chain of characters inside \"\".\nWe can print variables or strings with the function print()\n\n\n\n\n\n\n\nTypes of variables\n\n\n\n\n\n\n\n\n\n\n\nMacro code contains colorcoded text. Pink = strings Yellow = commands Green = comments\nIf something is off, likely you made a mistake in the code. And if so, when you run the code you’ll have a pop up window with a very broad description of what might have gone wrong and where (in general not very helpful).\n\n\n\nThe life changing tool that will make you grow fond of Fiji. Go to Plugins → Macro → Record...\n\n\n\nRecorder\n\n\nNow everything we do will be “recorded” and we can turn it into reproducible image analysis pipelines that run automatically.",
    "crumbs": [
      "Practicals",
      "Macro programming in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_macros/index.html#reproducible-and-automatic-pipeline",
    "href": "practicals/practical_macros/index.html#reproducible-and-automatic-pipeline",
    "title": "Macro programming in Fiji",
    "section": "",
    "text": "Now, with the recorder open we will create a Macro that changes automatically Brightness and contrast features of a multi channel image and creates a png figure with scale bar.\n\n\n\nOpen Fiji.\nFrom the menu bar, go to:\nFile → Open Samples → Fluorescent Cells\n\nThis will load a multichannel fluorescence image provided with Fiji.\n\n\n\n\nWith the image window active, go to:\nImage → Color → Split Channels\n\nFiji will create separate grayscale images for each fluorescence channel (e.g. C1, C2, C3).\n\n\n\nAdjust the display range of each channel individually.\n\nGo to:\nImage → Adjust → Brightness/Contrast…\nChange the minimum or maximum values. Do it arbitrarily, it’s just a test!\nClick Set or Apply.\nRepeat for the other channels\n\n\nNote: These values affect only the display, not the underlying pixel data.\n\n\n\n\n\nGo to:\nImage → Color → Merge Channels…\nClick OK.\n\nA merged multichannel image will be created.\n\n\n\n\nSelect the merged image window.\nGo to:\nAnalyze → Tools → Scale Bar… Ideally, the scale bar will be in microns but in this sample image it will be in pixels.\nClick OK.\n\n\n\n\n\nWith the merged image selected, go to:\nImage → Type → RGB Color\n\nThis step is required before saving the image in common formats such as PNG.\n\n\n\n\nGo to:\nFile → Save As → PNG…\nChoose a destination folder.\nName the file (e.g. FluorescentCells.png).\nClick Save.\n\nYou have now manually completed the same workflow that will later be automated using a Fiji macro. Go to the recorder and click Create. Now let’s run it! You will obtain the same result of your manual pipeline and you can apply this to other images from the same experiment to compare visually using the same B&C parameters and get png images ready for publications.\n\n\n\nRaw macro\n\n\nNote that the command run contains the instructions to run the specific functions in string format with a sintaxis that is very specific. Minor variations might occurr from MacOS/Ubuntu to Windows. Other commands such as saveAs include the specific title of the image we are processing, or directory where is being saved. We will pay attention to these when running the analysis in another image or directory.\n\n\n\nLet’s asume we would like to run this code in an image we previously opened. We can then remove the first line of code that opens the sample image.\nNext, the title of the opened image is being used throughout the code, it’s includded by default in the title of each channel after splitting, in the merge command and in the saveAs function. A good way to generalize the code is to create a variable with the title of the open image, whichever that is. For this wew use the function getTitle(). We can then replace VERY carefully, the title of the image by our variable.\nReaching the end of the code, we would like to have an interactive way to select the directory where the image will be saved. We can use the function getDirectory() to browse the folder we want and save that path as a variable to insert in the function. We usually would like to put this at the beginning of the code since it requires user input and if the analysis is long we would need to wait to select the saving directory. Now we can replace the dir and title variables in the saveAs function.\nTest and save the macro.\n\n\n\n\n\n\nRaw macro\n\n\n\n\n\n\n\nEdited macro\n\n\n\n\n\nFinal version of the code:\n//This macro process a 3 channel image with stainings A, B and C. Adjusts B&C, \n//Creates a png with scale bar and saves\n//Marina Cuenca 2026\n\ndir = getDirectory(\"Saving directory\");\n\ntitle = getTitle();\n\nrun(\"Split Channels\");\n\nselectImage(\"C1-\" + title);\nsetMinAndMax(0, 182);\n\nselectImage(\"C2-\" + title);\nsetMinAndMax(0, 182);\n\nselectImage(\"C3-\" + title);\nsetMinAndMax(0, 157);\n\nrun(\"Merge Channels...\", \"c1=C1-\" + title + \" c2=C2-\" + title + \" c3=C3-\" + title + \" create\");\n\nrun(\"Scale Bar...\", \"width=50 height=50 location=[Upper Right] bold overlay\");\n\nrun(\"RGB Color\"); \n\nsaveAs(\"PNG\", dir + \"/\" + title);",
    "crumbs": [
      "Practicals",
      "Macro programming in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_macros/index.html#batch-processing",
    "href": "practicals/practical_macros/index.html#batch-processing",
    "title": "Macro programming in Fiji",
    "section": "",
    "text": "Now we will build step by step a script to process images belonging to the same experiment stored in the same folder, from which we want to quantify morphology of the cells. We will have to automatically search what is inside the folder and loop through the contents, applying the pipeline to each image, storing the results, and closing the open windows before moving to the next image. This might sound overwheelming, but the strategy is to make it work for one image, generalize de code and then wrap it in a loop.\nWe will use the dataset in folder Drosophila-CartographicProjection (https://zenodo.org/records/18020241)\n\n\nThis section describes how to manually perform image segmentation and region-based morphometric analysis in Fiji, following the same steps that will later be automated using a macro.\nMake sure the recorder is clear.\n\n\n\nOpen Fiji.\nDrop the file or Go to:\nFile → Open… and open one of the images in the folder. Any is fine.\n\n\n\n\n\nMake sure the image window is active.\nGo to:\nImage → Type → 8-bit\n\nThis step converts the image to 8-bit grayscale, which is required by many thresholding and segmentation algorithms.\n\n\n\n\nWith the image selected, go to:\nImage → Adjust → Auto Local Threshold…\nIn the dialog, set:\n\nMethod: Otsu\nRadius: 15\nLeave other parameters at their default values\n\nClick OK\n\nThis will generate a binary image separating foreground objects from the background.\n\n\n\n\nWith the thresholded (binary) image active, go to:\nPlugins → MorphoLibJ → Binary Images → Connected Components Labeling\nSet:\n\nConnectivity: 4\nOutput type: 16 bits\n\nClick OK\n\nEach connected object in the image will be assigned a unique label.\n\n\n\n\nSelect the labeled image.\nGo to:\nPlugins → MorphoLibJ → Label Images → Remove Border Labels\nEnsure all borders are selected:\n\nLeft\nRight\nTop\nBottom\n\nClick OK\n\nObjects touching the image borders will be removed from the label image.\n\n\n\n\nWith the cleaned label image selected, go to:\nPlugins → MorphoLibJ → Analyze → Analyze Regions\nSelect the following measurements:\n\nArea\nPerimeter\nCircularity\n\nClick OK\n\nA Results table containing the selected morphometric measurements will appear.\n\n\n\n\nClick on the Results window.\nGo to:\nFile → Save As…\nSave the table as a CSV file, for example:\nBtd-cp000-Morphometry.csv*\n\n\n\n\n\nSelect the labeled image window.\nGo to:\nFile → Save As → Tiff…\nSave the image, for example as:\nBtd-cp000-lbl-killBorders.tif\n\nYou have now manually completed a full workflow including image preprocessing, segmentation, object filtering, and quantitative morphometric analysis.\nWhen clicking Create in the macro recorder, you will see a script like this:\n\n\n\nManual macro\n\n\n\n\n\n\n\nThe first line of code is to open the image, we will ignore this until the loop comes into place.\nLet’s get the title of the image to generalize the rest of the code. This will need to happen once the image is open.\n\n\n\n\n\n\nBefore\n\n\n\n\n\n\n\nAfter\n\n\n\n\n\nTry the macro. As result we have several open windows we would like to close before opening the next image in the folder. For this, we will use the command close. We can specify the name of the window we want to close by close('name'), close the current selected window close or close everything close(*), which is what we want in this case. Sadly, the Result windows are special windows that do not respond to this command, we do have to specify its name.\n\n\n\n\n\n\nBefore\n\n\n\n\n\n\n\nAfter\n\n\n\n\n\nTry the macro again. As result we get the processing done and no image open. Data will be saved in whichever directory we specified at the beginning.\n\n\n\n\n\nFirst we need to get the name of the source directory and get the file names inside. For this we will use the getDirectory and getFileList functions. I do recommend at this point to print the contents of the folder to make sure we are in the right place and for future debugging. We have to do this with a for loop, through all the values inside out file list:\n`sourceDir = getDirectory(“Source directory”); //select where images are\nfileList = getFileList(sourceDir); //list of files inside the sourceDir\nfor (i = 0; i &lt; fileList.length; i++) { //fileList.lenght is an integer print(fileList[i]); //element i inside fileList (starts counting from 0) }`\n\n\n\n\n\n\nBefore\n\n\n\n\n\n\n\nAfter\n\n\n\n\n\nRun the macro.\n\n\n\nRight now our log returns all the files inside the source directory, which now contains also the result of our analysis. It is best practice to create an Analysis folder, where data will be stored. In this way our raw data and our processed data remain separate. The easiest way is to create it ourselved by hand and use the getDirectory function, but you can also add an option in the code to create it if it does not exists already (safest).\noutputDir = sourceDir + \"/Analysis\"; //name of the saving folder within the iamge containing folder if (!File.exists(outputDir)) {      File.makeDirectory(outputDir); //creates folder if it does not exist already }\n\n\n\n\n\n\nBefore\n\n\n\n\n\n\n\nAfter\n\n\n\n\n\n\n\n\nNow let’s loop through the files ending with .tif and introduce the sourceDir, fileList and outputDir variables. Note that the fileName elements are identical to the titles, so we can change that variable.\n\n\n\n\n\n\nBefore\n\n\n\n\n\n\n\nAfter\n\n\n\n\n\nRun the code. At this point you should be seeing each image open individually, processing, and closing. In the Analysis folder you should get the output of the analysis.\nIf images are big and take time to open, we can activate BatchMode by includding the line of code setBatchmode(true) anywhere in the code (before looping through files and opening them).\nThe final version of the code is here:\n//Opens individual 2D membrane images from Drosophila-CarographicProjections\n//Segments, labels and quantifies area, perimeter and circularity of the cells\n//Saves labels and csv file in Analysis folder\n//Marina Cuenca 2026\n\nsourceDir = getDirectory(\"Source directory\"); //select where images are\n\nfileList = getFileList(sourceDir); //list of files inside the sourceDir\n\nfor (i = 0; i &lt; fileList.length; i++) { //fileList.lenght is an integer\n    print(fileList[i]); //element i inside fileList (starts counting from 0)\n}\n\noutputDir = sourceDir + \"/Analysis\"; //name of the saving folder within the iamge containing folder\nif (!File.exists(outputDir)) { \n    File.makeDirectory(outputDir); //creates folder if it does not exist already\n}\n\nsetBatchMode(true); //Won't show the images when open\n\nfor (i = 0; i &lt; fileList.length; i++) { //Loop through images\n    \n    title = fileList[i];\n    \n    if (!endsWith(title, \".tif\")) //only runs the code if file is an image .tif\n        continue;\n\n    open(sourceDir + \"/\" + title);\n    \n    selectImage(title);\n    \n    run(\"8-bit\");\n    run(\"Auto Local Threshold\", \"method=Otsu radius=15 parameter_1=0 parameter_2=0\");\n    run(\"Connected Components Labeling\", \"connectivity=4 type=[16 bits]\");\n    run(\"Remove Border Labels\", \"left right top bottom\");\n    run(\"Analyze Regions\", \"area perimeter circularity\");\n    \n    saveAs(\"Results\", outputDir + \"/\" + title + \"-Morphometry.csv\");\n    saveAs(\"Tiff\", outputDir + \"/\" + title + \"-lbl-killBorders.tif\");\n    \n    close(\"*\");\n    close( title + \"-Morphometry.csv\");\n    \n}\n\nprint(\"Done\");",
    "crumbs": [
      "Practicals",
      "Macro programming in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html",
    "href": "practicals/practical_cartography/index.html",
    "title": "Tissue cartography using Blender",
    "section": "",
    "text": "Tissue cartography is the art of projecting a tridimensional surface, usually of a biological sample, into a 2D surface. This is useful to visualize and analyze complex 3D microscopy data.\nThe state-of-the-art approach to generate cartographic projections was the MATLAB-based ImSAnE toolbox (Heemskerk and Streichan 2015). However, more recently, a Blender-based tool was released, known as Blender Tissue Cartography (Claussen et al. 2025) (code repository). This tool has an extensive documentation and several in-depth tutorials that I highly recommend anyone interested in tissue cartography to read.\nThis tutorial is a simplified version of the Blender Tissue Cartography tutorials. It is targeted to beginners wanting to learn the basics of tissue cartography, focusing on a single approach. It shows how to generate cartographic projections from 3D lightsheet microscopy data using Fiji to inspect data, ilastik to segment the tissues, and Blender with the Blender Tissue Cartography add-on to create the projections.\nIf you want to generate cartographic projections using the MATLAB-based ImSAnE toolbox (Heemskerk and Streichan 2015), please check the imsane-tutorial which explains how to set up and run the pipeline (Vellutini 2022).",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-summary",
    "href": "practicals/practical_cartography/index.html#sec-summary",
    "title": "Tissue cartography using Blender",
    "section": "",
    "text": "Tissue cartography is the art of projecting a tridimensional surface, usually of a biological sample, into a 2D surface. This is useful to visualize and analyze complex 3D microscopy data.\nThe state-of-the-art approach to generate cartographic projections was the MATLAB-based ImSAnE toolbox (Heemskerk and Streichan 2015). However, more recently, a Blender-based tool was released, known as Blender Tissue Cartography (Claussen et al. 2025) (code repository). This tool has an extensive documentation and several in-depth tutorials that I highly recommend anyone interested in tissue cartography to read.\nThis tutorial is a simplified version of the Blender Tissue Cartography tutorials. It is targeted to beginners wanting to learn the basics of tissue cartography, focusing on a single approach. It shows how to generate cartographic projections from 3D lightsheet microscopy data using Fiji to inspect data, ilastik to segment the tissues, and Blender with the Blender Tissue Cartography add-on to create the projections.\nIf you want to generate cartographic projections using the MATLAB-based ImSAnE toolbox (Heemskerk and Streichan 2015), please check the imsane-tutorial which explains how to set up and run the pipeline (Vellutini 2022).",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-requirements",
    "href": "practicals/practical_cartography/index.html#sec-requirements",
    "title": "Tissue cartography using Blender",
    "section": "Requirements",
    "text": "Requirements\n\nFiji (Schindelin et al. 2012)\nilastik v1.4.1.post1 (Berg et al. 2019)\nBlender v4.2.9 (Blender Foundation 2002)\nBlender Tissue Cartography (Blender add-on) (Claussen et al. 2025)\nDrosophila_CAAX-mCherry.tif dataset from Blender Tissue Cartography (available here) (Claussen et al. 2025)",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-setup",
    "href": "practicals/practical_cartography/index.html#sec-setup",
    "title": "Tissue cartography using Blender",
    "section": "Setup",
    "text": "Setup\n\nDownload Blender Tissue Cartography\n\nGo to https://github.com/nikolas-claussen/blender-tissue-cartography.\nPress the green button named Code &gt; Download ZIP to begin the download (or press here).\nUnzip the contents in your working directory.\nYou should see a new directory named blender-tissue-cartography.\nCopy the file Drosophila_CAAX-mCherry.tif located at blender-tissue-cartography/nbs/Tutorials/drosophila_example/ to your working directory.\n\n\n\nDownload Blender\n\nGo to https://download.blender.org/release/Blender4.2/.\nDownload Blender 4.2.9 (direct link for Linux/MacOS/Windows).\nUnzip the file into your working directory.\nYou should see a new directory named blender-4.2.9-linux-x64 (or similar for other systems).\n\nNote that older or newer versions of Blender might not work. Use v4.2.9 as it is known to work with this tutorial.\n\n\nInstall Blender Tissue Cartography\n\nOpen the directory blender-4.2.9-linux-x64.\nDouble-click the file blender to open the program.\nGo to Edit &gt; Preferences &gt; Add-ons &gt; Add-ons Settings (down arrow) &gt; Install from Disk....\nSelect the file blender_tissue_cartography-1.0.0-linux_x64.zip located in the directory blender-tissue-cartography/blender_addon/.\nClose Blender.\n\n\n\nDownload ilastik\n\nGo to https://www.ilastik.org/download.\nDownload ilastik 1.4.1.post1 (direct link for Linux/MacOS/Windows).\nUnzip the file into your working directory.\nYou should see a new directory named ilastik-1.4.1.post1-Linux.\n\n\n\nDownload Fiji\n\nGo to https://fiji.sc.\nChoose Distribution: Stable then click the big download button.\nUnzip the file into your working directory.\nYou should see a new directory named fiji-stable-linux64-jdk.",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-inspect-dataset",
    "href": "practicals/practical_cartography/index.html#sec-inspect-dataset",
    "title": "Tissue cartography using Blender",
    "section": "Inspect dataset in Fiji",
    "text": "Inspect dataset in Fiji\nBefore starting, let’s inspect the Drosophila_CAAX-mCherry.tif dataset in Fiji.\n\nOpen the directory fiji-stable-linux64-jdk/Fiji.app/ and double-click the fiji-linux-x64 launcher.\n\n\n\nDrag and drop Drosophila_CAAX-mCherry.tif in the Fiji window to open it.\nScroll through the Z slices of the stack.\n\n\n\nTo get more information, activate the orthogonal views with Image &gt; Stacks &gt; Orthogonal Views (or Ctrl+Shift+H).\n\n\n\n\n\n\n\n\n\n\n\nExplore the sample to understand well its shape. Try to figure out which side of the embryo is dorsal, which is ventral, and what is left/right. Also notice what are the characteristics of the tissues and of the background regions and think about the potential issues we might encounter with this dataset.\n\nOnce done, close the orthogonal views and stack (leave Fiji open).",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-segment-tissues",
    "href": "practicals/practical_cartography/index.html#sec-segment-tissues",
    "title": "Tissue cartography using Blender",
    "section": "Segment tissues in ilastik",
    "text": "Segment tissues in ilastik\nThe first step we need is to segment the stack in 3D to distinguish what is tissue and what is background. This is required to create a 3D mesh in Blender that has the shape of the sample. To accomplish that, we will use ilastik.\n\nOpen the directory ilastik-1.4.1.post1-Linux, right-click the file run_ilastik.sh, and select Run as a Program to open ilastik.\n\n\n\nCreate project\n\nMaximize the interface (we will need it).\nUnder Create New Project, click on Pixel Classification.\n\n\nThe window Create Ilastik Project will open.\n\nNavigate to your working directory and click Save.\n\n\nA file named MyProject.ilp will be created.\n\n\nInput Data\nThe ilastik interface is ready to define our input data.\n\nUnder the Raw Data tab, click on Add New... &gt; Add separate Image(s)....\n\n\n\nThen, select the file Drosophila_CAAX-mCherry.tif.\n\n\nilastik will open the dataset in three orthogonal views: XY (blue), XZ (green), YZ (red).\n\nNote, however, that the images are too dark; let’s fix this.\n\nRight-click the dataset row and select Edit properties....\nChange the value of Normalize Display to True and set the Range maximum value to 10000.\nPress OK.\n\n\n\n\n\n\n\n\n\n\n\nThe contrast will be much better now.\n\n\n\nFeature Selection\nNext we need to select the image features to take into account for the segmentation.\n\nOn the left column press 2. Feature Selection; the interface will update.\nThen press Select Features... to open the Features window.\nSelect all the features and press OK.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining\n\nOn the left column, click 3. Training.\n\nA new toolbox will open underneath showing two labels, Label 1 (yellow) and Label 2 (blue), buttons for paint or eraser modes, a size drop-down menu, and a Live Update button.\n\nNow is a good time to get familiar with the basic ilastik commands.\n\nScroll forward: go down through the slices of the orthogonal dimension.\nScroll backward: go up through the slices of the orthogonal dimension.\nCtrl+Scroll forward: zoom in.\nCtrl+Scroll backward: zoom out.\nMiddle-click and hold: drag view around.\n\nLearn how to zoom in/out, go through the slices, and drag the view. If we are able to zoom in significantly, and reach the top or bottom of a view by dragging, we are ready for painting.\n\nOur goal is to paint tissues in yellow and background in blue. But to accomplish that, we only need a few strokes at the right regions of the image.\n\nBegin by zooming in at the top region of the XZ (green) view.\nSelect Label 2 (blue), change the size to 7, and paint a line right above the tissue.\nNow select Label 1 (yellow) and paint the tissue immediately below the blue line.\n\n\nThese two simple lines are indicating to ilastik that all the image features in this region very close to the tissue correspond to “background”, and that the image features of the tissue below correspond to “foreground”. Putting the two lines adjacent to each other also helps ilastik to understand where the boundaries are.\n\nSince we want all of the tissue to be segmented (and not just the surface), also paint a line until the center of the embryo.\n\nThis is enough to get started. Based on these simple strokes, ilastik will learn and apply this to the entire dataset.\n\nTo start the training, press the button Live Update.\n\n\nilastik will overlay the current segmentation model over the image. We should see that most of the tissue regions are yellow and that the region around the embryo is more blue. The more vivid the color, the more confident the model is about that specific region.\nNow start painting with simple strokes areas which are wrong or pale. For example the corners of the images are background and should be blue; any area inside the sample should be yellow; use different brush sizes if needed; or the eraser. This sample also has giant, super bright beads; they are not tissue, we want them blue.\nNote that the segmentation model and overlay colors update upon each stroke, so we can see if what we did improved or worsened the segmentation. If it got worse, we can always erase the annotation.\n\nBe meticulous and pay special attention to the edges of the image. We do not want tissue (yellow) to be touching the border, because this will create a hole in the segmentation. The better the segmentation is, the better will be our visualization and cartographic projection.\nThere are ways that we can fix segmentation issues after converting it to a mesh, but they will not be covered in this tutorial. So, for this image, it is important to take care of the tip of the very top part of the sample because it is touching the edge.\n\nUse a size 1 brush to place a couple of blue lines at the very top.\n\n\n\nGo through the slices in each of the orthogonal views to fix any leftover segmentation uncertainties.\n\n\nThe segmentation overlay should be showing clearly separated yellow and blue regions that match the embryo and background.\n\n\nPrediction Export\nWe can now export the segmentation prediction.\n\nOn the left column, click on 4. Prediction Export.\nUnder Export Settings keep the Source value as Probabilities.\n\n\n\nThen, press Choose Export Image Settings... to open the Image Export Options window.\n\n\nThere are two options that we need to change.\n\nUnder Cutout Subregion uncheck the row c (for channels) and change the stop value to 1.\n\nSince this image has only one channel, changing this option avoids loading a duplicated channel into Blender.\n\nUnder Output File Info change the value of Format to multipage tiff.\n\nThis is required to be able to load the segmentation into Blender.\n\nPress OK to close the window.\nThen press the Export All button and wait…\n\nWhen the prediction is done, a new file will appear in the working directory named Drosophila_CAAX-mCherry_Probabilities.tiff.\n\n\nOpen this file in Fiji to see how it looks before our next step in the tutorial.\n\n\n\nNow close the file and let’s start with generating the actual projection.",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-import-blender",
    "href": "practicals/practical_cartography/index.html#sec-import-blender",
    "title": "Tissue cartography using Blender",
    "section": "Import data to Blender",
    "text": "Import data to Blender\nWe can now import the image stack and segmentation probabilities into Blender.\n\nOpen Blender\n\nEnter the directory blender-4.2.9-linux-x64 and double-click the file blender.\n\n\nBlender will open with a nice splash screen at the center.\n\nClick anywhere to close it.\n\n\nNotice at the top left region that we are in the Layout tab (important for later). There’s a gray cube at the center, we want to get rid of it.\n\nIn the top right panel under Scene Collection &gt; Collection, right-click the Cube line and select Delete.\n\n\nGreat. Let’s focus now at the bottom right corner, it is busy, full of icons and menus. Don’t get overwhelmed, we only need to select and use one of the modes.\n\nIf not yet selected, click on the Scene icon (it is the white triangle with two circles) to activate this panel.\nThen locate the tab named Tissue Cartography at the bottom.\n\n\n\nScroll down and make the side panels wider to be able to read the options of Tissue Cartography.\n\n\nThis is the main interface of the Blender Tissue Cartography add-on. It is through here that we will control most of the steps of this pipeline.\n\n\nLoad sample\nThe first thing we need to do is to load the sample.\n\nClick on the folder icon in the Tissue Cartography &gt; File Path, navigate to the working directory, and select the file Drosophila_CAAX-mCherry.tiff.\n\nTip: bookmark the directory for easy access in the future.\n\nClick Accept.\n\n\n\n\n\n\n\n\n\n\n\n\nThen press Load .tiff file to load it into Blender.\n\n\nA new row will appear at the top right panel under Scene Collection &gt; Collection named Drosophila_CAAX-mCherry_BoundingBox and the bounding box of the image stack will be visible in the main window as orange lines.\nOnly a portion of the bounding box is visible, but we want to see the whole thing. The controls to navigate the 3D space are at the top right corner of the main window. We have XYZ handles (red, green, blue), a zoom tool (magnifier lens), and a move tool (open hand).\n\nClick, hold, and drag any of these to move around.\nClicking on the X, Y, or Z will reorient the sample along these axes (very useful).\n\n\nTake some time to practice and finish by placing the bounding box at the center of the main window as in the image below.\n\n\nLoad probabilities\nNow let’s load the probabilities files.\n\nClick on the folder icon of the Tissue Cartography &gt; Segmentation File Path, navigate to the working directory, and select the file Drosophila_CAAX-mCherry_Probabilities.tiff.\nClick Accept.\n\n\nThe file name will appear in the field, but before loading we want to adjust one parameter. Blender will take the segmentation probabilities and convert it into a tridimensional mesh. Generally, the raw segmentation is full of sharp angles, which will not look very nice when we map the image information onto the mesh for visualization. Therefore, it is generally a good idea to apply a degree of smoothing upon importing the segmentation.\nWe can control the smoothing in the small field below and to the right of Segmentation File Path named S... 0.00. It should read Smoothing (µm) but the panel is too narrow to show the full name.\n\nClick on it and set it to 1.0.\n\n\n\n\n\n\n\n\n\n\n\n\nNow click on Get mesh(es) from binary segmentation .tiff file(s) to generate the mesh (it takes a second).\n\n\nA gray mesh shaped like our sample will appear inside the bounding box in the main window. Also notice that a new row appeared at the top right panel under Scene Collection &gt; Collection named Drosophila_CAAX-mCherry_Probabilities_c0.\nCongratulations! We have successfully generated a 3D mesh of the sample. That’s already a powerful visualization method. Celebrate by exploring the sample. Rotate all around, zoom in to see details, and check how good the mesh is. Are there any holes or other artifacts?\n\n\nApply shading\nThe mesh is nice, but it would be even better to see the actual image data overlaid on the mesh. We can accomplish that using the shading view and function of Blender.\n\nFirst, we need to activate the Shading workspace at the top right corner of the main window (an icon that looks like a pie chart).\n\n\nOnce clicked, the mesh will become almost white.\n\nBefore applying the shading, there’s one important parameter to set: Vertex ... 0.00 or, in full, Vertex Shader Normal Offset (µm).\n\nWhen this parameter is 0, the image data that corresponds to the limits of the segmentation is applied onto the mesh. In this case, this is the surface of the sample which, in this case, does not have much information. The fluorescent signal of the tissue, in this case, is a few microns deeper. Therefore, we can use the offset parameter to adjust the exact layer to be applied to the mesh as shading.\n\nIn this case, a value of 5 works well.\n\n\nEvery time we want to apply or refresh the shading, we need to select the bounding box and probabilities entries in the Scene Collection &gt; Collection top right panel.\n\nWe can do so by clicking on one and Ctrl+Click on the other to select both.\n\n\n\nFinally, press Initialize/refresh vertex shading to apply the shading and wait…\n\n\nAfter a few seconds, we should see cell membranes overlaid onto the mesh.\n\nTake the chance to explore the sample again, now with some biological information projected into 3D!",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-generate-projection",
    "href": "practicals/practical_cartography/index.html#sec-generate-projection",
    "title": "Tissue cartography using Blender",
    "section": "Generate projection",
    "text": "Generate projection\nWe are ready to generate our first cartographic projection. This is done in the UV Editing workspace.\n\nClick on the UV Editing tab to activate it.\n\n\nThe workspace will be divided in two. On the left we will see the projected mesh and on the right it is the mesh of the original sample in 3D.\n\nSet up UV Editing\nBefore starting, we need to make sure that the correct options are enabled.\n\nFirst, zoom out the right side window for the entire sample to be visible.\n\n\n\nThen, disable the bounding box entry in the top right panel under Scene Collection &gt; Collection by Ctrl+Click on the square nodes symbol on the left of the bounding box entry as shown below.\n\n\n\n\n\n\n\n\n\n\n\n\nNext, change the Select Mode from Vertex to Face.\n\n\n\n\n\n\n\n\n\n\n\nIn the right hand workspace, we should see the sample (not the bounding box) highlighted in light orange.\n\nWe might accidentally click somewhere and the mesh will become black.\n\nIf this happens, worry not!\n\nSimply press A or click on Select &gt; All to select all the mesh.\n\nTo finish the setup, we need to orient the sample properly for the projection, it matters!\n\nClick on the Y axis handle until the sample is oriented vertically with the narrower tip pointing upward.\n\n\nWe are ready to project the mesh.\n\n\nProject mesh\nWhile the right side shows the sample, the left side shows how the projected mesh looks like; it is initially empty.\n\nTo make the first projection, go to UV &gt; Cylinder Projection to project the mesh over the curved wall of a cylinder.\n\n\nA crazy, palisade-like wall will appear.\n\nBut don’t despair.\n\nClick on the tiny menu named Cylinder Projection that appeared at the bottom of the workspace.\n\nThe options for the cylinder projection will appear. There we can define the orientation of the axes and other options to change how the 3D mesh is transformed into a 2D surface. What we need for now is to contain the projection into the squared bounding area.\n\nActivate the checkbox Scale to Bounds.\n\n\nThis will nicely limit the mesh to the projection area.\nThat’s it. We have our first projected mesh. How good is it? Ideally, the mesh should occupy the entire projection area. Our projected mesh has a couple of portions slightly bulging outside the area, and we have an empty vertical portion on the right side. This could be fixed with some editing. However, for now, it looks good enough for a first try.\n\n\nProject data\nThe next step is to project the actual image data onto this projected mesh surface.\n\nChange back to the Layout workspace, select both Drosophila_CAAX-mCherry_BoundingBox and Drosophila_CAAX-mCherry_Probabilities_c0 under Scene Collection &gt; Collection, and change the option Normal Offs... (Normal Offsets (µm)) to 5 to match the Vertex Shader Normal Offset (µm) option.\n\nNote: Normal Offsets (µm) accepts a comma-separated list of values. We can put 0,1,2,3,4,5 to generate a projection with 6 slices representing onion layers deeper into the tissues.\n\nThen, click on Create Projection.\n\n\nWait… the interface might become unresponsive. If a dialog appears, click on Wait and wait. When done, the shading over the sample will blink. But, it should look very similar to how it was before (and if it doesn’t, it is a sign that something went wrong).\n\n\nCheck projection\n\nTo visually inspect the projected data, change back to the UV Editing workspace.\n\n\n\nZoom out on the left side window to see the entire projection.\nThen click anywhere outside the sample or projection to unselect the mesh.\n\n\nThe sample mesh will become visible in black, and the cartographic projection should appear on the left side window. The orientation of the sample will match that of the projection (if the sample is upside down when projecting the mesh, the projected mesh will also be upside down).\nSo, what happened here? We projected the sample mesh to 2D using the cylinder approach. Then the add-on Blender Tissue Cartography used this projected mesh to create a projection of the image data from the original stack. Quite nice!\nEvery time we create a new projection, the projected data is stored as an image which is available for Blender to display as an image “data-block”. We can see them by clicking on the picture icon in the top menu.\n\nThis first projection is named Channel_0_Layer_0. The next will be named Channel_0_Layer_0.001, Channel_0_Layer_0.002, and so on.\n\n\nSave projection\nThe projection now exists in Blender, but we need to export it to file.\n\nFor that, go back to the Layout workspace, select both Drosophila_CAAX-mCherry_BoundingBox and Drosophila_CAAX-mCherry_Probabilities_c0 under Scene Collection &gt; Collection again, then click on Save Projection.\n\n\nA Blender File View window will open.\n\nNavigate to the working directory.\nIn the file name field put the name of the file with _cylinder appended to it, to read Drosophila_CAAX-mCherry_cylinder.tif.\nThen press Save Projection.\n\nThree new files will appear in the working directory with BakedData, BakedNormals, and BakedPositions suffixes appended to the dataset filename.\n\n\nBakedData shows the original image data projected on the surface.\nBakedPositions shows the original XYZ positions projected on the surface in RGB.\nBakedNormals shows the XYZ surface directions perpendicular to each point in RGB.\n\n\n\nOpen projection\nLet’s open the files in Fiji for inspection.\n\nDrag and drop the files into Fiji.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore them in more detail, check with Image &gt; Color &gt; Channels Tool... (Ctrl+Shift+Z) how the individual channels look like. These files provide important information to reconstruct back the 3D information from the projected surface in downstream analyses.",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-optimize-projection",
    "href": "practicals/practical_cartography/index.html#sec-optimize-projection",
    "title": "Tissue cartography using Blender",
    "section": "Optimize projection",
    "text": "Optimize projection\nOur initial projection is satisfactory, but there are many ways to optimize it to our specific needs. One immediate thing is to try a Sphere Projection instead of Cylinder Projection. They are quite similar, but I found the sphere projection to be more consistent and predictable and might work better for more spherical samples.\nAnother common use case is to be able to determine where the mesh will unwrap. This might be important for downstream analyses. In our sample, for example, the projection put the dorsal side on the left side (where a clump of germ cells are visible at the bottom) and the ventral side on the right side. However, let’s say that I need for my analyses the dorsal side at the center of the projection.\nTo accomplish that we can manually mark a seam on our mesh to define the unwrapping position.\n\nMark seam on mesh\n\nGo to the UV Editing workspace and change the Select Mode to Edge select.\n\n\nIn this mode, when we click on the mesh an edge is selected and when we subsequently Ctrl+Click on another edge, the shortest path between the two edges will be selected. Like this we can quickly select a line along the sample to mark the unwrapping position. What we want is to trace a line through the ventral side of the sample. This is the region opposite to the germ cell clump.\n\nUsing the handle buttons, reorient the sample sideways with the ventral side facing us.\n\n\nWe will start by selecting an edge on one of the poles.\n\nTurn the sample to show one of the poles and zoom in to see the edges clearly.\nThen click on one edge at the center of the pole to select it.\n\n\n\nZoom out slightly, turn the sample, and Ctrl+Click on another edge further away from the pole.\nA yellow line will appear connecting the pole with the current edge.\n\n\n\nContinue Ctrl+Click on edges along the sample until the opposite pole (don’t worry if the line isn’t perfectly straight).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow press Ctrl+E to open a menu with edge options and select Mark Seam.\n\n\nThe seam will be marked in red (below it’s mixed with yellow line from the edge selection).\n\n\n\nProject mesh with seam\n\nNow reorient the sample vertically again with the narrow tip up and change the Select Mode back to Face select.\n\n\n\nPress A to select all the mesh (they will turn orange).\nGo to UV &gt; Cylinder Projection.\nThen check Preserve Seams in the option box.\n\n\nAs we can see, the projection changed. It is squeezed at the center of the projection area due to the very long protruding mesh at the top left and bottom right regions. Let’s evaluate how good it is by projecting the data.\n\n\nProject data with seam\n\nGo to the Layout workspace, select both the bounding box and probabilities under Scene Collection, and press Create Projection; then, wait…\n\n\n\nWhen done, go back to the UV Editing workspace and click anywhere outside the sample to unselect the mesh.\n\nThe new data projection should be visible on the left side.\n\nIf not, click on the image data-block picture icon and select Channel_0_Layer_0.001.\n\n\nWe have successfully changed the position of the unwrapping using the seam. The clump of germ cells is now at the center of the projection.\nThis projection is OK, but the contents are squeezed and it is not occupying the full bounding area. We can improve this by editing the projection mesh.\n\n\nEdit projected mesh\n\nPress A to select the entire mesh.\nSelect the tool Transform on the left side menu.\n\n\n\nHover the mouse on the right side edge to reveal the scale handle.\n\n\n\nDrag the right side to the right to extend the orange mesh until the edge of the projection area.\nThen, drag the other side to extend the mesh to the left side.\nFinally, make adjustments so that the projected mesh is covering most of the projection area as shown below.\n\n\n\n\n\n\n\n\n\n\n\nNow we can use the Pinch or Grab tools to edit the mesh at the corners, so that they are not clipped out. Or, we can leave them as is (they will be clipped out of the projection).\n\nUse Pinch to drag finer portions of the mesh to the corners.\nUse Grab to drag larger portions of the mesh to the corners.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt the end, the mesh should be filling more or less the projection area.\n\n\nCheck edited projection\nTo check the new projection with the edited mesh, we need to re-create the image data projection onto the edited mesh.\n\nGo to the Layout workspace, select both the bounding box and probabilities under Scene Collection, and press Create Projection; then, wait…\nWhen done, switch to the UV Editing workspace and deselect the mesh and check the new projection (select the latest image data-block, likely Channel_0_Layer_0.002).\n\n\nDespite the unevenness of the corners (next time we can improve our pinching and grabbing skills), the edited projection is better than the first one and the tissue is oriented the way we needed. Let’s save the projection to disk.\n\nGo to the Layout workspace, select both the bounding box and probabilities entries under Scene Collection &gt; Collection, and click on Save Projection.\nThen navigate to the working directory and give the file a different suffix.\n\n\n\nFinally, open the newly generated files in Fiji to investigate.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe projection is ready for image analyses.\nThere are several other use cases that are not covered in the current version of this tutorial. For example, how can we generate a projection with more layers, or create and register projections for different timepoints. But, if you are interested, these use cases are described in the Blender Tissue Cartography paper and documentation (Claussen et al. 2025).",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-citation",
    "href": "practicals/practical_cartography/index.html#sec-citation",
    "title": "Tissue cartography using Blender",
    "section": "Citation",
    "text": "Citation\nVellutini, B. C. (2026). Tissue cartography using Blender. Zenodo. https://doi.org/10.5281/zenodo.18090965",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-license",
    "href": "practicals/practical_cartography/index.html#sec-license",
    "title": "Tissue cartography using Blender",
    "section": "License",
    "text": "License\nThis tutorial is available under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-references",
    "href": "practicals/practical_cartography/index.html#sec-references",
    "title": "Tissue cartography using Blender",
    "section": "References",
    "text": "References\n\n\nBerg, Stuart, Dominik Kutra, Thorben Kroeger, Christoph N Straehle, Bernhard X Kausler, Carsten Haubold, Martin Schiegg, et al. 2019. “ilastik: Interactive Machine Learning for (Bio)image Analysis.” Nat. Methods, September. https://doi.org/10.1038/s41592-019-0582-9.\n\n\nBlender Foundation. 2002. “Blender - the Free and Open Source 3D Creation Software.” Computer software. https://www.blender.org/.\n\n\nClaussen, Nikolas, Cécile Regis, Susan Wopat, and Sebastian Streichan. 2025. “Blender Tissue Cartography: An Intuitive Tool for the Analysis of Dynamic 3D Microscopy Data.” bioRxiv, July, 2025.02.04.636523. https://doi.org/10.1101/2025.02.04.636523.\n\n\nHeemskerk, Idse, and Sebastian J Streichan. 2015. “Tissue Cartography: Compressing Bio-Image Data by Dimensional Reduction.” Nat. Methods 12 (December): 1139–42. https://doi.org/10.1038/nmeth.3648.\n\n\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. “Fiji: An Open-Source Platform for Biological-Image Analysis.” Nat. Methods 9 (June): 676–82. https://doi.org/10.1038/nmeth.2019.\n\n\nVellutini, Bruno C. 2022. How to Make Cartographic Projections Using ImSAnE. Zenodo. https://doi.org/10.5281/zenodo.7628299.",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html",
    "href": "practicals/practical_2d/index.html",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "What is Fiji?\nGetting Started with Fiji\nOpening and Viewing Images\nEssential Fiji Functions\nAlternative: Napari in Jupyter Notebooks\n\n\n\n\n\nFiji (Fiji Is Just ImageJ) is a free, open-source image processing and analysis platform built on top of ImageJ. It’s widely used in scientific research, particularly in microscopy, medical imaging, and general image analysis.\n\n\n\nUser-friendly GUI - No programming required for basic operations\nPre-installed plugins - Comes with a curated selection of image processing tools\nExtensible - Can add additional plugins for specialized tasks\nMulti-format support - Handles TIFF, JPEG, PNG, BMP, and many scientific formats\nBatch processing - Automate workflows with macros\nActive community - Extensive documentation and forum support\n\n\n\n\n\nMicroscopy image analysis\nSegmentation and thresholding\nIntensity measurements and statistics\nImage registration and stitching\n3D image visualization\nTime-series analysis\n\n\n\n\n\n\n\n\n\n\n\nVisit the official Fiji website: https://fiji.sc/\nClick “Download” and select your operating system\nExtract the downloaded file to your preferred location (preferably somewhere admin rights are not required to modify files)\nDouble-click Fiji.exe (Windows) or the Fiji application bundle to launch\n\nNo additional installation required - Fiji is portable and comes with Java bundled.\n\n\n\nWhen you launch Fiji, you should see: - The main ImageJ window (toolbar with various tools) - A menu bar with File, Edit, Image, Process, Analyze, Plugins, Window, Help\n\n\n\nFiji window\n\n\n\n\n\n\nFor this workshop, we recommend having sample images available. You can: - Use your own microscopy images or photographs - Use Fiji sample data - Download sample images from: - Fiji sample images - Scientific image repositories (e.g., Open Microscopy Environment)\n\n\n\n\n\n\n\nThe easiest way is to use drag-and-drop on the gray area.\nAnother option is to:\n\nFile → Open (or Ctrl+O / Cmd+O)\nNavigate to your image file and select it\nClick Open\n\nThe image will display in a new window with the filename in the title bar.\nIn many cases, the BioFormats Plugin will be automatically called to open the image.\n\n\n\nBioFormats Plugin\n\n\nHow many of these options do we understand?\n\n\n\n\n\n\n\n\n\n\n\nTool\nFunction\nShortcut\n\n\n\n\nZoom In/Out\nAdjust magnification to inspect details or see whole image\n+ / - or scroll wheel\n\n\nPan\nMove around the image when zoomed in\nClick and drag with spacebar\n\n\nColor Picker\nSample pixel values\nI key\n\n\nMeasure\nDetermine distances and areas\nM key\n\n\nROI (Region of Interest)\nSelect rectangular, oval, or free-form areas\nToolbar buttons\n\n\n\n\n\nZoom in as much as you can into a small region. What do you see? Are the squares part of the sample or a rendering? If you hover your mouse over some pixels, what do you see at the bottom grey area of the Fiji main window?\n\n\n\n\nAfter opening an image, first check its header. What are the values on top?\nCan you check its metadata? (go to Image&gt;Show Info...)\n\n\n\nShow Info Window in Fiji\n\n\nFill in the following table:\n\n\n\nMetadata Item\nValue\n\n\n\n\nPixel Size (µm)\n\n\n\nMagnification\n\n\n\nNumerical Aperture (NA)\n\n\n\nBit Depth\n\n\n\nImage Dimensions (width × height)\n\n\n\nNumber of Channels\n\n\n\nNumber of Slices (if z-stack)\n\n\n\nVoxel Dimensions\n\n\n\nUnit\n\n\n\n\n\n\n\nImages can be stored in different formats representing different levels of intensity information:\n\n8-bit (256 intensity levels) - Smallest file size, fastest processing, good for display\n16-bit (65,536 intensity levels) - More detail, larger file size, common in microscopy\n32-bit (floating-point) - Highest precision, largest file size, used for calculations\n\nTo change the bit depth of your image:\n\nGo to Image &gt; Type\nSelect your desired bit depth: 8-bit, 16-bit, or 32-bit\nIf converting from higher to lower bit depth (e.g., 16-bit to 8-bit), you may lose information\n\n\n\nWhy might you want to convert a 16-bit image to 8-bit? Consider factors like file size, processing speed, memory usage, and the type of analysis you plan to do. When would it be a bad idea?\n\n\n\n\n\n\n\n\nMost of the operations are applied on the currently selected image, which might change its intensity values. Some operations cannot be undone, which means that you have to reopen the image and go through the whole process again if something goes wrong. To avoid this, it’s a good idea to duplicate the image just in case.\nGo to Image &gt; Duplicate.\n\n\n\n\nImage &gt; Adjust &gt; Brightness/Contrast...\nDrag sliders to enhance visibility\nCheck Auto for automatic adjustment\nUse Reset to revert to original\n\n\n\n\nBrightness and Contrast window\n\n\nUse case: Make dim images more visible for analysis.\n\n\nTry clicking on Apply to see what happens to the pixel values.\n\n\n\n\nMost of the cameras or detectors are monochromatic, which means that the color with which images are represented is a false color. Go to Image &gt; LookUp Tables or click on the LUT button and choose a different color. What do you think the different palettes are for? Try for example HiLo, cool and 16-colors.\n\n\n\nFirst choose an image with several dimension, like a z-stack with more than one channel. If you don’t have one, you can choose File &gt; Open samples &gt; Mitosis (5D stack).\n\n\n\n5D stack image\n\n\nThe sliders at the bottom allow you to go through the different channels, z-planes and timepoints. Some of the sample images already have what is called a composite, where channels are shown simultaneously. Try doing the following:\n\n\n\nImage &gt; Color &gt; Split Channels\nEach channel opens in a separate window\nAnalyze or process channels independently\n\n\n\n\n\nImage &gt; Color &gt; Merge Channels\nSelect source windows for each channel\nDo not create a composite image\n\n\n\n\n\nAdjust color assignments for better visualization\nGo to Image &gt; Color &gt; Make Composite\n\n\n\n\n\nChoose any structure in your image.\n\nClick on straight line button\nDraw a line across your object of interest\nGo to Analyze &gt; Plot Profile\nDisplays intensity values along the line (useful for colocalization studies)\n\n\n\n\nLine profile image\n\n\n\n\n\nPlot Profile image\n\n\nWhat happens if you right click on the straight line button?\n\n\n\nSelect an ROI (Region of Interest) using selection tools\nAnalyze → Measure (or Ctrl+M)\nResults include: Area, Mean intensity, Std Dev, Min, Max\n\n\n\n\n\n\nUse rectangle selection tool to define region\nImage → Crop\nImage is reduced to selected area\n\n\n\n\n\n\nIf you prefer a Python-based, interactive approach, napari is a fast, interactive image viewer for multi-dimensional images.\n\n\nPixi is a package manager that makes it easy to manage Python environments and dependencies. Follow these steps to set up Napari with pixi:\n\n\n\nVisit the official Pixi website: https://pixi.sh/\nDownload and install Pixi for your operating system (Windows, Mac, or Linux)\nVerify installation by running in your terminal:\npixi --version\n\n\n\n\n\nCreate a new directory for your image analysis project:\nmkdir my-image-analysis\ncd my-image-analysis\nInitialize a Pixi project:\npixi init\nThis creates a pixi.toml configuration file for your project.\n\n\n\n\nAdd Python, napari, and Jupyter to your project:\npixi add python napari jupyter scikit-image\nThis command: - Installs Python and the specified packages - Creates a pixi.lock file with exact versions for reproducibility - Sets up an isolated environment specific to your project\n\n\n\nStart Jupyter notebook directly from your pixi project:\npixi run jupyter notebook\nThis runs Jupyter within your project’s environment, giving it access to napari, scikit-image, and all other packages you added.\n\n\n\n\n\n\nimport napari\nfrom skimage import io\n\n# Load an image\nimage = io.imread('path/to/image.tif')\n\n# Create viewer and add image layer\nviewer = napari.view_image(image, name='raw image')\n\n\n\nWe can do this either programatically from within the Jupyter notebook, or we can do this interactively from the Napari GUI.\nimport napari\nfrom skimage import io\n\nimage = io.imread('path/to/image.tif')\n\nviewer = napari.Viewer()\nlayer = viewer.add_image(image)\n\n# Add shapes layer for ROI drawing\nshapes_layer = viewer.add_shapes(\n    shape_type='rectangle',\n    edge_color='cyan',\n    face_color='transparent',\n    name='ROIs'\n)\nOr:\n\nClick on the button to generate a new Shapes Layer\nAdd a new rectangle\n\nOnce the rectangle is made, try accessing it via a cell in the Jupyter notebook. Access drawn shapes via shapes_layer.data.\n\n\n\n\n\nInteractive - Real-time parameter adjustment\nJupyter integration - Works in notebooks for reproducible workflows\nPython-native - Easy integration with scientific Python stack (scikit-image, scipy, etc.)\nMulti-dimensional - Handles time series, z-stacks, and multi-channel data\nProgrammatic ROI - Extract measurements from selected regions in code\n\n\n\n\n\nLess pre-built analysis functions (requires coding)\nSmaller ecosystem of plugins compared to Fiji\nLearning curve for Python programming\n\n\n\n\n\n\n\n\n\nDownload a sample image\nOpen in Fiji\nCheck image properties (dimensions, bit depth, channels)\nUse zoom and pan tools to explore\nMeasure a feature using the measure tool\n\n\n\n\n\nOpen a dim or low-contrast image\nApply Brightness/Contrast adjustment\nApply Gaussian blur to reduce noise\nUse Unsharp Mask to enhance edges\nCompare original and processed versions\n\n\n\n\n\nOpen a 2D image with distinct objects\nApply threshold to create binary image\nUse morphological operations (Erode/Dilate) to clean up\nRun Analyze Particles to count and measure objects\nGenerate summary statistics\n\n\n\n\n\nLoad an image using scikit-image\nApply preprocessing (blur, threshold)\nPerform segmentation\nVisualize results with multiple layers in napari\nExtract measurements from labeled regions\n\n\n\n\n\n\n\n\n\nFor very large images, reduce memory usage via Edit → Options → Memory & Threads\nDisable auto-update of image window to speed up processing: Edit → Options\nUse 8-bit images instead of 16-bit when precision isn’t critical\n\n\n\n\n\nDocument your workflow by recording macro actions: Plugins → Macros → Record\nExport processing steps as macro code for batch processing\n\n\n\n\n\nSave images and results to standardized output directories\nUse consistent naming conventions for variables\nDocument parameters used for each processing step\nVersion your analysis notebooks in git\n\n\n\n\n\n\n\nOfficial Fiji Documentation: https://fiji.sc/\nImageJ User Guide: https://imagej.net/\nNapari Documentation: https://napari.org/\nScientific Image Analysis Tutorials: https://imagej.net/Tutorials\nScikit-image (Python): https://scikit-image.org/\n\n\n\n\n\nCorbat, A. A. (2026). Workshop on 2D Image Visualization: Fiji and Napari. Zenodo. https://doi.org/10.5281/zenodo.18199646\n\n\n\n\nThis work is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0).\nFor more details, visit: https://creativecommons.org/licenses/by/4.0/ and https://github.com/acorbat/2D_visualization_workshop\n\nLast Updated: January 2, 2026",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html#table-of-contents",
    "href": "practicals/practical_2d/index.html#table-of-contents",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "What is Fiji?\nGetting Started with Fiji\nOpening and Viewing Images\nEssential Fiji Functions\nAlternative: Napari in Jupyter Notebooks",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html#what-is-fiji",
    "href": "practicals/practical_2d/index.html#what-is-fiji",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "Fiji (Fiji Is Just ImageJ) is a free, open-source image processing and analysis platform built on top of ImageJ. It’s widely used in scientific research, particularly in microscopy, medical imaging, and general image analysis.\n\n\n\nUser-friendly GUI - No programming required for basic operations\nPre-installed plugins - Comes with a curated selection of image processing tools\nExtensible - Can add additional plugins for specialized tasks\nMulti-format support - Handles TIFF, JPEG, PNG, BMP, and many scientific formats\nBatch processing - Automate workflows with macros\nActive community - Extensive documentation and forum support\n\n\n\n\n\nMicroscopy image analysis\nSegmentation and thresholding\nIntensity measurements and statistics\nImage registration and stitching\n3D image visualization\nTime-series analysis",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html#getting-started-with-fiji",
    "href": "practicals/practical_2d/index.html#getting-started-with-fiji",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "Visit the official Fiji website: https://fiji.sc/\nClick “Download” and select your operating system\nExtract the downloaded file to your preferred location (preferably somewhere admin rights are not required to modify files)\nDouble-click Fiji.exe (Windows) or the Fiji application bundle to launch\n\nNo additional installation required - Fiji is portable and comes with Java bundled.\n\n\n\nWhen you launch Fiji, you should see: - The main ImageJ window (toolbar with various tools) - A menu bar with File, Edit, Image, Process, Analyze, Plugins, Window, Help\n\n\n\nFiji window\n\n\n\n\n\n\nFor this workshop, we recommend having sample images available. You can: - Use your own microscopy images or photographs - Use Fiji sample data - Download sample images from: - Fiji sample images - Scientific image repositories (e.g., Open Microscopy Environment)",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html#opening-and-viewing-images",
    "href": "practicals/practical_2d/index.html#opening-and-viewing-images",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "The easiest way is to use drag-and-drop on the gray area.\nAnother option is to:\n\nFile → Open (or Ctrl+O / Cmd+O)\nNavigate to your image file and select it\nClick Open\n\nThe image will display in a new window with the filename in the title bar.\nIn many cases, the BioFormats Plugin will be automatically called to open the image.\n\n\n\nBioFormats Plugin\n\n\nHow many of these options do we understand?\n\n\n\n\n\n\n\n\n\n\n\nTool\nFunction\nShortcut\n\n\n\n\nZoom In/Out\nAdjust magnification to inspect details or see whole image\n+ / - or scroll wheel\n\n\nPan\nMove around the image when zoomed in\nClick and drag with spacebar\n\n\nColor Picker\nSample pixel values\nI key\n\n\nMeasure\nDetermine distances and areas\nM key\n\n\nROI (Region of Interest)\nSelect rectangular, oval, or free-form areas\nToolbar buttons\n\n\n\n\n\nZoom in as much as you can into a small region. What do you see? Are the squares part of the sample or a rendering? If you hover your mouse over some pixels, what do you see at the bottom grey area of the Fiji main window?\n\n\n\n\nAfter opening an image, first check its header. What are the values on top?\nCan you check its metadata? (go to Image&gt;Show Info...)\n\n\n\nShow Info Window in Fiji\n\n\nFill in the following table:\n\n\n\nMetadata Item\nValue\n\n\n\n\nPixel Size (µm)\n\n\n\nMagnification\n\n\n\nNumerical Aperture (NA)\n\n\n\nBit Depth\n\n\n\nImage Dimensions (width × height)\n\n\n\nNumber of Channels\n\n\n\nNumber of Slices (if z-stack)\n\n\n\nVoxel Dimensions\n\n\n\nUnit\n\n\n\n\n\n\n\nImages can be stored in different formats representing different levels of intensity information:\n\n8-bit (256 intensity levels) - Smallest file size, fastest processing, good for display\n16-bit (65,536 intensity levels) - More detail, larger file size, common in microscopy\n32-bit (floating-point) - Highest precision, largest file size, used for calculations\n\nTo change the bit depth of your image:\n\nGo to Image &gt; Type\nSelect your desired bit depth: 8-bit, 16-bit, or 32-bit\nIf converting from higher to lower bit depth (e.g., 16-bit to 8-bit), you may lose information\n\n\n\nWhy might you want to convert a 16-bit image to 8-bit? Consider factors like file size, processing speed, memory usage, and the type of analysis you plan to do. When would it be a bad idea?",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html#essential-fiji-functions",
    "href": "practicals/practical_2d/index.html#essential-fiji-functions",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "Most of the operations are applied on the currently selected image, which might change its intensity values. Some operations cannot be undone, which means that you have to reopen the image and go through the whole process again if something goes wrong. To avoid this, it’s a good idea to duplicate the image just in case.\nGo to Image &gt; Duplicate.\n\n\n\n\nImage &gt; Adjust &gt; Brightness/Contrast...\nDrag sliders to enhance visibility\nCheck Auto for automatic adjustment\nUse Reset to revert to original\n\n\n\n\nBrightness and Contrast window\n\n\nUse case: Make dim images more visible for analysis.\n\n\nTry clicking on Apply to see what happens to the pixel values.\n\n\n\n\nMost of the cameras or detectors are monochromatic, which means that the color with which images are represented is a false color. Go to Image &gt; LookUp Tables or click on the LUT button and choose a different color. What do you think the different palettes are for? Try for example HiLo, cool and 16-colors.\n\n\n\nFirst choose an image with several dimension, like a z-stack with more than one channel. If you don’t have one, you can choose File &gt; Open samples &gt; Mitosis (5D stack).\n\n\n\n5D stack image\n\n\nThe sliders at the bottom allow you to go through the different channels, z-planes and timepoints. Some of the sample images already have what is called a composite, where channels are shown simultaneously. Try doing the following:\n\n\n\nImage &gt; Color &gt; Split Channels\nEach channel opens in a separate window\nAnalyze or process channels independently\n\n\n\n\n\nImage &gt; Color &gt; Merge Channels\nSelect source windows for each channel\nDo not create a composite image\n\n\n\n\n\nAdjust color assignments for better visualization\nGo to Image &gt; Color &gt; Make Composite\n\n\n\n\n\nChoose any structure in your image.\n\nClick on straight line button\nDraw a line across your object of interest\nGo to Analyze &gt; Plot Profile\nDisplays intensity values along the line (useful for colocalization studies)\n\n\n\n\nLine profile image\n\n\n\n\n\nPlot Profile image\n\n\nWhat happens if you right click on the straight line button?\n\n\n\nSelect an ROI (Region of Interest) using selection tools\nAnalyze → Measure (or Ctrl+M)\nResults include: Area, Mean intensity, Std Dev, Min, Max\n\n\n\n\n\n\nUse rectangle selection tool to define region\nImage → Crop\nImage is reduced to selected area",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html#alternative-napari-in-jupyter-notebooks",
    "href": "practicals/practical_2d/index.html#alternative-napari-in-jupyter-notebooks",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "If you prefer a Python-based, interactive approach, napari is a fast, interactive image viewer for multi-dimensional images.\n\n\nPixi is a package manager that makes it easy to manage Python environments and dependencies. Follow these steps to set up Napari with pixi:\n\n\n\nVisit the official Pixi website: https://pixi.sh/\nDownload and install Pixi for your operating system (Windows, Mac, or Linux)\nVerify installation by running in your terminal:\npixi --version\n\n\n\n\n\nCreate a new directory for your image analysis project:\nmkdir my-image-analysis\ncd my-image-analysis\nInitialize a Pixi project:\npixi init\nThis creates a pixi.toml configuration file for your project.\n\n\n\n\nAdd Python, napari, and Jupyter to your project:\npixi add python napari jupyter scikit-image\nThis command: - Installs Python and the specified packages - Creates a pixi.lock file with exact versions for reproducibility - Sets up an isolated environment specific to your project\n\n\n\nStart Jupyter notebook directly from your pixi project:\npixi run jupyter notebook\nThis runs Jupyter within your project’s environment, giving it access to napari, scikit-image, and all other packages you added.\n\n\n\n\n\n\nimport napari\nfrom skimage import io\n\n# Load an image\nimage = io.imread('path/to/image.tif')\n\n# Create viewer and add image layer\nviewer = napari.view_image(image, name='raw image')\n\n\n\nWe can do this either programatically from within the Jupyter notebook, or we can do this interactively from the Napari GUI.\nimport napari\nfrom skimage import io\n\nimage = io.imread('path/to/image.tif')\n\nviewer = napari.Viewer()\nlayer = viewer.add_image(image)\n\n# Add shapes layer for ROI drawing\nshapes_layer = viewer.add_shapes(\n    shape_type='rectangle',\n    edge_color='cyan',\n    face_color='transparent',\n    name='ROIs'\n)\nOr:\n\nClick on the button to generate a new Shapes Layer\nAdd a new rectangle\n\nOnce the rectangle is made, try accessing it via a cell in the Jupyter notebook. Access drawn shapes via shapes_layer.data.\n\n\n\n\n\nInteractive - Real-time parameter adjustment\nJupyter integration - Works in notebooks for reproducible workflows\nPython-native - Easy integration with scientific Python stack (scikit-image, scipy, etc.)\nMulti-dimensional - Handles time series, z-stacks, and multi-channel data\nProgrammatic ROI - Extract measurements from selected regions in code\n\n\n\n\n\nLess pre-built analysis functions (requires coding)\nSmaller ecosystem of plugins compared to Fiji\nLearning curve for Python programming",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html#additional-workshop-exercises",
    "href": "practicals/practical_2d/index.html#additional-workshop-exercises",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "Download a sample image\nOpen in Fiji\nCheck image properties (dimensions, bit depth, channels)\nUse zoom and pan tools to explore\nMeasure a feature using the measure tool\n\n\n\n\n\nOpen a dim or low-contrast image\nApply Brightness/Contrast adjustment\nApply Gaussian blur to reduce noise\nUse Unsharp Mask to enhance edges\nCompare original and processed versions\n\n\n\n\n\nOpen a 2D image with distinct objects\nApply threshold to create binary image\nUse morphological operations (Erode/Dilate) to clean up\nRun Analyze Particles to count and measure objects\nGenerate summary statistics\n\n\n\n\n\nLoad an image using scikit-image\nApply preprocessing (blur, threshold)\nPerform segmentation\nVisualize results with multiple layers in napari\nExtract measurements from labeled regions",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html#tips-tricks",
    "href": "practicals/practical_2d/index.html#tips-tricks",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "For very large images, reduce memory usage via Edit → Options → Memory & Threads\nDisable auto-update of image window to speed up processing: Edit → Options\nUse 8-bit images instead of 16-bit when precision isn’t critical\n\n\n\n\n\nDocument your workflow by recording macro actions: Plugins → Macros → Record\nExport processing steps as macro code for batch processing\n\n\n\n\n\nSave images and results to standardized output directories\nUse consistent naming conventions for variables\nDocument parameters used for each processing step\nVersion your analysis notebooks in git",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html#resources",
    "href": "practicals/practical_2d/index.html#resources",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "Official Fiji Documentation: https://fiji.sc/\nImageJ User Guide: https://imagej.net/\nNapari Documentation: https://napari.org/\nScientific Image Analysis Tutorials: https://imagej.net/Tutorials\nScikit-image (Python): https://scikit-image.org/",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html#sec-citation",
    "href": "practicals/practical_2d/index.html#sec-citation",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "Corbat, A. A. (2026). Workshop on 2D Image Visualization: Fiji and Napari. Zenodo. https://doi.org/10.5281/zenodo.18199646",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "practicals/practical_2d/index.html#license",
    "href": "practicals/practical_2d/index.html#license",
    "title": "Visualization of 2D data",
    "section": "",
    "text": "This work is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0).\nFor more details, visit: https://creativecommons.org/licenses/by/4.0/ and https://github.com/acorbat/2D_visualization_workshop\n\nLast Updated: January 2, 2026",
    "crumbs": [
      "Practicals",
      "Visualization of 2D data"
    ]
  },
  {
    "objectID": "lectures/talk_lightsheet_concepts.html",
    "href": "lectures/talk_lightsheet_concepts.html",
    "title": "Principles of light-sheet microscopy",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\nBy the end of this session, you will be able to:\n\nExplain the fundamental principles of light-sheet microscopy\nCompare light-sheet imaging with confocal and widefield microscopy\nIdentify applications appropriate for light-sheet microscopy",
    "crumbs": [
      "Lectures",
      "Principles of light-sheet microscopy"
    ]
  },
  {
    "objectID": "lectures/talk_lightsheet_concepts.html#slides",
    "href": "lectures/talk_lightsheet_concepts.html#slides",
    "title": "Principles of light-sheet microscopy",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Lectures",
      "Principles of light-sheet microscopy"
    ]
  },
  {
    "objectID": "lectures/talk_lightsheet_concepts.html#citation",
    "href": "lectures/talk_lightsheet_concepts.html#citation",
    "title": "Principles of light-sheet microscopy",
    "section": "Citation",
    "text": "Citation\nCuenca, M. B. (2025). Principles of light-sheet microscopy. Light-Sheet Image Analysis Workshop 2026 (LiSIAW2026), Santiago, Chile. Zenodo. https://doi.org/10.5281/zenodo.18106542",
    "crumbs": [
      "Lectures",
      "Principles of light-sheet microscopy"
    ]
  },
  {
    "objectID": "lectures/talk_experimental_design.html",
    "href": "lectures/talk_experimental_design.html",
    "title": "Experimental design and sample mounting",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\nBy the end of this session, you will be able to:\n\nDesign light-sheet microscopy experiments optimized for your biological question\nSelect appropriate sample mounting strategies and media\nPlan image acquisition parameters considering speed, resolution, and sample health",
    "crumbs": [
      "Lectures",
      "Experimental design and sample mounting"
    ]
  },
  {
    "objectID": "lectures/talk_experimental_design.html#slides",
    "href": "lectures/talk_experimental_design.html#slides",
    "title": "Experimental design and sample mounting",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Lectures",
      "Experimental design and sample mounting"
    ]
  },
  {
    "objectID": "lectures/talk_experimental_design.html#citation",
    "href": "lectures/talk_experimental_design.html#citation",
    "title": "Experimental design and sample mounting",
    "section": "Citation",
    "text": "Citation\nCuenca, M. B. (2025). Experimental Design. Light-Sheet Image Analysis Workshop 2026 (LiSIAW2026), Santiago, Chile. Zenodo. https://doi.org/10.5281/zenodo.17913589",
    "crumbs": [
      "Lectures",
      "Experimental design and sample mounting"
    ]
  },
  {
    "objectID": "lectures/talk_deep_learning.html",
    "href": "lectures/talk_deep_learning.html",
    "title": "Deep learning for image analysis",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\nBy the end of this session, you will be able to:\n\nBe aware of how machine and deep learning work\nKnow where to look for models and tools that can be used\nUnderstand deep learning and neural network fundamentals\nExplain how convolutional networks learn spatial features\nUnderstand machine learning principles for image classification\nExplain how pixel classification differs from deep learning\nUse tools like ilastik, WEKA, and APOC for interactive classification\nRecognize when machine learning is appropriate versus classical methods",
    "crumbs": [
      "Lectures",
      "Deep learning for image analysis"
    ]
  },
  {
    "objectID": "lectures/talk_deep_learning.html#slides",
    "href": "lectures/talk_deep_learning.html#slides",
    "title": "Deep learning for image analysis",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Lectures",
      "Deep learning for image analysis"
    ]
  },
  {
    "objectID": "lectures/talk_deep_learning.html#materials",
    "href": "lectures/talk_deep_learning.html#materials",
    "title": "Deep learning for image analysis",
    "section": "Materials",
    "text": "Materials\nCorbat, A. A., & Vellutini, B. (2026). 3D segmentation using machine/deep learning. Light-Sheet Image Analysis Workshop 2026 (LiSIAW2026), Santiago, Chile. Zenodo. https://doi.org/10.5281/zenodo.18155250",
    "crumbs": [
      "Lectures",
      "Deep learning for image analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome! This website contains the materials for the Light-Sheet Image Analysis Workshop organized by the Light-Sheet Imaging at Universidad Mayor (LiSIUM). The course will be held at the Center for Integrative Biology of Universidad Mayor during 5–9 of January, 2026 in Santiago, Chile.\n\n\n\nProgram Resources Course Website Course Information\n\n\n\nThe Light-Sheet Image Analysis Workshop is aimed at scientists at all levels and facility staff who wish to receive training in quantitative image analysis from multidimensional light-sheet microscopy data. The workshop is composed of theory sessions in the mornings and practical sessions in the afternoon. Participants will work on projects using the provided datasets or their own data and perform short presentations with what they learned and achieved during the course.\n\n\n\n\n\nAníbal Vargas Ríos\nLuz María Fuentealba\nCharlotte Buckley\n\n\n\n\n\n\nMarina Cuenca\nAgustín Corbat\nBruno Vellutini"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Home",
    "section": "",
    "text": "The Light-Sheet Image Analysis Workshop is aimed at scientists at all levels and facility staff who wish to receive training in quantitative image analysis from multidimensional light-sheet microscopy data. The workshop is composed of theory sessions in the mornings and practical sessions in the afternoon. Participants will work on projects using the provided datasets or their own data and perform short presentations with what they learned and achieved during the course.\n\n\n\n\n\nAníbal Vargas Ríos\nLuz María Fuentealba\nCharlotte Buckley\n\n\n\n\n\n\nMarina Cuenca\nAgustín Corbat\nBruno Vellutini"
  },
  {
    "objectID": "lectures/talk_data_management.html",
    "href": "lectures/talk_data_management.html",
    "title": "Best practices in microscopy data management",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\nBy the end of this session, you will be able to:\n\nUnderstand what is data management and why it is impportant\nPlan storage and backup strategies for large image datasets\nConsider data management plans in the experimental planning stage",
    "crumbs": [
      "Lectures",
      "Best practices in microscopy data management"
    ]
  },
  {
    "objectID": "lectures/talk_data_management.html#slides",
    "href": "lectures/talk_data_management.html#slides",
    "title": "Best practices in microscopy data management",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Lectures",
      "Best practices in microscopy data management"
    ]
  },
  {
    "objectID": "lectures/talk_data_management.html#citation",
    "href": "lectures/talk_data_management.html#citation",
    "title": "Best practices in microscopy data management",
    "section": "Citation",
    "text": "Citation\nVellutini, B. C. (2026). Best practices in microscopy data management. Light-Sheet Image Analysis Workshop 2026 (LiSIAW2026), Santiago, Chile. Zenodo. https://doi.org/10.5281/zenodo.18174180",
    "crumbs": [
      "Lectures",
      "Best practices in microscopy data management"
    ]
  },
  {
    "objectID": "lectures/talk_digital_images.html",
    "href": "lectures/talk_digital_images.html",
    "title": "Visualization and processing of digital images",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\nBy the end of this session, you will be able to:\n\nExplain fundamental concepts of digital image representation (pixels, voxels, bit depth)\nUnderstand how digital images represent information\nBe aware of how light intensity information is digitalized\nBridge concepts between optical and digital resolution",
    "crumbs": [
      "Lectures",
      "Visualization and processing of digital images"
    ]
  },
  {
    "objectID": "lectures/talk_digital_images.html#slides",
    "href": "lectures/talk_digital_images.html#slides",
    "title": "Visualization and processing of digital images",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Lectures",
      "Visualization and processing of digital images"
    ]
  },
  {
    "objectID": "lectures/talk_digital_images.html#citation",
    "href": "lectures/talk_digital_images.html#citation",
    "title": "Visualization and processing of digital images",
    "section": "Citation",
    "text": "Citation\nCorbat, A. A. (2026). Digital Image. Light-Sheet Image Analysis Workshop 2026 (LiSIAW2026), Santiago, Chile. Zenodo. https://doi.org/10.5281/zenodo.18148523",
    "crumbs": [
      "Lectures",
      "Visualization and processing of digital images"
    ]
  },
  {
    "objectID": "lectures/talk_image_processing.html",
    "href": "lectures/talk_image_processing.html",
    "title": "Image processing and analysis",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\nBy the end of this session, you will be able to:\n\nUnderstand bioimage analysis workflows and pipelines\nAcknowledging the characteristics of bioimage analysis and why it is needed\nAn overview of classical bioimage analysis pipelines and a examples of classical algorithms\nApply denoising and background subtraction techniques\nPerform segmentation and labeling on microscopy images\nUse classical image processing algorithms in Fiji and other bioimage tools",
    "crumbs": [
      "Lectures",
      "Image processing and analysis"
    ]
  },
  {
    "objectID": "lectures/talk_image_processing.html#slides",
    "href": "lectures/talk_image_processing.html#slides",
    "title": "Image processing and analysis",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Lectures",
      "Image processing and analysis"
    ]
  },
  {
    "objectID": "lectures/talk_image_processing.html#citation",
    "href": "lectures/talk_image_processing.html#citation",
    "title": "Image processing and analysis",
    "section": "Citation",
    "text": "Citation\nCorbat, A. A. (2026). Image Analysis. Light-Sheet Image Analysis Workshop 2026 (LiSIAW2026), Santiago, Chile. Zenodo. https://doi.org/10.5281/zenodo.18148566",
    "crumbs": [
      "Lectures",
      "Image processing and analysis"
    ]
  },
  {
    "objectID": "lectures/talk_multiview_cartography.html",
    "href": "lectures/talk_multiview_cartography.html",
    "title": "Multiview reconstruction and tissue cartography",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\nBy the end of this session, you will be able to:\n\nUnderstand multiview image registration and fusion strategies\nExplain how multiview imaging improves resolution and reduces artifacts\nApply tissue cartography concepts to map anatomical structures\nDesign multiview experiments for optimal resolution coverage",
    "crumbs": [
      "Lectures",
      "Multiview reconstruction and tissue cartography"
    ]
  },
  {
    "objectID": "lectures/talk_multiview_cartography.html#slides",
    "href": "lectures/talk_multiview_cartography.html#slides",
    "title": "Multiview reconstruction and tissue cartography",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Lectures",
      "Multiview reconstruction and tissue cartography"
    ]
  },
  {
    "objectID": "lectures/talk_multiview_cartography.html#citation",
    "href": "lectures/talk_multiview_cartography.html#citation",
    "title": "Multiview reconstruction and tissue cartography",
    "section": "Citation",
    "text": "Citation\nVellutini, B. C. (2026). Multiview Reconstruction and Tissue Cartography. Light-Sheet Image Analysis Workshop 2026 (LiSIAW2026), Santiago, Chile. Zenodo. https://doi.org/10.5281/zenodo.18174387",
    "crumbs": [
      "Lectures",
      "Multiview reconstruction and tissue cartography"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html",
    "href": "practicals/practical_3d/index.html",
    "title": "Visualization of 3D data",
    "section": "",
    "text": "This tutorial shows different approaches to visualize 3D microscopy data in Fiji (Schindelin et al. 2012). It provides a quick introduction to some of the tools bundled in Fiji, such as Orthogonal Views, Volume Viewer, 3D Viewer (Schmid et al. 2010), and BigDataViewer (Pietzsch et al. 2015), and a more in-depth explanation of the plugin 3Dscript (Schmid et al. 2019).",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-summary",
    "href": "practicals/practical_3d/index.html#sec-summary",
    "title": "Visualization of 3D data",
    "section": "",
    "text": "This tutorial shows different approaches to visualize 3D microscopy data in Fiji (Schindelin et al. 2012). It provides a quick introduction to some of the tools bundled in Fiji, such as Orthogonal Views, Volume Viewer, 3D Viewer (Schmid et al. 2010), and BigDataViewer (Pietzsch et al. 2015), and a more in-depth explanation of the plugin 3Dscript (Schmid et al. 2019).",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-requirements",
    "href": "practicals/practical_3d/index.html#sec-requirements",
    "title": "Visualization of 3D data",
    "section": "Requirements",
    "text": "Requirements\n\nFiji (Schindelin et al. 2012)\n3Dscript plugin (Schmid et al. 2019)\nFly Embryo Timelapse Lightsheet dataset (4x downsampled) (Vellutini 2025)",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-setup",
    "href": "practicals/practical_3d/index.html#sec-setup",
    "title": "Visualization of 3D data",
    "section": "Setup",
    "text": "Setup\n\nInstall Fiji\n\nGo to https://fiji.sc, choose Distribution: Stable, and click the download button.\nCopy the downloaded archive to your working directory and unzip it.\nOpen the Fiji.app directory and double-click on the launcher.\nThe main window of Fiji will open.\n\n\n\nInstall 3Dscript\n\nClick on Help &gt; Update... and wait.\nClick on Manage Update Sites.\nA list of plugins will open.\nSearch for 3Dscript and click on the checkbox.\nClick Apply and Close and then Apply Changes.\nWait until the downloads are finished. Then, click OK.\nRestart Fiji (close it and double-click the launcher).\nCheck if 3Dscript is installed under Plugins &gt; 3Dscript.\nYou are ready!\n\n\n\nDownload 3D datasets\n\nT1 Head (16-bits) dataset is included in Fiji; no need to download in advance.\nbtd-gap-stg_3_z3_t53s_E3_4x.tif dataset from this Zenodo repository (Vellutini 2025). The direct link to the file is here (2.3GB).",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-open-dataset",
    "href": "practicals/practical_3d/index.html#sec-open-dataset",
    "title": "Visualization of 3D data",
    "section": "Open dataset",
    "text": "Open dataset\nWe will begin by visualizing an MRI dataset of a human head that is bundled in Fiji.\n\nGo to File &gt; Open Samples &gt; T1 Head (16-bits).\n\n\n\n\n\n\n\n\n\n\n\n\nIncrease the zoom to 200% for better visualization.",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-orthogonal-views",
    "href": "practicals/practical_3d/index.html#sec-orthogonal-views",
    "title": "Visualization of 3D data",
    "section": "Orthogonal Views",
    "text": "Orthogonal Views\nOrthogonal Views is a tool that shows the optical sections through the orthogonal planes of XY: XZ and YZ. It is an easy and quick way to get a sense of the tridimensionality of your dataset. Whenever I’m opening a dataset for the first time I use Orthogonal Views. To activate it:\n\nClick on Image &gt; Stacks &gt; Orthogonal Views (or press Ctrl+Shift+H).\nThe XZ and YZ panels will open next to your XY stack.\n\n\n\nThe yellow lines are synchronized between the panels.\nClick around the different parts of the head to inspect the same position under different angles.\n\n\n\n\n\n\n\n\n\n\n\nOrthogonal Views is a great way to start understanding your 3D data.",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-volume-viewer",
    "href": "practicals/practical_3d/index.html#sec-volume-viewer",
    "title": "Visualization of 3D data",
    "section": "Volume Viewer",
    "text": "Volume Viewer\nVolume Viewer is a more powerful plugin for 3D visualization as it supports slicing, projections, and rendering. The interface is interactive and intuitive to use. To open it:\n\nGo to Plugins &gt; Volume Viewer. The main interface will open in the Slice mode.\n\n\n\nClick around and move the sample to see optical sections from different angles.\nThen, activate the Volume mode to render the sample’s surface in 3D and explore it as well, playing with the different rendering parameters.\n\n\n\n\n\n\n\n\n\n\n\nVolume Viewer also provides a way to take snapshots of the current view.",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-bigdata-viewer",
    "href": "practicals/practical_3d/index.html#sec-bigdata-viewer",
    "title": "Visualization of 3D data",
    "section": "BigDataViewer",
    "text": "BigDataViewer\nBigDataViewer (Pietzsch et al. 2015) is one of the most important tools for visualizing large, multidimensional datasets. It provides a simple and intuitive interface and shortcuts to swiftly navigate through your sample even on a regular laptop. This is possible because of the underlying file format used by BigDataViewer: the XML/HDF5 combo. Therefore, before opening the plugin, we must convert our dataset.\n\nGo to Plugins &gt; BigDataViewer &gt; Export Current Image as XML/HDF5.\n\n\nA dialog with export options will open.\n\nUnder Export path, click on Browse to select the output directory for t1-head.xml.\n\n\nThe export process will start. Since this is a small dataset, it’ll be fast. But, for large datasets, this can take hours.\nWhen done, you will find two new files in your working directory: t1-head.xml and t1-head.h5.\n\nThe XML file stores metadata information about the image. The HDF5 file stores actual image data. These two files will always be in a pair. To open the XML/HDF5:\n\nGo to Plugins &gt; BigDataViewer &gt; Open XML/HDF5 and select the t1-head.xml.\n\n\nThe BigDataViewer interface will open showing an optical section of the head sample.\n\nGetting familiar with BigDataViewer is an essential skill for navigating large 3D datasets. It’ll also be important for the multiview registration pipeline. So, take the time to learn the basic commands and shortcuts. It is nicely intuitive. The BigDataViewer’s page on the ImageJ Docs has the official documentation and we can also go to Help &gt; Show Help for an up-to-date overview.\nSome of the movements to try:\n\nLeft-click and drag: turn the sample around the mouse pointer at any arbitrary angle.\nRight-click and drag: move the sample in the XY plane (of the view).\nScroll: move through the Z plane (of the view). Use Shift+Scroll to move fast.\nCtrl+Shift+Scroll: zoom in or out.\n\n\n\n\n\n\n\nleft-click\n\n\n\n\n\n\n\nright-click\n\n\n\n\n\n\n\nscroll\n\n\n\n\n\n\n\nzoom\n\n\n\n\n\nBut, most importantly, are the commands to put your sample back to its original orientation or along any of the original dimension axes:\n\nShift+Z: orient the sample on the XY plane.\nShift+X: orient the sample on the ZY plane.\nShift+Y: orient the sample on the ZX plane.\n\n\n\n\n\n\n\nXY\n\n\n\n\n\n\n\nZY\n\n\n\n\n\n\n\nZX\n\n\n\n\n\nFinally, a visual tip. The default interpolation between image slices is nearest-neighbors. Press I to activate the tri-linear interpolation to obtain a much smoother (and improved) data visualization.\n\n\n\n\n\n\nnearest-neighbors\n\n\n\n\n\n\n\ntri-linear",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-threed-project",
    "href": "practicals/practical_3d/index.html#sec-threed-project",
    "title": "Visualization of 3D data",
    "section": "3D Project",
    "text": "3D Project\nThis is a simple tool to quickly generate a 3D animation (e.g., 360-degree rotation) from an image stack. There are basic parameters for adjusting the rendering, like projection method and opacity, and for controlling the animation. There’s only a bit of documentation. To try:\n\nGo to Image &gt; Stacks &gt; 3D Project... and click OK to generate a basic animation.\n\n\nAs noticeable above, 3D Project doesn’t do so well with our MRI dataset. However, it works OK for fluorescent microscopy images, so I encourage you to try with other datasets in the future.",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-threed-viewer",
    "href": "practicals/practical_3d/index.html#sec-threed-viewer",
    "title": "Visualization of 3D data",
    "section": "3D Viewer",
    "text": "3D Viewer\nThe 3D Viewer (Schmid et al. 2010) is a 3D visualization plugin bundled in Fiji. It has been the default 3D rendering engine for many years and provides a good starting point for visualizing and interacting with 3D images. The interface provides some rendering and animation options, but it is possible to create more advanced visualizations and animations with code. For more details, please consult the documentation.\nHere, we’ll only open our dataset with 3D Viewer for visualization.\n\nGo to Plugins &gt; 3D Viewer.\n\n\nAn import dialog will open. In addition to the image field itself, pay attention to the Resampling factor parameter. The default is 2, which means 2x downsampling of the original stack. Always downsample at least 2x because 3D Viewer will crash when trying to open large datasets.\n\nClick OK on the options dialog and when asked about converting to 8-bit.\n\n\n\n\n\n\n\n\n\n\n\nThe main interface will open.\n\nExplore the sample interactively.",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-bigvolume-browser",
    "href": "practicals/practical_3d/index.html#sec-bigvolume-browser",
    "title": "Visualization of 3D data",
    "section": "BigVolumeBrowser",
    "text": "BigVolumeBrowser\nBigVolumeBrowser is a Fiji plugin to render and interact with 3D data. It’s a fork of the unreleased BigVolumeViewer (a BigDataViewer cousin). The project is being actively developed and seems to have good documentation already. It’s a good candidate for some testing and for keeping an eye on in the future. However, we’ll not cover it in this tutorial as it’s simply too recent.",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-threedscript",
    "href": "practicals/practical_3d/index.html#sec-threedscript",
    "title": "Visualization of 3D data",
    "section": "3Dscript",
    "text": "3Dscript\n3Dscript is a GPU-accelerated Fiji plugin to generate animations of 3D/4D data (Schmid et al. 2019). It supports stacks with multiple channels and timepoints, has several options to control the rendering appearance, allows custom transformations and cropping of the data, and generates animations using a natural language, which is great to have precise control over the animation. 3Dscript is also incredibly fast to generate the animations and doesn’t require a lot of memory since the datasets can be opened as virtual stacks (more about this below).\nThis tutorial covers the basics for generating animations for 3D and 4D datasets. For more detailed information and documentation please refer to 3Dscript’s wiki.\nTo get started, make sure the head dataset is still open.\n\nGo to Plugins &gt; 3Dscript &gt; Interactive Animation.\n\n\nTwo new windows will open: 3D Animation with the initial rendering of the data and Interactive Raycaster with all the fields to control the rendering parameters.\n\n\nContrast\nThe Contrast section shows a histogram of pixel intensities of the image for each channel, which we can choose using the dropdown menu. We can set the minimum, gamma, and maximum values for the intensity and alpha (transparency) properties of each pixel. The weight option controls the general opacity of the channel (0=invisible, 100=visible). There’s also more advanced options like lighting and rendering algorithm which we’ll simply use the default states as they are usually good for most use cases.\nAdjusting the intensity and alpha values is the most impactful way to improve the 3D rendering. With the intensity setting we can define which pixel value in the image corresponds to total black (minimum) and which corresponds to total white. It’s the same as in the standard Brightness & Contrast tool. By default, 3Dscript will load these values from the original stack. In this case, it loaded min=3 and max=521.\n\n\n\n\n\n\n\n\n\n\nLet’s change these values to see how it impacts the 3D rendering.\n\nChange the intensity minimum to 250.\n\n\n\n\n\n\n\n\n\n\n\nYou will see that the darkest parts of the rendering will become even darker and no longer visible. We are losing real information from the data; we do not want that.\n\nSet the min to 0, for now.\nThen, change the max to 250.\n\n\n\n\n\n\n\n\n\n\n\nThe brightest parts of the rendering will become all white. It is so bright that we can no longer resolve details of the surface. We are losing information and also do not want that.\n\nSet the max to 500.\n\nNote that when you change an intensity value, the min/max black line in the histogram moves. You can also grab the line and move it manually to change the values.\nThe blue line represents the alpha values. In 3D rendering, a pixel has a transparency value linked to its intensity. The alpha min defines the value for full transparency and the max the value for full opacity.\n\nSet the alpha min to 250.\n\n\n\n\n\n\n\n\n\n\n\nThis will make darker pixels more transparent and information gets lost.\n\nSet it to 0.\n\n\n\n\n\n\n\n\n\n\n\nBy default 3Dscript sets the gamma value of alpha to 2.0. That’s a good default for fluorescence microscopy (see the next dataset below), but since this is MRI data, we need to tweak it a little differently.\n\nSet the alpha gamma value to 1.0.\n\n\n\n\n\n\n\n\n\n\n\nNote that this improves the visualization as the head’s surface becomes better visible.\n\nNow set the alpha max to 250.\n\n\n\n\n\n\n\n\n\n\n\nThe surface will become even more solid because we are defining that pixels that have a value above 250 will be fully opaque.\n\nTo compare, set alpha max to 5000.\n\n\n\n\n\n\n\n\n\n\n\nYou will notice that the sample will become more transparent. Even the brain inside the skull will be visible.\n\nSet alpha max back to 500.\n\n\n\n\n\n\n\n\n\n\n\nGenerally, setting the intensity and alpha to the same values is a good starting point for optimizing the rendering.\n\n\nTransformation\nThe transformation menu has controls for rotating, translating, and scaling the sample. We can either add values or manually interact with the 3D Animation window to reorient the sample. Let’s try the latter.\n\nLeft-click on the head and move it around.\n\n\nThat’s a great way to see your sample from different angles. And note that the values in the Transformation panel get updated every time you move the sample interactively. In this way you can roughly position the sample and then check and update the precise values for the target transformation.\n\nPress Reset and change the Rotation Y to 180 to look at the right side of the head.\n\n\n\n\n\n\n\n\n\n\n\n\nNow change Scale to 5 and Translation X to -500 to focus on the nose.\n\n\n\n\n\n\n\n\n\n\n\n\nPress Reset to return the sample to its original position.\n\n\n\nCropping\nAnother useful 3Dscript option is the ability to crop the bounding box to show the inside of the sample. We can do it in the XYZ directions or in the near/far axis, defined from the user point of view.\n\nSet the Z range minimum value to 60 (you can also drag the slider) to slice the sample through the Z axis.\n\n\n\nNow rotate the sample to see the cropped region from other angles.\n\n\n\n\n\n\n\n\n\n\n\n\nSet the Y range min to 125 and rotate around.\n\n\n\nFinally, reset the position and cropping parameters and set the Near/Far minimum to 0 and move the sample around to see the dynamic reslice of the sample with this cropping parameter.\n\n\n\nReset transformations and cropping parameters.\n\n\n\nBookmark\n3Dscript allows you to bookmark a view for later inspection. Add the current Contrast, Transformation, and Cropping parameters to the bookmark.\n\nFor that, simply click on the green icon.\n\n\n\n\nOutput\nThis last panel defines the dimensions of the output animation. By default it uses the original stack dimensions.\n\nYou can also define if the bounding box or the scale bar will be visible (enabled by default).\n\n\nAnimation 3D\nNow that we learned the basics of setting up the rendering and view parameters, we can start generating animations of the data.\n\nClick on the Animation section and on the Start text-based animation editor button.\n\n\nThis will open a special editor window for writing the animation script.\n\n\nRotate horizontally\nLet’s start with the simplest animation: a rotation of the head around 360 degrees. We need to define the number of frames that the animation will have and what will happen during these frames. We can start by defining that the animation will have ten frames.\nNote: frame counting in 3Dscript begins from 0 (frame 0 to 9 has 10 frames).\nThe editor has a strong autocomplete; you only need to type one letter at a time to be able to write the exact text needed for the animation.\n\nType f. The editor will autocomplete with From frame &lt;frame&gt;.\nType 0 and space. The autocomplete will fill with From frame 0 to frame &lt;frame&gt;.\nType 9 and space. The autocomplete will show a dropdown menu with several options (rotate by, translate, zoom by a factor of, reset transformation, and change).\nChoose rotate by and press Tab. The autocomplete will show From frame 0 to frame 9 rotate by &lt;degrees&gt;.\nType 360 and space. The sentence will be From frame 0 to frame 9 rotate by 360 degrees and a dropdown will show the options horizontally, vertically, and around.\nChoose horizontally and on the next menu choose (none).\n\n\n\n\n\n\n\n\n\n\n\nWe have our first animation script and it’s just this single sentence:\nFrom frame 0 to frame 9 rotate by 360 degrees horizontally\n\nPress Run.\n\nA new window will show up with an image stack of 10 frames containing the generated animation.\n\n\nPress play or  and watch the head turn 360 degrees during these 10 frames.\n\nNote that we did not need to define how many degrees the head would turn for each frame. We can simply state that we need the head to turn 360 in these 10 frames and 3Dscript will deal with it.\n\n\nMake it smoother\nOur first animation is cool, but a bit jumpy. To make it smoother we can add more frames.\n\nChange the final frame from 9 to 35, so that the animation will have 36 frames in total.\n\n\nNow each frame rotates by 1 degree and the animation is much smoother.\n\n\nAdd easing\nThe standard animation creates a linear rotation; every frame turns a fixed number of degrees. 3Dscript can add easing to create non-linear transitions by accelerating or decelerating the rotation.\n\nType ease at the end of the script sentence:\n\nFrom frame 0 to frame 35 rotate by 360 degrees horizontally ease\n\nThen, press Run.\n\n\n\n\n\n\n\n0°\n\n\n\n\n\n\n\n90°\n\n\n\n\n\n\n\n\n\n180°\n\n\n\n\n\n\n\n270°\n\n\n\n\n\nThe left head is the one without easing (linear transition) and the right head is the animation with easing. Note how the right head accelerates the rotation at the beginning, turning much faster, and then decelerates towards the end of the rotation. Both end the rotation at the same time. Play both animations side-by-side to see the difference (it’s very clear, once you see it).\n\n\nAnimate cropping\nLet’s add a couple more commands below our rotation sentence. We want that, after the 360 degree rotation, the animation slices through the head to show the tissues inside. For that, we can change the cropping parameters to control the position of the bounding box during the animation.\n\nAfter the first sentence, write the two commands as shown below:\n\nFrom frame 0 to frame 35 rotate by 360 degrees horizontally\nFrom frame 36 to frame 71 change channel 1 bounding box min z to 60\nFrom frame 72 to frame 99 change channel 1 bounding box min z to 0\n\nPress Run.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe script is saying to rotate 360 degrees horizontally, as before, set the Z range minimum to 60 (roughly halfway through the sample) for about 30 frames, and then set the Z range minimum back to 0 in the subsequent 30 frames. And that’s what we get.\n\n\nDefine multiple commands\nAnother useful 3Dscript feature is the ability to issue multiple commands to happen simultaneously, within the defined frames. For example, we can make a script that defines a horizontal rotation and Z cropping at the same time.\n\nWrite the code below in the editor and press Run:\n\nFrom frame 0 to frame 71:\n- rotate by 270 degrees horizontally\n- change channel 1 bounding box max z to 60\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow cropping is happening simultaneously with the rotation.\n\n\nSet initial conditions\nIf you simply re-run the command above, the head will already start cropped. That’s because 3Dscript takes the current parameters as the initial conditions for the animation. Since the previous animation changed the bounding box without changing it back, the value remains set at the current value (cropping the head). In fact, all the options set manually in the Raycaster window will be applied to the current animation. This can cause problems if you need to generate the animation again after closing 3Dscript and can’t remember the exact parameters.\nTo prevent this issue, we can set the initial conditions of the animation. This is highly recommended in general, but it’s also necessary when you want to start the animation with the sample in an orientation that is different than the default sample orientation. You can set the initial conditions using the At frame 0: construction.\n\nWrite the code below in the editor and press Run:\n\nAt frame 0:\n- rotate by 90 degrees around (0, 1, 0)\n- change channel 1 bounding box z to (0, 129)\n\nFrom frame 0 to frame 71:\n- rotate by 150 degrees horizontally\n- change channel 1 bounding box max z to 60\n- zoom by a factor of 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTweak appearance\nWe can now also tweak the animation to reach the final appearance that we want. For example, we can change scale bar width and height and hide the bounding box lines around the sample.\n\nWrite the code below in the editor and press Run:\n\nAt frame 0:\n- rotate by 90 degrees around (0, 1, 0)\n- change channel 1 bounding box z to (0, 129)\n- change bounding box visibility to off\n- change scalebar length to 50\n- change scalebar width to 10\n- change scalebar offset to 20\n\nFrom frame 0 to frame 71:\n- rotate by 180 degrees horizontally\n- change channel 1 bounding box max z to 60\n- zoom by a factor of 2\n\nFrom frame 72 to frame 100:\n- change channel 1 bounding box max z to 129\n- zoom by a factor of 0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote how there’s no longer a bounding box and the scale bar is much more visible.\nThis ends this part of the tutorial using a single-channel, single-timepoint dataset. Save the animation script to file for later re-use or incremental improvements.\n\n\n\nAnimation 4D\nLet’s now try a more challenging sample with two channels and several timepoints. This dataset shows a fly embryo during early development. The file is named btd-gap-stg_3_z3_t53s_E3_4x.tif. It has 2 channels, 30 slices, and 200 timepoints taken every 53s. The original file size is 37GB, but here we will use the 4x downsampled dataset of 2.3GB.\n\nOpen virtual stack\nEven though the original dataset would not even fit in the memory of today’s high-end laptops, we would still be able to generate animations using 3Dscript. That’s because 3Dscript works with the so-called Virtual Stacks in Fiji. This is a way to open large stacks without loading all the image data into memory (only what’s current on view is loaded). Virtual stacks are really, really great. Let’s open and inspect the new dataset as a virtual stack.\n\nClose all the previous 3Dscript windows (including the editor).\nGo to File &gt; Import &gt; TIFF Virtual Stack... or drag and drop the file on top of the &gt;&gt; arrows at the right corner of the Fiji window (secret trick) to open the dataset as a virtual stack.\n\n\n\n\n\n\n\n\n\n\n\n\nZoom in to 200% and inspect the dataset with Orthogonal Views.\n\n\n\nClose the orthogonal views.\n\n\n\nStart 3Dscript\n3Dscript will use the current image dimensions to generate the animation. For this reason, it is extremely important to return the stack’s zoom to 100% before opening 3Dscript! Otherwise the upsampled data may create image artifacts.\n\nOpen 3Dscript and zoom the 3D Animation window to 200% (this one is fine).\n\n\nThe initial 3D rendering is always showing the position and timepoint of the original stack (if you change the timepoint of the original stack and re-open 3Dscript, the current timepoint would be rendered).\n\nRotate the sample interactively to see the other side, where the surface of the embryo is.\nThen, reset the transformation and change the Rotation Y to 180.\n\n\n\n\nAnimate timepoints\nThis time, before we start optimizing the rendering, we will generate a simple animation across timepoints to have an overview of how the sample changes over time. This is generally good practice as the signal of live samples tends to vary over time.\n\nClick on Animation &gt; Start text-based animation editor.\nWrite the initial conditions of frame 0 as shown below (set to timepoint 1 and rotate sample 180 degrees horizontally), and generate a 10-frame animation from timepoint 1 to 200 (the last).\nThen, press Run:\n\nAt frame 0:\n- change timepoint to 1\n- rotate by 180 degrees horizontally\n\nFrom frame 0 to frame 9 change timepoint to 200\n\n3Dscript will generate the 10-frame animation and set the 3D Animation window to the last timepoint. Unfortunately, there’s no way to set the timepoint for the 3D Animation window manually (it always shows the last timepoint of the most recent animation).\nFrom this first animation, we can observe three things that we want to improve.\n\nThe signal from channel 1 is overexposed in the last timepoint. The intensity of this channel changes over time. The signal becomes so strong in the last timepoints that it becomes overexposed. We will fix this.\nThe signal from channel 2 is not so bright. We want to increase the contrast.\nThe sample is tilted upwards (the right side is pointing up). We want to make the sample completely horizontal, parallel to the bounding box.\n\n\n\nAdjust channel 1\n\nChange channel 1’s intensity and alpha maximum values from 600 to 1500.\n\n\nNote how the details along the bright stripe over the embryo are now more visible. As we have manually changed a Contrast value, we should add this information to the initial conditions of the animation.\n\nAdd the new values of min/max intensity/alpha for channel 1 in the script.\nThen press Run:\n\nAt frame 0:\n- change timepoint to 1\n- rotate by 180 degrees horizontally\n- change channel 1 min intensity to 200\n- change channel 1 max intensity to 1500\n- change channel 1 min alpha to 200\n- change channel 1 max alpha to 1500\n\nFrom frame 0 to frame 9 change timepoint to 200\n\n\nAdjust channel 2\nNow we will improve the contrast of the other channel.\n\nSwitch to channel 2 in the Contrast menu.\n\n\n\nChange channel 2’s intensity and alpha maximum values from 7000 to 3000.\n\n\nNote how the gray signal is now brighter and the sample surface looks more solid and less porous. That’s because we changed the alpha maximum value to make all the pixels above 3000 have 100% opacity.\nUnfortunately, the increase in brightness of channel 2 led to some regions of channel 1 becoming a little overexposed. Before writing the new values to the code, let’s make a small correction to channel 1. We don’t want to change the intensity, but we can make the pixels more transparent, so that they don’t become so bright.\n\nSet channel 1’s alpha maximum value from 1500 to 3000.\nThen, update the script as below and press Run:\n\nAt frame 0:\n- change timepoint to 1\n- rotate by 180 degrees horizontally\n- change channel 1 min intensity to 200\n- change channel 1 max intensity to 1500\n- change channel 1 min alpha to 200\n- change channel 1 max alpha to 3000\n- change channel 2 min intensity to 150\n- change channel 2 max intensity to 3000\n- change channel 2 min alpha to 150\n- change channel 2 max alpha to 3000\n\nFrom frame 0 to frame 9 change timepoint to 200\n\nGreat, the animation is looking nice now.\n\n\nAdjust orientation\nThe last detail to adjust is the sample orientation around the Z axis.\n\nChange the Rotation Z to -4 in the Transformation menu.\n\n\nThe sample will become parallel to the Y axis of the window. The bounding box will appear tilted.\nWe can now make this change permanent.\n\nAdd the following line to the script and press Run:\n\n- rotate by 4 degrees around (0, 0, 1)\n\nThis notation is a little different, but it just means that it is rotating 4 degrees around the Z axis (X, Y, Z).\n\n\nTweak appearance\nGreat. The core editing is done. We can now change some general parameters of the animation like we did for the previous dataset. We want to hide the bounding box and make the scale bar more visible.\n\nAdd the corresponding lines to the script, then press Run:\n\nAt frame 0:\n- change timepoint to 1\n- rotate by 180 degrees horizontally\n- rotate by 4 degrees around (0, 0, 1)\n- change channel 1 min intensity to 200\n- change channel 1 max intensity to 1500\n- change channel 1 min alpha to 200\n- change channel 1 max alpha to 3000\n- change channel 2 min intensity to 150\n- change channel 2 max intensity to 3000\n- change channel 2 min alpha to 150\n- change channel 2 max alpha to 3000\n- change bounding box visibility to off\n- change scalebar length to 50\n- change scalebar width to 5\n- change scalebar offset to 10\n\nFrom frame 0 to frame 9 change timepoint to 200\n\n\n\nMake it smoother\nWe can now finish off the animation by increasing the number of frames to make it smoother. The maximum number of frames is 200, as we have 200 timepoints. If you add more frames, you’ll get duplicated frames and the animation might lag.\n\nChange the last frame from 9 to 199, then press Run:\n\nFrom frame 0 to frame 199 change timepoint to 200\n\nEach frame is now a timepoint and the animation is as smooth as it can be, given the original data.\n\n\nSave animation\nWe are done with this animation, let’s save it. Always save the original animation as a .tif stack.\n\nPress Ctrl+S or File &gt; Save or File &gt; Save As &gt; Tiff....\n\n\nI normally add the 3D prefix to the filename.\nThen, also save the animation as a .avi video file.\n\nGo to File &gt; Save As &gt; AVI....\nChange the Compression to None (otherwise your image quality will be degraded) and choose the frame rate for the video (15fps works fine in this case).\n\n\n\n\n\n\n\n\n\n\n\nThis will create an uncompressed .avi file. You can usually play this file on your video player.\n\nHowever, this video can be large. So for presentation purposes and other usages, it is good practice to compress the video into a .mp4 container using a high-quality compression parameter to reduce the file size without affecting the image quality. A good software for this is HandBrake.",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-citation",
    "href": "practicals/practical_3d/index.html#sec-citation",
    "title": "Visualization of 3D data",
    "section": "Citation",
    "text": "Citation\nVellutini, B. C. (2026). Visualization of 3D data in Fiji using built-in tools and 3Dscript. Zenodo. https://doi.org/10.5281/zenodo.18070016",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-license",
    "href": "practicals/practical_3d/index.html#sec-license",
    "title": "Visualization of 3D data",
    "section": "License",
    "text": "License\nThis tutorial is available under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html#sec-references",
    "href": "practicals/practical_3d/index.html#sec-references",
    "title": "Visualization of 3D data",
    "section": "References",
    "text": "References\n\n\nPietzsch, Tobias, Stephan Saalfeld, Stephan Preibisch, and Pavel Tomancak. 2015. “BigDataViewer: Visualization and Processing for Large Image Data Sets.” Nat. Methods 12 (June): 481–83. https://doi.org/10.1038/nmeth.3392.\n\n\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. “Fiji: An Open-Source Platform for Biological-Image Analysis.” Nat. Methods 9 (June): 676–82. https://doi.org/10.1038/nmeth.2019.\n\n\nSchmid, Benjamin, Johannes Schindelin, Albert Cardona, Mark Longair, and Martin Heisenberg. 2010. “A High-Level 3D Visualization API for Java and ImageJ.” BMC Bioinformatics 11 (December): 274. https://doi.org/10.1186/1471-2105-11-274.\n\n\nSchmid, Benjamin, Philipp Tripal, Tina Fraaß, Christina Kersten, Barbara Ruder, Anika Grüneboom, Jan Huisken, and Ralf Palmisano. 2019. “3Dscript: Animating 3D/4D Microscopy Data Using a Natural-Language-Based Syntax.” Nat. Methods 16 (April): 278–80. https://doi.org/10.1038/s41592-019-0359-1.\n\n\nVellutini, Bruno C. 2025. “Fly Embryo Timelapse Lightsheet Dataset.” Zenodo. https://doi.org/10.5281/zenodo.18065738.",
    "crumbs": [
      "Practicals",
      "Visualization of 3D data"
    ]
  },
  {
    "objectID": "practicals/practical_deep_learning_segmentation/index.html",
    "href": "practicals/practical_deep_learning_segmentation/index.html",
    "title": "3D Deep Learning Segmentation",
    "section": "",
    "text": "This guide will walk you through the excercises for this workshop",
    "crumbs": [
      "Practicals",
      "3D Deep Learning Segmentation"
    ]
  },
  {
    "objectID": "practicals/practical_deep_learning_segmentation/index.html#sec-citation",
    "href": "practicals/practical_deep_learning_segmentation/index.html#sec-citation",
    "title": "3D Deep Learning Segmentation",
    "section": "Citation",
    "text": "Citation\nCorbat, A. A. (2026). Workshop on 3D Deep Learning Segmentation. Zenodo. https://doi.org/10.5281/zenodo.18187178",
    "crumbs": [
      "Practicals",
      "3D Deep Learning Segmentation"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html",
    "href": "practicals/practical_multiview/index.html",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "",
    "text": "Multiview reconstruction is the process of registering and fusing microscopy data acquired from multiple angles into a single, isotropic stack.\nThis tutorial shows how to register and fuse multiview lightsheet microscopy datasets using the plugin BigStitcher (Preibisch et al. 2010; Hörl et al. 2019) in Fiji (Schindelin et al. 2012).\nWe will cover how to convert the raw data for visualization in the BigDataViewer (Pietzsch et al. 2015), how to best detect interests points for registration, the different approaches to register views, and how to fuse the views to reconstruct an isotropic dataset.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-summary",
    "href": "practicals/practical_multiview/index.html#sec-summary",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "",
    "text": "Multiview reconstruction is the process of registering and fusing microscopy data acquired from multiple angles into a single, isotropic stack.\nThis tutorial shows how to register and fuse multiview lightsheet microscopy datasets using the plugin BigStitcher (Preibisch et al. 2010; Hörl et al. 2019) in Fiji (Schindelin et al. 2012).\nWe will cover how to convert the raw data for visualization in the BigDataViewer (Pietzsch et al. 2015), how to best detect interests points for registration, the different approaches to register views, and how to fuse the views to reconstruct an isotropic dataset.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-requirements",
    "href": "practicals/practical_multiview/index.html#sec-requirements",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Requirements",
    "text": "Requirements\n\nFiji (Schindelin et al. 2012)\nBigStitcher plugin (Preibisch et al. 2010; Hörl et al. 2019)\nFly Embryo Multiview Lightsheet dataset (Vellutini 2025)",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-setup",
    "href": "practicals/practical_multiview/index.html#sec-setup",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Setup",
    "text": "Setup\n\nInstall Fiji\n\nGo to https://fiji.sc, choose Distribution: Stable, and click the download button.\nCopy the downloaded archive to your working directory and unzip it.\n\n\n\nOpen the Fiji.app directory and double-click on the launcher.\n\n\nThe main window of Fiji will open.\n\n\nInstall BigStitcher\n\nClick on Help &gt; Update....\n\n\nThe updater will run and say if Fiji is up-to-date.\n\nClick on Manage Update Sites.\n\n\nA window will open with a list of plugins available to install in Fiji.\n\n\nFind BigStitcher in the list and click on the checkbox and Apply and Close.\n\n\n\nThen click on Apply Changes.\n\n\n\nWait… until the downloads are finished. Then, click OK.\n\n\n\nRestart Fiji (close window and double-click the launcher).\nCheck if BigStitcher is installed under Plugins &gt; BigStitcher.\n\n\nFiji and BigStitcher are ready!\n\n\nDownload dataset\n\nDmel_btd-gap_1tp_5v_2c_beads.czi dataset from this Zenodo repository (Vellutini 2025). The direct link to the file is here (3.2GB).",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-inspect-dataset",
    "href": "practicals/practical_multiview/index.html#sec-inspect-dataset",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Inspect dataset",
    "text": "Inspect dataset\nThe dataset is a single CZI file that contains a single timepoint with 5 different views, each view with 2 channels and 60 slices. The XY resolution is 0.276 µm and the Z resolution is 3 µm. The dataset has fluorescent beads around the sample, which we will use to register the views.\nLet’s inspect the dataset in Fiji.\n\nOpen file\n\nDrag and drop the CZI file in Fiji’s main window.\n\n\nA Bio-Formats Import Options window should open. That’s the default importer for proprietary file formats with many options, but for now we go with the default.\n\nPress OK.\n\n\nA Bio-Formats Series Options window will open. Bio-Formats recognized that this file contains more than one view (series) and is asking which ones do we want to open. We just want to inspect the first view since they will be quite similar.\n\nPress OK.\n\n\n\n\nAdjust contrast\nA big window with a black background will open.\n\nCheck if the dimensions were correctly assigned (information line at the top and sliders at the bottom).\n\n\nTo see something, we first need to adjust the levels.\n\nOpen the Brightness/Contrast (B&C) tool with Image &gt; Adjust &gt; Brightness/Contrast... (or Ctrl+Shift+C) and the Channels Tool with Image &gt; Color &gt; Channels Tool... (or Ctrl+Shift+Z).\n\n\n\nPress Reset to adjust the levels of Channel 1 then slide the Z position to the middle of the sample and press Reset again.\n\n\n\n\n\n\n\n\n\n\n\n\nNow move the Channel slider to Channel 2 and press Reset.\n\n\n\nIn the Channels window, change the menu Color to Composite.\n\n\nThe sample is ready to be visualized.\n\n\nOpen orthogonal views\nTo get a sense of the data tridimensionality we want to look at the XY, XZ, and YZ optical sections.\n\nClick on Image &gt; Stacks &gt; Orthogonal Views (or Ctrl+Shift+H). It takes a moment; XZ and YZ panels will open.\n\n\n\nResize the main window to fit the screen.\n\n\nThe sample is a fly embryo which resembles a cylinder in 3D.\n\nExplore the dataset by clicking and sliding the mouse pointer through the different images.\nWhen done, close the stack.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-define-dataset",
    "href": "practicals/practical_multiview/index.html#sec-define-dataset",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Define dataset",
    "text": "Define dataset\nBefore we begin, we need to define a multiview dataset and resave the dataset. Defining a multiview dataset will create an XML file where all the dataset metadata and information and data from the registration process will be stored.\n\nGo to Plugins &gt; BigStitcher &gt; General &gt; Define Multi-View Dataset.\n\n\nA window named Choose method to define dataset will open.\n\nOn the Define Dataset field choose Zeiss Lightsheet Z.1 Dataset Loader (Bioformats) from the dropdown menu, since our testing dataset is from Zeiss Lightsheet Z.1.\nThen, press OK.\n\n\n\nClick Browse, select the CZI file, and press OK.\n\n\nBigStitcher will read the metadata of the CZI and show a dialog with the details.\n\nCheck if five angles are present, if there are two channels, and if the XYZ resolution matches the expected values (see above).\nThen, press OK.\n\n\nIf we look into the working directory, an XML file named dataset will have appeared there.\n\nWe can open this file in a text editor to see the information stored there.\n\nRight-click the file, select Open With... and choose Text Editor.\nThe file has the file name, the image dimensions, the XYZ resolution, etc.\n\n\nNote: the XML only stores metadata from the dataset and not the actual image data.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-resave-dataset",
    "href": "practicals/practical_multiview/index.html#sec-resave-dataset",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Resave dataset",
    "text": "Resave dataset\nNext, we need to convert the actual image, which is still stored in the CZI, to a format that allows us to open and visualize this heavy dataset in an efficient and lightweight manner. For that, we will resave the data into HDF5 format.\n\nGo to Plugins &gt; BigStitcher &gt; I/O &gt; Resave as HDF5 (local).\n\n\nThe new window Select dataset for Resaving as HDF5 will automatically load the last used XML file, in this case, our dataset.xml. We can choose whether we want to convert every angle, all channels, all timepoints, or only a subset of those.\n\nWe want it all, press OK.\n\n\nAnother window will appear with some resaving options. Leave the options as is, but make sure that the Export path is pointing to the dataset.xml file.\n\nClick on Browse and, if the file is not selected, navigate and select dataset.xml.\nPress OK and wait…\n\n\nResaving this dataset takes about 3 min. However, larger datasets with several timepoints, for example, will take significantly longer. The Log window will show that it’s done.\n\nNote that another XML file named dataset.xml~1 and a new HDF5 file named dataset.h5 were created. Every time the dataset file is saved, BigStitcher creates a backup copy. dataset.xml~1 was the original dataset.xml which was renamed after the resaving. If we inspect the new dataset.xml in a Text Editor we will see that it now points to the dataset.h5 file.\n\nOpen the new dataset.xml in a text editor.\n\n\nWhenever we refer to the multiview dataset, we mean the XML/HDF5 pair; they are always together.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-visualize-dataset",
    "href": "practicals/practical_multiview/index.html#sec-visualize-dataset",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Visualize dataset",
    "text": "Visualize dataset\nWe can finally open the main BigStitcher application and begin the multiview reconstruction.\n\nStart BigStitcher\n\nGo to Plugins &gt; BigStitcher &gt; BigStitcher.\n\n\nThe last dataset.xml file will be automatically loaded in the select dataset window.\n\nClick OK.\n\n\nThis will open two windows, the BigDataViewer and the Multiview Explorer.\n\nThe Multiview Explorer shows a table with the individual views of the dataset. We have 5 views, each with 2 channels. Therefore, we have in total 10 views. Clicking in a row will show the data in the BigDataViewer. We can also sort the table by channel or angle.\n\nSelect multiple rows freely and try sorting it.\nThen, select the five views from Channel 561 (channel 2).\n\n\nThe image is too bright, we need to adjust the contrast.\n\nGo to Settings &gt; Brightness & Color.\n\n\nA new window will open.\n\nChange the max value of channel 2 to 2500.\n\n\nNow we can visualize the dataset in more detail.\n\n\nLearn BigDataViewer\nIt is important to familiarize yourself with the BigDataViewer commands and shortcuts. BigDataViewer is very intuitive to use but a quick look at the Help is important to not get lost.\nShift+X, Shift+Y, Shift+Z are our compass. If we get lost, pressing one of these shortcuts will get us back to the original XY, YZ, ZX orientation. In this scope the rotation axis is Y. Therefore, pressing Shift+Y will show the different angles from “above”.\n\nPress Shift+Y and adjust the view to see the five angles.\n\n\nSome of the most important commands are as follows:\n\nLeft-click and drag to rotate the data around the mouse pointer.\nRight-click and drag to move the view across XY plane.\nCtrl+Shift+Scroll to zoom in/out fast (Ctrl+Scroll for normal speed).\nSelect the BigDataViewer window and press I to activate tri-linear interpolation for better visualization.\n\n\n\n\n\n\n\n\n\n\n\n\nSelect the Multiview Explorer window with the five views selected and press C. This will autocolor the views which is great for visualization.\n\n\nTake some time to explore the data, the different views, zoom in and out, find the beads, and get familiar with the BigDataViewer. Then finish with the 5-views oriented as in the image above.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-detect-points",
    "href": "practicals/practical_multiview/index.html#sec-detect-points",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Detect points",
    "text": "Detect points\nWe can now start the first processing step of the pipeline, detecting points of interest to be used for the registration. The Multiview Explorer is always the starting point.\n\nSelect the five views of channel 561 (if not selected).\n\nIn this case, we want only the views of channel 2 to be selected since it is in this channel that the beads are visible.\n\nThen, right-click the rows with the mouse.\n\nA long menu will appear. This is the main menu of BigStitcher. Anything that we select will only be applied to the selected views.\n\nFind the section Processing and select Detect Interest Points....\n\n\nIn this first window we can choose the Type of interest point detection (Default: Difference of Gaussian) and a label to describe these interest points. One dataset can have multiple sets of interest points.\n\nWe will keep the default options and press OK.\n\n\nWe can set different parameters for the Difference of Gaussian approach. What is important to us at this point is to make sure that Interactive... is set for Interest point specification and that we change Downsample XY from Match Z Resolution (less downsampling) to 2x. The latter is not essential for all datasets but it works better for this dataset which has relatively high anisotropy (lower Z resolution compared to XY). Keep Downsample Z as 1x.\n\nPress OK.\n\n\nIn this case, we can select which view we will open the interactive window for and if we will load the entire stack.\n\nPress OK to load the entire first view.\n\n\nA stack will open with a rectangular ROI placed at the top left corner. The ROI shows a live view of detected points for the current parameters Sigma and Threshold in the other window.\n\nSigma is the size (radius) of the point and Threshold is the intensity-based cut value to discard low-quality detections. The default is to Find DoG maxima (red), in other words, bright spots. We can also find dark spots surrounded by bright areas (useful for some samples). Note that this is a normal ImageJ window, so we can zoom, adjust contrast, and if we lose the ROI, we can simply draw a new rectangular ROI. We can drag the ROI around by clicking inside it and holding and dragging it.\n\nWhat we want to do now is to adjust the Sigma and Threshold so that most of the beads outside the embryo are properly detected with the least spurious detections. The best way to begin is to:\n\nZoom in on a bead outside the embryo.\nCheck if the circle size is matching well the bead.\nMove the Sigma slider, aiming for a circle slightly larger than the bead.\n\nRemember to also move through the Z slices of the stack to see how the detection behaves.\n\nIf the size looks good, increase the Threshold until real beads begin to not be detected, then roll the slider back until most real beads are detected.\n\nNote that, no matter how much we tweak these parameters, there’ll always be many non-bead detections in the sample tissues. These detections will not have a strong influence on the registration given that we have enough real beads in the sample.\n\nWhen satisfied, press Done.\n\n\nBead detection takes some time.\n\nOnce DONE, go to the Multiview Explorer window and press Save.\n\n\nThis is important because the detections are initially saved in memory and will only be written to disk after pressing Save (detections are saved in a directory named interestpoints.n5).\nNote that the column #InterestPoints in the Multiview Explorer now shows 1 for the five views of channel 561 (but not for channel 488).",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-register-views",
    "href": "practicals/practical_multiview/index.html#sec-register-views",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Register views",
    "text": "Register views\nNow we can try to register these views using the detected interest points.\n\nWith the 5 views selected, right-click and run Register using Interest Points....\n\n\nWe can change different registration parameters like the algorithm to be used or the specific set of previously detected interest points. We want to go with the default Fast descriptor-based (rotation invariant) as Registration algorithm since this works well for bead-based registration with Z.1 datasets.\n\nBecause our views have almost no overlap, change the option Registration in between views to Compare all views against each other.\n\nLeave the other options as is making sure we are using the Interest points labeled as beads.\n\nPress OK.\n\n\nA window will open with several other parameters to tweak. Please refer to the BigStitcher documentation for the specific function of these. For us, it is important to note two.\nThe option Fix views set to Fix first view means that all other views will be mapped to the first angle. The option Transformation model set to Affine means that the data will be transformed non-rigidly to fit the individual views. This is important since different portions of the stack might have a certain degree of distortion from the objective lenses and an affine transformation helps to fit the views better together. The other parameters we will only need to change if our initial registration fails.\n\nPress OK.\n\n\nThe next two small windows to open, Regularization Parameters (Rigid and 0.10) and Select interest point grouping (Group interest points and 5) can be kept as is.\n\nPress OK on both.\n\n\n\n\n\n\n\n\n\n\n\nThe registration will begin and be over in a few seconds. Don’t blink or you will miss it! If successful, you will see that the individual views will now have moved over (registered) the first view and they are all overlapping in the BigDataViewer window.\n\n\n\n\n\n\n\n\n\n\nNow that the views are registered, explore the dataset to verify that the registration worked well. The best way to do this is visually. One of the first things that you can do is to:\n\nPress Shift+Y and zoom in on a bead close to the embryo’s surface.\n\n\nYou will see the point spread function of one bead in each individual view forming a star with generally four views (as the fifth view is too far away). If the sample is registered well, the center of the point spread functions of the different views should match in the middle of the star.\n\nAnother thing that you can do is to find a structure you know well in the sample and check that the tissues are actually registered. It can happen that the beads are nicely registered, but the tissues themselves are a bit off.\n\nWhen done checking, make sure to Save the project again.\n\n\nAfter saving, the #Registrations column should now show the number 3 for the selected views (if not, deselect and select them again to update the counter).",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-set-bounding",
    "href": "practicals/practical_multiview/index.html#sec-set-bounding",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Set bounding box",
    "text": "Set bounding box\nOur dataset is now registered, but before fusing the views it is important to set a bounding box around the sample. This reduces the final dimensions and file size of the fused data.\n\nFor that, right-click and select Define Bounding Box....\n\n\nWe want to define it interactively, so leave the Bounding Box option as Define using the BigDataViewer interactively. You can give the bounding box a custom name and define different bounding boxes for different purposes, but the default name is good enough for this tutorial.\n\nClick OK.\n\n\nTwo windows will open: BigDataViewer with the sample and some purple shade and a bounding box window full of sliders.\n\nFor defining the bounding box I follow a specific procedure, always in the same order, to avoid inadvertently leaving out a part of your sample when fusing.\n\nFirst, press Shift+X to orient the sample on XY.\n\n\n\nGo through the sample (Z) top to bottom to get a sense of the entire volume and stop back at the middle.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMove the x min slider to the right to cut out the region on the left of the sample (the dashed line is the reference edge).\nGet close to the sample, but leave a gap.\n\n\n\nOnce the x min is set, go again top to bottom through Z to make sure nothing was cut out.\nNow do the same for x max to cut out the region on the right side of the sample.\n\n\nNext we want to cut a bit from the top and bottom regions.\n\nMove the slider y min to cut from the top and y max to cut from the bottom.\n\nRemember to go through Z to make sure it is not cutting the tip off the embryo (it happens).\n\n\n\n\n\n\n\n\n\n\n\nFinally, press Shift+Y to cut out the exceeding portions in the Z direction.\n\n\n\nUse z min to cut from the top (in this orientation) always going through Y to check!\nThen adjust z max to cut from the bottom (in this orientation), also going through Y.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen done, press OK in the bounding box window.\n\n\nThe dimensions of the interactively defined bounding box and the estimated size of the fused image will appear.\n\nPress OK and Save.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-fuse-single",
    "href": "practicals/practical_multiview/index.html#sec-fuse-single",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Fuse dataset (one channel)",
    "text": "Fuse dataset (one channel)\nFinally, let’s begin the fusing of registered views.\n\nSelect the five registered views in the Multiview Explorer, right-click and press Image Fusion....\n\n\nThe window with fusion options will appear.\n\nOur “My Bounding Box” is automatically selected for the Bounding Box.\nWe can choose to downsample the fused image. I highly recommend downsampling 2x, 4x, or even 8x, when fusing a dataset for the first time. Fusing without downsampling (1x) can take a really long time for large samples (many hours), so it is good practice to downsample the fused image to make sure the fusing parameters are good for the dataset before fusing the whole thing.\n\nFor this tutorial, leave Downsampling at 1x.\nFor Interpolation keep Linear interpolation, as it gives better outputs.\n\nThe Fusion type is an important parameter to choose wisely.\n\nThe simplest is Avg, which averages the signal of every view per pixel. This is quick but it is combining the good contrast of one view with the blurred side of another view and the resulting contrast will be suboptimal.\nAvg, Blending is the same as Avg, but it blends smoothly the edges of the different views giving a slightly better fusing than simple Avg.\nAvg, Blending & Content Based improves the other two options by adding a step that checks and keeps only the best information for each coordinate (keep good contrast, discard blurred information). This option gives the best results. However, it is also the one that requires more memory and takes longer to finish (much longer).\n\nA sane start would be 2x or 4x downsample using Avg, Blending before trying less downsampling and the content based fusion.\n\nFor this tutorial, set Fusion type to Avg, Blending.\n\nFor the Pixel type I often use 16-bit, but it depends on what the goal of the fused image is. If it is only to have a volume visualization, 8-bit might be enough. If further processing and analysis is expected, definitely go for 16-bit or, in special cases, 32-bit.\n\nSet Pixel type to 16-bit unsigned integer.\n\nBigStitcher also has an option for using the interest points information during the fusion step to obtain better results (Interest Points for Non-Rigid). This is a newer feature for advanced use cases and I have not tried it enough to have an opinion about it.\n\nLeave Interest Points for Non-Rigid as -= Disable Non-Rigid =-.\n\nGenerally, we want to have one fused image per timepoint per channel.\n\nSet Produce one fused image for to Each timepoint & channel.\n\nAnd I always save the fused image to TIFF stacks. Choosing Display using ImageJ can be dangerous as the fused image will be large and the computer can run out of memory and crash. Writing to disk is safer.\n\nSet Fused image to Save as (compressed) TIFF stacks.\nPress OK.\nA window with min/max levels will open.\nPress OK.\n\n\nThe output directory is the same where the dataset.xml is. We can add a prefix to the filename to distinguish different types of fusion and downsampling (useful when doing it multiple times).\n\nSet Filename addition to avg_blend_1x.\nPress OK and fusion will start.\n\n\nWhen done, open the fused dataset in Fiji.\n\nDrag and drop the file avg_blend_1x_fused_tp_0_ch_1 into Fiji’s window.\nThen, adjust the contrast to see the data.\n\n\n\n\n\n\n\n\n\n\n\nInspect the fusion result, checking for artifacts. If the sample has a membrane staining, for example, check for doubled membranes.\n\nCheck the fused dataset with the Orthogonal Views.\n\n\nNote that the resulting fused image is isotropic.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-duplicate-transformation",
    "href": "practicals/practical_multiview/index.html#sec-duplicate-transformation",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Duplicate transformation",
    "text": "Duplicate transformation\nNow that we have successfully registered and fused the views of one channel, we can simply apply the series of transformations to the other channel without the need to detect interest points or register the channel independently.\n\nWe can do so using the tool Duplicate Transformations from BigStitcher.\n\nFirst, take a note of which channel we have registered (it’s the 561).\nThen, close the Multiview Explorer and the Select dataset window that pops up.\nFinally, go to Plugins &gt; BigStitcher &gt; General &gt; Tools &gt; Duplicate Transformations.\n\n\n\nSelect the option One channel to other channels.\n\n\nA Select dataset window will open with the last dataset.xml already opened.\n\nPress OK.\n\n\n\nSet the Source channel to 561.\nSet Target channel(s) to All Channels (all the other channels except for the source one).\n\nThe last option, Duplicate which transformations is important. Generally, Replace all transformations works for most cases. However, I often prefer to use Add last transformation only. This will take the last transformation from the source channel and apply it to the target channel. For this to work, however, the source channel can only be one transformation ahead of the target. If, for instance, we had run two subsequent transformations on the source channel, then applied only the last one to the other channels, we would not entirely duplicate all the transformations between the channels. So, always check the #Registrations in the Multiview Explorer to be sure if only the last duplication would work, or simply replace all transformations.\n\nFor now, set Duplicate which transformations to Add last transformation only.\nPress OK.\n\n\nThe transformations will be applied and recorded in the XML file. It’s quick.\n\nOpen BigStitcher again and check if the 5 views of the other channel are registered (they should be).",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-fuse-all",
    "href": "practicals/practical_multiview/index.html#sec-fuse-all",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Fuse dataset (all channels)",
    "text": "Fuse dataset (all channels)\nWe now have both channels registered, but only one was fused.\n\nTo fuse both channels select all the views in the Multiview Explorer, right-click, and select Image Fusion....\n\n\n\nSet the desired parameters for fusion (optimized previously) and run the fusion again as described above.\n\nThis time there will be two files as output: avg_blend_1x_fused_tp_0_ch_0.tif and avg_blend_1x_fused_tp_0_ch_1.tif.\n\n\nOpen both files in Fiji and adjust their contrast.\n\n\n\nThen go to Image &gt; Color &gt; Merge Channels....\n\n\n\nSelect ch_0 for C1 and ch_1 for C2 and press OK.\n\n\nA red-green 2-channel stack will open. But, red-green combination isn’t good.\n\nUpdate the red to magenta using the LUT tool.\n\n\n\n\n\n\n\n\n\n\n\nWe can even compare this fused dataset with one of the views of the original dataset.\n\nDrag and drop the CZI file, select the first view only to import, and put the stacks side-by-side for a comparison slice by slice.\n\n\nNote how the missing data in the single view is nicely present in the fused dataset.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-citation",
    "href": "practicals/practical_multiview/index.html#sec-citation",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "Citation",
    "text": "Citation\nVellutini, B. C. (2026). Multiview reconstruction in Fiji using BigStitcher. Zenodo. https://doi.org/10.5281/zenodo.18090752",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-license",
    "href": "practicals/practical_multiview/index.html#sec-license",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "License",
    "text": "License\nThis tutorial is available under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#references",
    "href": "practicals/practical_multiview/index.html#references",
    "title": "Multiview reconstruction using BigStitcher",
    "section": "References",
    "text": "References\n\n\nHörl, David, Fabio Rojas Rusak, Friedrich Preusser, Paul Tillberg, Nadine Randel, Raghav K Chhetri, Albert Cardona, et al. 2019. “BigStitcher: Reconstructing High-Resolution Image Datasets of Cleared and Expanded Samples.” Nat. Methods 16 (September): 870–74. https://doi.org/10.1038/s41592-019-0501-0.\n\n\nPietzsch, Tobias, Stephan Saalfeld, Stephan Preibisch, and Pavel Tomancak. 2015. “BigDataViewer: Visualization and Processing for Large Image Data Sets.” Nat. Methods 12 (June): 481–83. https://doi.org/10.1038/nmeth.3392.\n\n\nPreibisch, Stephan, Stephan Saalfeld, Johannes Schindelin, and Pavel Tomancak. 2010. “Software for Bead-Based Registration of Selective Plane Illumination Microscopy Data.” Nat. Methods 7 (June): 418–19. https://doi.org/10.1038/nmeth0610-418.\n\n\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. “Fiji: An Open-Source Platform for Biological-Image Analysis.” Nat. Methods 9 (June): 676–82. https://doi.org/10.1038/nmeth.2019.\n\n\nVellutini, Bruno C. 2025. “Fly Embryo Multiview Lightsheet Dataset.” Zenodo. https://doi.org/10.5281/zenodo.18078061.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher"
    ]
  },
  {
    "objectID": "practicals/practical_segmentation/index.html",
    "href": "practicals/practical_segmentation/index.html",
    "title": "Post-processing, segmentation and labelling",
    "section": "",
    "text": "In this exercise we will:\n\nCompare global and local thresholding methods and understand when each is useful.\n\nImprove segmentation using background subtraction and Gaussian smoothing.\n\nRefine masks using binary operations, including Fill Holes and Kill Borders.\n\nGenerate labeled objects using MorphoLibJ’s Connected Components Labeling.\n\nExtract object-level measurements (area, perimeter, circularity, etc.).\n\nVisualize measurement values on labeled images.\n\nFilter segmented objects based on size or other properties.\n\nThese steps form a complete workflow: raw image → pre-processing → segmentation → cleanup → labeling → measurement → filtering\nWe’ll use MAX_Lund.tif as the example image https://zenodo.org/records/17986091\nRequirements: - Fiji - MorphoLibJ plugin https://imagej.net/plugins/morpholibj\n\n\n\nStart Fiji.\n\nOpen the image:\nFile → Open… → MAX_Lund.tif\n\nOpen the histogram and visualization tools:\nImage → Adjust → Brightness/Contrast…\n\nYou should see a histogram like this:\n\n\n\nBrightness/Contrast histogram\n\n\nIdea: Thresholding chooses an intensity value that separates “foreground” from “background”. The histogram shows how many pixels exist at each intensity.\n\n\n\nGlobal thresholding applies one single threshold to the whole image.\n\nWith MAX_Lund.tif active, run:\nImage → Adjust → Auto Threshold…\nChoose:\n\nMethod: Try all\n\nWhite objects\n\n\nConfirm.\n\nFiji generates a montage of all global methods:\n\n\n\nGlobal auto threshold montage\n\n\n\n\n\nLocal thresholding requires 8-bit input.\n\nSelect the original image:\nWindow → MAX_Lund.tif\nConvert to 8-bit:\nImage → Type → 8-bit\n\nNow the image is ready for local methods.\n\n\n\nLocal thresholding computes a threshold for each local neighborhood.\n\nRun:\nImage → Adjust → Auto Local Threshold…\nChoose:\n\nMethod: Try all\n\nRadius: 15\n\nParameter 1: 0\n\nParameter 2: 0\n\nWhite objects\n\nConfirm.\n\n\n\n\nLocal auto threshold montage\n\n\n\n\n\nGood segmentation often benefits from reducing background and noise first.\n\n\n\nOpen MAX_Lund.png.\n\nSubtract background:\nProcess → Subtract Background…\n\nRolling ball radius: 50\n\n\nSmooth noise:\nProcess → Filters → Gaussian Blur…\n\nSigma: 1\n\n\nAfter preprocessing:\n\n\n\nAuto Local Threshold after preprocessing\n\n\n\n\n\n\n\n\n\nRun:\nImage → Adjust → Auto Local Threshold…\n\nMethod: Otsu\n\nRadius: 15\n\n\nLocal Otsu mask:\n\n\n\nLocal Otsu mask\n\n\n\n\n\n\nFill holes inside objects:\nProcess → Binary → Fill Holes\nSlightly shrink objects:\nProcess → Binary → Erode\nOptionally restore outlines:\nEdit → Draw\nExpand objects after erosion:\nProcess → Binary → Dilate\n\nRefined mask:\n\n\n\nRefined mask\n\n\n\n\n\n\nRemove partial objects touching image borders:\nPlugins → MorphoLibJ → Filtering → Kill Borders\n\n\n\n\nFilled + border-removed mask\n\n\n\n\n\n\nConvert each connected region into a uniquely labeled object:\nPlugins → MorphoLibJ → Label → Connected Components Labeling\n\nConnectivity: 4\n\nOutput type: 16-bit\n\nLabeled objects:\n\n\n\nConnected components labeling\n\n\n\n\n\nQuantifying each object is often the main goal after segmentation.\nRun measurements:\nPlugins → MorphoLibJ → Analyze → Analyze Regions\nThis extracts a number of morphological associated features. We will select:\n\nArea\n\nPixel count\n\nPerimeter\n\nCircularity\n\nEllipse geometry\n\nBounding box\n\n\n\n\nMorphometry table\n\n\n\n\n\nVisualize one measurement (e.g. area) mapped onto the label image:\nPlugins → MorphoLibJ → Label Images → Assign Measure to Label\nChoose Area and apply a colormap.\n\n\n\nArea visualization on labels\n\n\nUseful for:\n\nspotting unusually big/small objects\n\ndeciding filtering thresholds\n\nchecking measurement correctness\n\n\n\n\nRemove small objects based on size:\nPlugins → MorphoLibJ → Label Images → Label Size Filtering\n\nOperation: Lower Than\n\nSize threshold: 100 px\n\n\n\n\nSize filtering",
    "crumbs": [
      "Practicals",
      "Post-processing, segmentation and labelling"
    ]
  },
  {
    "objectID": "practicals/practical_segmentation/index.html#d-segmentation-in-fiji",
    "href": "practicals/practical_segmentation/index.html#d-segmentation-in-fiji",
    "title": "Post-processing, segmentation and labelling",
    "section": "",
    "text": "In this exercise we will:\n\nCompare global and local thresholding methods and understand when each is useful.\n\nImprove segmentation using background subtraction and Gaussian smoothing.\n\nRefine masks using binary operations, including Fill Holes and Kill Borders.\n\nGenerate labeled objects using MorphoLibJ’s Connected Components Labeling.\n\nExtract object-level measurements (area, perimeter, circularity, etc.).\n\nVisualize measurement values on labeled images.\n\nFilter segmented objects based on size or other properties.\n\nThese steps form a complete workflow: raw image → pre-processing → segmentation → cleanup → labeling → measurement → filtering\nWe’ll use MAX_Lund.tif as the example image https://zenodo.org/records/17986091\nRequirements: - Fiji - MorphoLibJ plugin https://imagej.net/plugins/morpholibj\n\n\n\nStart Fiji.\n\nOpen the image:\nFile → Open… → MAX_Lund.tif\n\nOpen the histogram and visualization tools:\nImage → Adjust → Brightness/Contrast…\n\nYou should see a histogram like this:\n\n\n\nBrightness/Contrast histogram\n\n\nIdea: Thresholding chooses an intensity value that separates “foreground” from “background”. The histogram shows how many pixels exist at each intensity.\n\n\n\nGlobal thresholding applies one single threshold to the whole image.\n\nWith MAX_Lund.tif active, run:\nImage → Adjust → Auto Threshold…\nChoose:\n\nMethod: Try all\n\nWhite objects\n\n\nConfirm.\n\nFiji generates a montage of all global methods:\n\n\n\nGlobal auto threshold montage\n\n\n\n\n\nLocal thresholding requires 8-bit input.\n\nSelect the original image:\nWindow → MAX_Lund.tif\nConvert to 8-bit:\nImage → Type → 8-bit\n\nNow the image is ready for local methods.\n\n\n\nLocal thresholding computes a threshold for each local neighborhood.\n\nRun:\nImage → Adjust → Auto Local Threshold…\nChoose:\n\nMethod: Try all\n\nRadius: 15\n\nParameter 1: 0\n\nParameter 2: 0\n\nWhite objects\n\nConfirm.\n\n\n\n\nLocal auto threshold montage\n\n\n\n\n\nGood segmentation often benefits from reducing background and noise first.\n\n\n\nOpen MAX_Lund.png.\n\nSubtract background:\nProcess → Subtract Background…\n\nRolling ball radius: 50\n\n\nSmooth noise:\nProcess → Filters → Gaussian Blur…\n\nSigma: 1\n\n\nAfter preprocessing:\n\n\n\nAuto Local Threshold after preprocessing\n\n\n\n\n\n\n\n\n\nRun:\nImage → Adjust → Auto Local Threshold…\n\nMethod: Otsu\n\nRadius: 15\n\n\nLocal Otsu mask:\n\n\n\nLocal Otsu mask\n\n\n\n\n\n\nFill holes inside objects:\nProcess → Binary → Fill Holes\nSlightly shrink objects:\nProcess → Binary → Erode\nOptionally restore outlines:\nEdit → Draw\nExpand objects after erosion:\nProcess → Binary → Dilate\n\nRefined mask:\n\n\n\nRefined mask\n\n\n\n\n\n\nRemove partial objects touching image borders:\nPlugins → MorphoLibJ → Filtering → Kill Borders\n\n\n\n\nFilled + border-removed mask\n\n\n\n\n\n\nConvert each connected region into a uniquely labeled object:\nPlugins → MorphoLibJ → Label → Connected Components Labeling\n\nConnectivity: 4\n\nOutput type: 16-bit\n\nLabeled objects:\n\n\n\nConnected components labeling\n\n\n\n\n\nQuantifying each object is often the main goal after segmentation.\nRun measurements:\nPlugins → MorphoLibJ → Analyze → Analyze Regions\nThis extracts a number of morphological associated features. We will select:\n\nArea\n\nPixel count\n\nPerimeter\n\nCircularity\n\nEllipse geometry\n\nBounding box\n\n\n\n\nMorphometry table\n\n\n\n\n\nVisualize one measurement (e.g. area) mapped onto the label image:\nPlugins → MorphoLibJ → Label Images → Assign Measure to Label\nChoose Area and apply a colormap.\n\n\n\nArea visualization on labels\n\n\nUseful for:\n\nspotting unusually big/small objects\n\ndeciding filtering thresholds\n\nchecking measurement correctness\n\n\n\n\nRemove small objects based on size:\nPlugins → MorphoLibJ → Label Images → Label Size Filtering\n\nOperation: Lower Than\n\nSize threshold: 100 px\n\n\n\n\nSize filtering",
    "crumbs": [
      "Practicals",
      "Post-processing, segmentation and labelling"
    ]
  },
  {
    "objectID": "practicals/practical_segmentation/index.html#d-segmentation-in-napari",
    "href": "practicals/practical_segmentation/index.html#d-segmentation-in-napari",
    "title": "Post-processing, segmentation and labelling",
    "section": "3D segmentation in Napari",
    "text": "3D segmentation in Napari\nIn this exercise we will:\n\nUse Napari to open a 3D image\nUse Napari assistant to visualize a workflow for 3D image segmentation and labelling\nUse region props to quantify morphological parameters and make colorcoded plots\n\nThese steps form a complete workflow:\nraw image → pre-processing → segmentation → cleanup → labeling → measurement → filtering\nWe’ll use Lund.tif as the example image https://zenodo.org/records/17986091.\nRequirements: - Everything you need is in the toml file in the Pixi/napari-assistant folder https://github.com/cuenca-mb/pixi-napari-assistant\n\n0. Open Napari assistant using Pixi\nIn the terminal, go to the directory Pixi/napari-assistant and run:\npixi run assistant\n\n\n1. Open a 3D stack\nDrag and drop the file or\nFile → Open File\n\n\n\nBrightness/Contrast histogram\n\n\nWe will be able to see and explore the stack, and we can change to a 3D rendering with the option Toogle 2D/3D view in the lower left button pannel. We can also make orthogonal views by clicking the button to the right Change order of the visible axis.\nIn the right pannel we will see the Assistant plugin, where it suggests operations in the appropriate order. The amount of operations and options depends on your installed plugins. Some of them are redundant.\n\n\n2. Remove background, binarization and labeling\nSelect Remove Background → White top hat  → radius = 10\n\n\n\nBrightness/Contrast histogram\n\n\nThen select Binarize → Threshold Yen, making sure to select the Result of White top-hat image.\n\n\n\nBrightness/Contrast histogram\n\n\nI recommend looking at the result in 3D.\n\n\n\nBrightness/Contrast histogram\n\n\nFinally, we can select Label → Connected component labeling, make sure to select the Result of Threshold image. We can additionally select the exclude on edges option.\n\n\n\nBrightness/Contrast histogram\n\n\nSome of them are stuck together. Let’s try and fix that.\n\n\n3. Fix labels\nLet’s select again the previous layer Result of Threshold. Then select Process labels → Binary erosion → radius = 3. This will reduce the objects of the binary segmentation.\n\n\n\nBrightness/Contrast histogram\n\n\nNow let’s recreate the labels Label → Connected component labeling, make sure to select the Result of Binary Erosion.\nThen Process labels → Expand Labels → radius = 3. Explore the labels in 3D.\n\n\n\nBrightness/Contrast histogram\n\n\nNow we can accurately measure morphological features of these labels. You can close the assistant pannel now.\n\n\n4. Measure morphological properties\nSelect Tools → Measure Tables → Object Features/Properties. Here make sure to select the Result of Expanded Labels image. You can select different features, includding intensity features extracted from the raw data. After running a table should appear which can be exported in csv format.\n\n\n\nBrightness/Contrast histogram\n\n\n\n\n\nBrightness/Contrast histogram\n\n\nby double clicking any of the columns of this table, a new layer image will appear with colorcoded labels indicating the value of the selected measurement. Colormaps can be adjusted for preference.\n\n\n\nBrightness/Contrast histogram",
    "crumbs": [
      "Practicals",
      "Post-processing, segmentation and labelling"
    ]
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Program",
    "section": "",
    "text": "Monday, 5 Jan 2026\n\n\n\n09:00–09:15 - Welcome session and course overview\n09:15–10:00 - Principles of light-sheet microscopy (Marina)\n10:00–11:00 - Experimental design and sample mounting (Marina)\n\n\n\n\n\n\n\n\n11:45–12:30 - Visualization and processing of digital images (Agustín)\n12:30–13:00 - Best practices in microscopy data management (Bruno)\n\n\n\n\n\n\n\n\n14:00–14:30 - Talk from MicroxChile\n14:30–15:30 - Visualization of 2D images (Agustín)\n15:30–17:30 - Visualization of 3D images (Bruno)\n\n\n\n\n\nTuesday, 6 Jan 2026\n\n\n\n09:00–10:00 - Image processing and analysis (Agustín)\n10:00–11:00 - Multiview reconstruction and tissue cartography (Bruno)\n\n\n\n\n\n\n\n\n11:45–13:00 - Pre-processing, denoising, segmentation (Marina)\n\n\n\n\n\n\n\n\n14:00–14:30 - Talk from Galenica\n14:30–16:00 - ImageJ macro programming (Marina)\n16:00–17:30 - Multiview reconstruction (Bruno)\n\n\n\n\n\nWednesday, 7 Jan 2026\n\n\n\n09:00–10:30 - Deep learning (Agustín/Bruno)\n\n\n\n\n\n\n\n\n11:00–11:45 - Object-classification, dimensionality reduction and clustering (Marina)\n11:45–12:30 - 3D Deep Learning Segmentation (Agustín)\n12:30–13:00 - Talk from Bruker\n\n\n\n\n\n\n\n\n14:00–16:00 - Tissue cartography (Bruno)\n16:00–17:30 - Cell tracking (Bruno)\n\n\n\n\n\nThursday, 8 Jan 2026\n\n\n\n09:00–09:30 - Talk Bruno\n09:30–10:00 - Talk Marina\n10:00–10:30 - Talk Charlotte\n10:30–11:00 - Talk Leo\n\n\n\n\n\n\n\n\n11:30–13:00 - Project work (Groups)\n\n\n\n\n\n14:00–17:30 - Project work (Groups)\n\n\n\n\n\nFriday, 9 Jan 2026\n\n\n\n09:00–11:00 - Project work (Groups)\n\n\n\n\n\n\n\n\n11:30–13:00 - Project work (Groups)\n\n\n\n\n\n14:00–16:30 - Group presentations (Groups)\n16:30–17:00 - Closing remarks (Organizers)\n17:00–onwards - Group dinner (Everyone)",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "program.html#day-1-light-sheet-principles-and-digital-images",
    "href": "program.html#day-1-light-sheet-principles-and-digital-images",
    "title": "Program",
    "section": "",
    "text": "Monday, 5 Jan 2026\n\n\n\n09:00–09:15 - Welcome session and course overview\n09:15–10:00 - Principles of light-sheet microscopy (Marina)\n10:00–11:00 - Experimental design and sample mounting (Marina)\n\n\n\n\n\n\n\n\n11:45–12:30 - Visualization and processing of digital images (Agustín)\n12:30–13:00 - Best practices in microscopy data management (Bruno)\n\n\n\n\n\n\n\n\n14:00–14:30 - Talk from MicroxChile\n14:30–15:30 - Visualization of 2D images (Agustín)\n15:30–17:30 - Visualization of 3D images (Bruno)",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "program.html#day-2-image-processing-and-multiview-reconstruction",
    "href": "program.html#day-2-image-processing-and-multiview-reconstruction",
    "title": "Program",
    "section": "",
    "text": "Tuesday, 6 Jan 2026\n\n\n\n09:00–10:00 - Image processing and analysis (Agustín)\n10:00–11:00 - Multiview reconstruction and tissue cartography (Bruno)\n\n\n\n\n\n\n\n\n11:45–13:00 - Pre-processing, denoising, segmentation (Marina)\n\n\n\n\n\n\n\n\n14:00–14:30 - Talk from Galenica\n14:30–16:00 - ImageJ macro programming (Marina)\n16:00–17:30 - Multiview reconstruction (Bruno)",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "program.html#day-3-machine-learning-and-advanced-workflows",
    "href": "program.html#day-3-machine-learning-and-advanced-workflows",
    "title": "Program",
    "section": "",
    "text": "Wednesday, 7 Jan 2026\n\n\n\n09:00–10:30 - Deep learning (Agustín/Bruno)\n\n\n\n\n\n\n\n\n11:00–11:45 - Object-classification, dimensionality reduction and clustering (Marina)\n11:45–12:30 - 3D Deep Learning Segmentation (Agustín)\n12:30–13:00 - Talk from Bruker\n\n\n\n\n\n\n\n\n14:00–16:00 - Tissue cartography (Bruno)\n16:00–17:30 - Cell tracking (Bruno)",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "program.html#day-4-scientific-applications-and-project-work",
    "href": "program.html#day-4-scientific-applications-and-project-work",
    "title": "Program",
    "section": "",
    "text": "Thursday, 8 Jan 2026\n\n\n\n09:00–09:30 - Talk Bruno\n09:30–10:00 - Talk Marina\n10:00–10:30 - Talk Charlotte\n10:30–11:00 - Talk Leo\n\n\n\n\n\n\n\n\n11:30–13:00 - Project work (Groups)\n\n\n\n\n\n14:00–17:30 - Project work (Groups)",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "program.html#day-5-project-work-and-presentations",
    "href": "program.html#day-5-project-work-and-presentations",
    "title": "Program",
    "section": "",
    "text": "Friday, 9 Jan 2026\n\n\n\n09:00–11:00 - Project work (Groups)\n\n\n\n\n\n\n\n\n11:30–13:00 - Project work (Groups)\n\n\n\n\n\n14:00–16:30 - Group presentations (Groups)\n16:30–17:00 - Closing remarks (Organizers)\n17:00–onwards - Group dinner (Everyone)",
    "crumbs": [
      "Program"
    ]
  }
]