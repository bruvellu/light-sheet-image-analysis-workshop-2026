[
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Datasets\nFind here the example datasets available for the course.",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "talks/talk_machine_learning.html",
    "href": "talks/talk_machine_learning.html",
    "title": "Machine learning",
    "section": "",
    "text": "Machine learning\n\nPixel Classification Theory\n\nExamples: ilastik, weka, APOC, labkit",
    "crumbs": [
      "Talks",
      "Machine learning"
    ]
  },
  {
    "objectID": "talks/talk_experimental_design.html",
    "href": "talks/talk_experimental_design.html",
    "title": "Experimental design and sample mounting",
    "section": "",
    "text": "Experimental design and sample mounting\n\nExperimental setup\n\nRecognizing image artifacts\nTouch upon clearing, but focus on showing who (or which review article) to refer to.\n\n(Talk adapted from Marina‚Äôs LISH 2025 talk)",
    "crumbs": [
      "Talks",
      "Experimental design and sample mounting"
    ]
  },
  {
    "objectID": "talks/talk_digital_images.html",
    "href": "talks/talk_digital_images.html",
    "title": "Visualization and processing of digital images",
    "section": "",
    "text": "Visualization and processing of digital images\n\nWhat is an image? (bit depth, voxel size)\n\nCameras?\n\nHistograms\n\nLUT\n\nDimensions\n\nB+C\n\nFormats (focus on HDF5)\n\nBased on this lecture",
    "crumbs": [
      "Talks",
      "Visualization and processing of digital images"
    ]
  },
  {
    "objectID": "talks/talk_welcome.html",
    "href": "talks/talk_welcome.html",
    "title": "üî¨ Light-Sheet Image Analysis Workshop 2026",
    "section": "",
    "text": "Marina can give the welcome talk. We can ask Anibal what information to mention in here. Otherwise acknowlege funding bodies, introduce the instructors and organizers. Maybe also put the students in teams of 2 (do we do this or we let them self organise?) Introduce course materials.",
    "crumbs": [
      "Talks",
      "Talk Welcome"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Warning\n\n\n\nThis website is under construction!\n\n\nWelcome! This website contains the materials for the Light-Sheet Image Analysis Workshop organized by the Light-Sheet Imaging at Universidad Mayor (LiSIUM). The course will be held at the Center for Integrative Biology of Universidad Mayor between 5‚Äì9 January 2026 in Santiago, Chile.\n\n\nProgram Resources\n\n\n\nThe Light-Sheet Image Analysis Workshop is aimed at scientists at all levels and facility staff who wish to receive training in quantitative image analysis from multidimensional light-sheet microscopy data.\nThe workshop is composed of theory sessions in the mornings and practical sessions in the afternoon. Participants will work on projects using the provided datasets or their own data and perform short presentations with what they learned and achieved during the course.\n\n\n\n\n\nAn√≠bal Vargas R√≠os\nLuz Mar√≠a Fuentealba\nCharlotte Buckley\n\n\n\n\n\n\nMarina Cuenca\nAgust√≠n Corbat\nBruno Vellutini"
  },
  {
    "objectID": "index.html#shortcuts",
    "href": "index.html#shortcuts",
    "title": "Home",
    "section": "",
    "text": "Program Resources"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Home",
    "section": "",
    "text": "The Light-Sheet Image Analysis Workshop is aimed at scientists at all levels and facility staff who wish to receive training in quantitative image analysis from multidimensional light-sheet microscopy data.\nThe workshop is composed of theory sessions in the mornings and practical sessions in the afternoon. Participants will work on projects using the provided datasets or their own data and perform short presentations with what they learned and achieved during the course.\n\n\n\n\n\nAn√≠bal Vargas R√≠os\nLuz Mar√≠a Fuentealba\nCharlotte Buckley\n\n\n\n\n\n\nMarina Cuenca\nAgust√≠n Corbat\nBruno Vellutini"
  },
  {
    "objectID": "practicals/practical_2d/index.html",
    "href": "practicals/practical_2d/index.html",
    "title": "Visualization of 2D images",
    "section": "",
    "text": "Visualization of 2D images\n\nOpening an image\n\nExploring dimensions, scale, formats, bits, metadata\n\nB+C\n\nLUTs\n\nScalebar",
    "crumbs": [
      "Practicals",
      "Visualization of 2D images"
    ]
  },
  {
    "objectID": "practicals/practical_segmentation/index.html",
    "href": "practicals/practical_segmentation/index.html",
    "title": "Post-processing, segmentation and labelling",
    "section": "",
    "text": "In this exercise we will:\n\nCompare global and local thresholding methods and understand when each is useful.\n\nImprove segmentation using background subtraction and Gaussian smoothing.\n\nRefine masks using binary operations, including Fill Holes and Kill Borders.\n\nGenerate labeled objects using MorphoLibJ‚Äôs Connected Components Labeling.\n\nExtract object-level measurements (area, perimeter, circularity, etc.).\n\nVisualize measurement values on labeled images.\n\nFilter segmented objects based on size or other properties.\n\nThese steps form a complete workflow: raw image ‚Üí pre-processing ‚Üí segmentation ‚Üí cleanup ‚Üí labeling ‚Üí measurement ‚Üí filtering\nWe‚Äôll use MAX_Lund.tif as the example image https://zenodo.org/records/17986091\nRequirements: - Fiji - MorphoLibJ plugin https://imagej.net/plugins/morpholibj\n\n\n\nStart Fiji.\n\nOpen the image:\nFile ‚Üí Open‚Ä¶ ‚Üí MAX_Lund.tif\n\nOpen the histogram and visualization tools:\nImage ‚Üí Adjust ‚Üí Brightness/Contrast‚Ä¶\n\nYou should see a histogram like this:\n\n\n\nBrightness/Contrast histogram\n\n\nIdea: Thresholding chooses an intensity value that separates ‚Äúforeground‚Äù from ‚Äúbackground‚Äù. The histogram shows how many pixels exist at each intensity.\n\n\n\nGlobal thresholding applies one single threshold to the whole image.\n\nWith MAX_Lund.tif active, run:\nImage ‚Üí Adjust ‚Üí Auto Threshold‚Ä¶\nChoose:\n\nMethod: Try all\n\nWhite objects\n\n\nConfirm.\n\nFiji generates a montage of all global methods:\n\n\n\nGlobal auto threshold montage\n\n\n\n\n\nLocal thresholding requires 8-bit input.\n\nSelect the original image:\nWindow ‚Üí MAX_Lund.tif\nConvert to 8-bit:\nImage ‚Üí Type ‚Üí 8-bit\n\nNow the image is ready for local methods.\n\n\n\nLocal thresholding computes a threshold for each local neighborhood.\n\nRun:\nImage ‚Üí Adjust ‚Üí Auto Local Threshold‚Ä¶\nChoose:\n\nMethod: Try all\n\nRadius: 15\n\nParameter 1: 0\n\nParameter 2: 0\n\nWhite objects\n\nConfirm.\n\n\n\n\nLocal auto threshold montage\n\n\n\n\n\nGood segmentation often benefits from reducing background and noise first.\n\n\n\nOpen MAX_Lund.png.\n\nSubtract background:\nProcess ‚Üí Subtract Background‚Ä¶\n\nRolling ball radius: 50\n\n\nSmooth noise:\nProcess ‚Üí Filters ‚Üí Gaussian Blur‚Ä¶\n\nSigma: 1\n\n\nAfter preprocessing:\n\n\n\nAuto Local Threshold after preprocessing\n\n\n\n\n\n\n\n\n\nRun:\nImage ‚Üí Adjust ‚Üí Auto Local Threshold‚Ä¶\n\nMethod: Otsu\n\nRadius: 15\n\n\nLocal Otsu mask:\n\n\n\nLocal Otsu mask\n\n\n\n\n\n\nFill holes inside objects:\nProcess ‚Üí Binary ‚Üí Fill Holes\nSlightly shrink objects:\nProcess ‚Üí Binary ‚Üí Erode\nOptionally restore outlines:\nEdit ‚Üí Draw\nExpand objects after erosion:\nProcess ‚Üí Binary ‚Üí Dilate\n\nRefined mask:\n\n\n\nRefined mask\n\n\n\n\n\n\nRemove partial objects touching image borders:\nPlugins ‚Üí MorphoLibJ ‚Üí Filtering ‚Üí Kill Borders\n\n\n\n\nFilled + border-removed mask\n\n\n\n\n\n\nConvert each connected region into a uniquely labeled object:\nPlugins ‚Üí MorphoLibJ ‚Üí Label ‚Üí Connected Components Labeling\n\nConnectivity: 4\n\nOutput type: 16-bit\n\nLabeled objects:\n\n\n\nConnected components labeling\n\n\n\n\n\nQuantifying each object is often the main goal after segmentation.\nRun measurements:\nPlugins ‚Üí MorphoLibJ ‚Üí Analyze ‚Üí Analyze Regions\nThis extracts a number of morphological associated features. We will select:\n\nArea\n\nPixel count\n\nPerimeter\n\nCircularity\n\nEllipse geometry\n\nBounding box\n\n\n\n\nMorphometry table\n\n\n\n\n\nVisualize one measurement (e.g.¬†area) mapped onto the label image:\nPlugins ‚Üí MorphoLibJ ‚Üí Label Images ‚Üí Assign Measure to Label\nChoose Area and apply a colormap.\n\n\n\nArea visualization on labels\n\n\nUseful for:\n\nspotting unusually big/small objects\n\ndeciding filtering thresholds\n\nchecking measurement correctness\n\n\n\n\nRemove small objects based on size:\nPlugins ‚Üí MorphoLibJ ‚Üí Label Images ‚Üí Label Size Filtering\n\nOperation: Lower Than\n\nSize threshold: 100 px\n\n\n\n\nSize filtering",
    "crumbs": [
      "Practicals",
      "Post-processing, segmentation and labelling"
    ]
  },
  {
    "objectID": "practicals/practical_segmentation/index.html#d-segmentation-in-fiji",
    "href": "practicals/practical_segmentation/index.html#d-segmentation-in-fiji",
    "title": "Post-processing, segmentation and labelling",
    "section": "",
    "text": "In this exercise we will:\n\nCompare global and local thresholding methods and understand when each is useful.\n\nImprove segmentation using background subtraction and Gaussian smoothing.\n\nRefine masks using binary operations, including Fill Holes and Kill Borders.\n\nGenerate labeled objects using MorphoLibJ‚Äôs Connected Components Labeling.\n\nExtract object-level measurements (area, perimeter, circularity, etc.).\n\nVisualize measurement values on labeled images.\n\nFilter segmented objects based on size or other properties.\n\nThese steps form a complete workflow: raw image ‚Üí pre-processing ‚Üí segmentation ‚Üí cleanup ‚Üí labeling ‚Üí measurement ‚Üí filtering\nWe‚Äôll use MAX_Lund.tif as the example image https://zenodo.org/records/17986091\nRequirements: - Fiji - MorphoLibJ plugin https://imagej.net/plugins/morpholibj\n\n\n\nStart Fiji.\n\nOpen the image:\nFile ‚Üí Open‚Ä¶ ‚Üí MAX_Lund.tif\n\nOpen the histogram and visualization tools:\nImage ‚Üí Adjust ‚Üí Brightness/Contrast‚Ä¶\n\nYou should see a histogram like this:\n\n\n\nBrightness/Contrast histogram\n\n\nIdea: Thresholding chooses an intensity value that separates ‚Äúforeground‚Äù from ‚Äúbackground‚Äù. The histogram shows how many pixels exist at each intensity.\n\n\n\nGlobal thresholding applies one single threshold to the whole image.\n\nWith MAX_Lund.tif active, run:\nImage ‚Üí Adjust ‚Üí Auto Threshold‚Ä¶\nChoose:\n\nMethod: Try all\n\nWhite objects\n\n\nConfirm.\n\nFiji generates a montage of all global methods:\n\n\n\nGlobal auto threshold montage\n\n\n\n\n\nLocal thresholding requires 8-bit input.\n\nSelect the original image:\nWindow ‚Üí MAX_Lund.tif\nConvert to 8-bit:\nImage ‚Üí Type ‚Üí 8-bit\n\nNow the image is ready for local methods.\n\n\n\nLocal thresholding computes a threshold for each local neighborhood.\n\nRun:\nImage ‚Üí Adjust ‚Üí Auto Local Threshold‚Ä¶\nChoose:\n\nMethod: Try all\n\nRadius: 15\n\nParameter 1: 0\n\nParameter 2: 0\n\nWhite objects\n\nConfirm.\n\n\n\n\nLocal auto threshold montage\n\n\n\n\n\nGood segmentation often benefits from reducing background and noise first.\n\n\n\nOpen MAX_Lund.png.\n\nSubtract background:\nProcess ‚Üí Subtract Background‚Ä¶\n\nRolling ball radius: 50\n\n\nSmooth noise:\nProcess ‚Üí Filters ‚Üí Gaussian Blur‚Ä¶\n\nSigma: 1\n\n\nAfter preprocessing:\n\n\n\nAuto Local Threshold after preprocessing\n\n\n\n\n\n\n\n\n\nRun:\nImage ‚Üí Adjust ‚Üí Auto Local Threshold‚Ä¶\n\nMethod: Otsu\n\nRadius: 15\n\n\nLocal Otsu mask:\n\n\n\nLocal Otsu mask\n\n\n\n\n\n\nFill holes inside objects:\nProcess ‚Üí Binary ‚Üí Fill Holes\nSlightly shrink objects:\nProcess ‚Üí Binary ‚Üí Erode\nOptionally restore outlines:\nEdit ‚Üí Draw\nExpand objects after erosion:\nProcess ‚Üí Binary ‚Üí Dilate\n\nRefined mask:\n\n\n\nRefined mask\n\n\n\n\n\n\nRemove partial objects touching image borders:\nPlugins ‚Üí MorphoLibJ ‚Üí Filtering ‚Üí Kill Borders\n\n\n\n\nFilled + border-removed mask\n\n\n\n\n\n\nConvert each connected region into a uniquely labeled object:\nPlugins ‚Üí MorphoLibJ ‚Üí Label ‚Üí Connected Components Labeling\n\nConnectivity: 4\n\nOutput type: 16-bit\n\nLabeled objects:\n\n\n\nConnected components labeling\n\n\n\n\n\nQuantifying each object is often the main goal after segmentation.\nRun measurements:\nPlugins ‚Üí MorphoLibJ ‚Üí Analyze ‚Üí Analyze Regions\nThis extracts a number of morphological associated features. We will select:\n\nArea\n\nPixel count\n\nPerimeter\n\nCircularity\n\nEllipse geometry\n\nBounding box\n\n\n\n\nMorphometry table\n\n\n\n\n\nVisualize one measurement (e.g.¬†area) mapped onto the label image:\nPlugins ‚Üí MorphoLibJ ‚Üí Label Images ‚Üí Assign Measure to Label\nChoose Area and apply a colormap.\n\n\n\nArea visualization on labels\n\n\nUseful for:\n\nspotting unusually big/small objects\n\ndeciding filtering thresholds\n\nchecking measurement correctness\n\n\n\n\nRemove small objects based on size:\nPlugins ‚Üí MorphoLibJ ‚Üí Label Images ‚Üí Label Size Filtering\n\nOperation: Lower Than\n\nSize threshold: 100 px\n\n\n\n\nSize filtering",
    "crumbs": [
      "Practicals",
      "Post-processing, segmentation and labelling"
    ]
  },
  {
    "objectID": "practicals/practical_segmentation/index.html#d-segmentation-in-napari",
    "href": "practicals/practical_segmentation/index.html#d-segmentation-in-napari",
    "title": "Post-processing, segmentation and labelling",
    "section": "3D segmentation in Napari",
    "text": "3D segmentation in Napari\nIn this exercise we will:\n\nUse Napari to open a 3D image\nUse Napari assistant to visualize a workflow for 3D image segmentation and labelling\nUse region props to quantify morphological parameters and make colorcoded plots\n\nThese steps form a complete workflow:\nraw image ‚Üí pre-processing ‚Üí segmentation ‚Üí cleanup ‚Üí labeling ‚Üí measurement ‚Üí filtering\nWe‚Äôll use Lund.tif as the example image https://zenodo.org/records/17986091.\nRequirements: - Everything you need is in the toml file in the Pixi/napari-assistant folder https://github.com/cuenca-mb/pixi-napari-assistant\n\n0. Open Napari assistant using Pixi\nIn the terminal, go to the directory Pixi/napari-assistant and run:\npixi run assistant\n\n\n1. Open a 3D stack\nDrag and drop the file or\nFile ‚Üí Open File\n\n\n\nBrightness/Contrast histogram\n\n\nWe will be able to see and explore the stack, and we can change to a 3D rendering with the option Toogle 2D/3D view in the lower left button pannel. We can also make orthogonal views by clicking the button to the right Change order of the visible axis.\nIn the right pannel we will see the Assistant plugin, where it suggests operations in the appropriate order. The amount of operations and options depends on your installed plugins. Some of them are redundant.\n\n\n2. Remove background, binarization and labeling\nSelect Remove Background ‚Üí White top hat  ‚Üí radius = 10\n\n\n\nBrightness/Contrast histogram\n\n\nThen select Binarize ‚Üí Threshold Yen, making sure to select the Result of White top-hat image.\n\n\n\nBrightness/Contrast histogram\n\n\nI recommend looking at the result in 3D.\n\n\n\nBrightness/Contrast histogram\n\n\nFinally, we can select Label ‚Üí Connected component labeling, make sure to select the Result of Threshold image. We can additionally select the exclude on edges option.\n\n\n\nBrightness/Contrast histogram\n\n\nSome of them are stuck together. Let‚Äôs try and fix that.\n\n\n3. Fix labels\nLet‚Äôs select again the previous layer Result of Threshold. Then select Process labels ‚Üí Binary erosion ‚Üí radius = 3. This will reduce the objects of the binary segmentation.\n\n\n\nBrightness/Contrast histogram\n\n\nNow let‚Äôs recreate the labels Label ‚Üí Connected component labeling, make sure to select the Result of Binary Erosion.\nThen Process labels ‚Üí Expand Labels ‚Üí radius = 3. Explore the labels in 3D.\n\n\n\nBrightness/Contrast histogram\n\n\nNow we can accurately measure morphological features of these labels. You can close the assistant pannel now.\n\n\n4. Measure morphological properties\nSelect Tools ‚Üí Measure Tables ‚Üí Object Features/Properties. Here make sure to select the Result of Expanded Labels image. You can select different features, includding intensity features extracted from the raw data. After running a table should appear which can be exported in csv format.\n\n\n\nBrightness/Contrast histogram\n\n\n\n\n\nBrightness/Contrast histogram\n\n\nby double clicking any of the columns of this table, a new layer image will appear with colorcoded labels indicating the value of the selected measurement. Colormaps can be adjusted for preference.\n\n\n\nBrightness/Contrast histogram",
    "crumbs": [
      "Practicals",
      "Post-processing, segmentation and labelling"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html",
    "href": "practicals/practical_tracking/index.html",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "",
    "text": "This is a (very) basic tutorial on how to track cells using Mastodon (Girstmair et al. 2025) in Fiji (Schindelin et al. 2012).\n\nObjectives: load/create Mastodon dataset, get familiar with navigating BigDataViewer and lineage windows, perform basic manual cell tracking with cell divisions, basic editing of lineages, try semi-automated detection and tracking, and some advanced analysis\nWe will use the Mastodon dataset from Girstmair, J. (2024). Mastodon Auto-Tracking Demo on Parhyale hawaiensis Limb Development. Zenodo. https://doi.org/10.5281/zenodo.13944688\nThe original data is from this paper https://elifesciences.org/articles/34410\nMastodon has a detailed documentation. Please check it out for more details https://mastodon.readthedocs.io/",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-summary",
    "href": "practicals/practical_tracking/index.html#sec-summary",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "",
    "text": "This is a (very) basic tutorial on how to track cells using Mastodon (Girstmair et al. 2025) in Fiji (Schindelin et al. 2012).\n\nObjectives: load/create Mastodon dataset, get familiar with navigating BigDataViewer and lineage windows, perform basic manual cell tracking with cell divisions, basic editing of lineages, try semi-automated detection and tracking, and some advanced analysis\nWe will use the Mastodon dataset from Girstmair, J. (2024). Mastodon Auto-Tracking Demo on Parhyale hawaiensis Limb Development. Zenodo. https://doi.org/10.5281/zenodo.13944688\nThe original data is from this paper https://elifesciences.org/articles/34410\nMastodon has a detailed documentation. Please check it out for more details https://mastodon.readthedocs.io/",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-requirements",
    "href": "practicals/practical_tracking/index.html#sec-requirements",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "Requirements",
    "text": "Requirements\n\nParhyale dataset\nFiji/ImageJ\nMastodon",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-setup",
    "href": "practicals/practical_tracking/index.html#sec-setup",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "Setup",
    "text": "Setup\n\nDownload Dataset\nNote! If you are following this during the course, the dataset has already been downloaded.\n\nGo to https://zenodo.org/records/13944688\nClick to download https://zenodo.org/records/13944688/files/Mastodon_Auto-Tracking_Demo_Ph-limb-dev.zip?download=1\nWait. It‚Äôs a 4.3GB ZIP file\nMove the file to a working directory\nUnzip Mastodon_Auto-Tracking_Demo_Ph-limb-dev.zip\nWait. It‚Äôll be unzipped to 23GB\n\n\n\n\n\n\n\nYour working directory after downloading and unzipping the dataset file.\n\n\n\n\n\n\n\nContents of the dataset directory.\n\n\n\n\n\n\n\nDownload Fiji\n\nGo to https://fiji.sc\nChoose Distribution: Stable\nClick the big download button\nCopy fiji-stable-linux64-jdk.zip to working directory and unzip it\nOpen the new directory fiji-stable-linux64-jdk/Fiji.app/\nDouble-click on fiji-linux-x64 launcher\nFiji will open\n\n\n\n\nInstall Mastodon\n\nClick on Help &gt; Update...\n\n\n\nThe updater will open and say Fiji is up-to-date\nClick Manage Update Sites\n\n\n\n\n\n\n\n\n\n\n\n\nA window will open with a list of plugins available to install in Fiji\n\n\n\nSearch for ‚Äúmastodon‚Äù\n\n\n\nSeveral Mastodon related plugins will appear\nClick on the checkbox for Mastodon\n\n\n\nClick Apply and Close and then Apply Changes\n\n\n\nWait‚Ä¶ until the downloads are finished. Then, click OK\n\n\n\n\n\n\n\n\n\n\n\n\nRestart Fiji (close window and double-click the launcher)",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-open-project",
    "href": "practicals/practical_tracking/index.html#sec-open-project",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "Open Mastodon Project",
    "text": "Open Mastodon Project\n\nIn Fiji click on Plugins &gt; Tracking &gt; Mastodon &gt; Mastodon Launcher\n\n\n\nMastodon Launcher window will open\nClick on ‚Äúopen Mastodon project‚Äù (top left) and ‚ÄúOpen another project‚Äù (bottom right)\n\n\n\n\n\n\n\n\n\n\n\n\nNavigate to the directory Mastodon_Auto-Tracking_Demo_Ph-limb-dev/\nSelect the file Parhyale_LimbDev_30tps.mastodon\n\n\n\nSeveral new windows will open (Console, Mastodon, BigDataViewer, TrackScheme, Data table)",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-inspect-dataset",
    "href": "practicals/practical_tracking/index.html#sec-inspect-dataset",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "Inspect the Dataset",
    "text": "Inspect the Dataset\n\nLet‚Äôs focus on the Mastodon window. Close Console, BigDataViewer, TrackScheme, Data table\n\n\n\nThis is the main project menu from where you can open windows, set options, process data and save the project\nThe most important buttons for this tutorial are bdv (BigDataViewer) and trackscheme\nClick on bdv and make the window larger\n\n\n\nDrag the timepoint slider at the bottom to see cells moving and dividing\nUsing your acquired BigDataViewer skills, focus on the surface of the embryo\nIf you get lost, press Shift+Z to re-orient the embryo\nFind a cell that divides before timepoint 15 and looks trackable\nZoom on it using Ctrl+Shift+scroll and center it by holding the right button and dragging the mouse\nUse Shift+scroll to navigate through z and M-N to go through time\n\n\n\n\n\n\n\n\n\n\n\n\nYou are ready to track",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-manual-tracking",
    "href": "practicals/practical_tracking/index.html#sec-manual-tracking",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "Manual Tracking",
    "text": "Manual Tracking\n\nOn the Mastodon project window, click on the trackscheme button\nThe TrackScheme window will appear\n\n\n\nResize the bdv window to be side-by-side with the TrackScheme\nClick on the bdv window, set the timepoint to some frames before mitosis, Shift+scroll to find the center of the nucleus, put your mouse pointer there and press A\nA round magenta circle will appear over the nucleus and in the TrackScheme\n\n\n\nWith your mouse over the circle, use Shift+Q and Shift+E to adjust the size of the spot to roughly the nucleus diameter\n\n\n\nZoom in on the new spot in the TrackScheme, hover and click on it and watch what happens in the bdv window\n\n\n\nGo back to the bdv, hover the pointer over the circle and hold the spacebar to adjust the position of the circle and the nucleus\n\n\n\nNow let‚Äôs add a second spot\nHover the mouse inside the circle and hold A. This will advance to the next frame showing you the first spot in white dashed line and the second spot in white solid line with a white solid link between the two.\n\n\n\nStill holding A, position the second spot, then release A to create the new linked spot\nCheck how the second spot and a link were created in the TrackScheme automatically\n\n\n\nContinue to track the nucleus for a few more frames, until the frame immediately before division\nNote that when you click on a spot in the bdv window, the corresponding spot is highlighted in the TrackScheme window\nSee what happens to the bdv when clicking the spots in TrackScheme‚Ä¶ (nothing, unless it is the spot in view)\nLet‚Äôs change that\nIn the menu bar of bdv and TrackScheme windows there are locks 1, 2, 3. Click on lock 1 in both windows\n\n\n\nNow click through spots in the TrackScheme; the view in the bdv will change to show the selected spot at the center\n\n\n\nBefore we continue tracking the cell division, let‚Äôs check one of the amazing Mastodon features\nClick on the bdv button in the Mastodon project window and another bdv window will open\n\n\n\nNow activate lock 1 and click on one of the TrackScheme spots; both windows will be synchronized!\n\n\n\nWhy is this useful for manual tracking?\nAdjust the view to center the spot in the second bdv window, press Shift+Y, and select a spot from the TrackScheme. Now we have both XY and ZY views of the same nucleus!\n\n\n\nWhich is great for tracking in 3D. You can check, for instance, that your spot is well centered in Z and adjust it in this window\n\n\n\nContinue the tracking of one of the daughter cells. Select the last spot in the TrackScheme, go to the XY bdv, hover the mouse over the circle and hold A, move the spot, and release A to add it.\nDo it for a few frames\nThen go back to the pre-division spot and add a linked spot corresponding to the other daughter cell\nThis will create the first branch of the lineage tree\n\n\n\nContinue tracking the second daughter cell for a few frames\n\n\n\nIf you zoom out the TrackScheme view you will be able to see the full branched tree",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-semiauto-tracking",
    "href": "practicals/practical_tracking/index.html#sec-semiauto-tracking",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "Semi-Automated Tracking",
    "text": "Semi-Automated Tracking\n\nThis mode will try to guess where the next nucleus is and automatically create the spots and links\nTo start, choose a different nucleus to track and press A to add a new spot\n\n\n\nNow, hovering the pointer above the spot press Ctrl+T\nA lineage will appear in the TrackScheme.\n\n\n\nCheck how accurate it is by clicking on the spots and watching their position relative to the nucleus in the bdv windows\nTry going further by hovering on a spot and pressing Ctrl+T to continue the semi-automated tracking. See how long you can go, how it behaves with cell divisions, and which cells work well with it and which don‚Äôt\n\n\n\nThere are many parameters that can be adjusted to tweak the semi-automated tracking behavior, check the documentation",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-auto-tracking",
    "href": "practicals/practical_tracking/index.html#sec-auto-tracking",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "Automated Tracking",
    "text": "Automated Tracking\n\nThe final part of this tutorial is to try automatic detection and linking of spots\nThis is the dream: loading your data and getting out your lineage. However, in practice, it‚Äôs a lot messier. Cleaning, fixing, and curating the data is required to get a nice informative lineage\nWe will use a simplified version of the protocol included in this dataset in the file Protocol_Auto-Detection_Auto-Linking.docx\n\n\n\nDetection\n\nIn the Mastodon window, go to Plugins &gt; Tracking &gt; Detection...\n\n\n\nPress Next twice (leave options as is)\n\n\n\n\n\n\n\n\n\n\n\n\nChoose Advanced DoG detector and press Next\nKeep Detect: bright blobs, change Estimated diameter to 35px and Quality threshold to 100. Behavior should remain as Add\n\n\n\n\n\n\n\n\n\n\n\n\nClick Preview and see how well the detection will work by exploring a new bdv window.\n\n\n\nDo you observe too many false positives? You can change the diameter, for example, and try the preview again to see what happens to the detected spots\nOnce satisfied, press Next and wait\n\n\n\nWhen the detection is done, press Finish\n\n\n\nThe bdv windows and the TrackScheme will be showing a lot of new spots.\nExplore the spots in the BigDataViewer and TrackScheme windows. If you zoom in a lot you‚Äôll see the unlinked, individual spots per frame\n\n\n\n\nCleaning\n\nBefore we try to automatically link these spots, let‚Äôs remove low quality detections\nOn the Mastodon window click on Table, resize it to have more space, and resize the column Detection q... to show Detection quality\n\n\n\n\n\n\n\n\n\n\n\n\nClick on Detection quality to sort the table\n\n\n\nClick on the first row to select it. Select all rows where Detection quality is &lt;400\nThen click Edit &gt; Delete Selection\n\n\n\nClose the table\nYou can also manually delete obviously wrong spots by hovering and pressing D. Clean up the ones outside the embryo\n\n\n\nLinking\n\nNow let‚Äôs try linking\nIn the main Mastodon window click on Plugins &gt; Tracking &gt; Linking...\nKeep All spots selected for all timepoints (0-29) and press Next\nChoose Lap linker and click Next\n\n\n\n\n\n\n\n\n\n\n\n\nChange the parameters to:\n\nFrame to frame linking: Max distance to 40px\nGap closing: Max distance to 60px (keep others as is)\nTrack division: Check Allow track division, set Max distance to 40px, and press + to add a Feature penalty and set it to Center ch1 to 0.3\n\nPress Next to start linking and wait‚Ä¶ then press Finish\n\n\n\n\n\n\n\n\n\n\n\n\nNote that there are now tracks in the bdv and TrackScheme windows. Explore them a bit\n\n\n\nMastodon can calculate features (position, displacement, velocity, etc.) of individual spots, links and branches. Let‚Äôs do that\nIn the main Mastodon window press compute features. A Feature calculation window will open\n\n\n\nPress compute and wait‚Ä¶ when it‚Äôs done, close it.\nNote that now the tracks in the BigDataViewer is showing colored links\n\n\n\nOpen the table window from the main window\nIt‚Äôll be filled with computed features",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-feature-visualization",
    "href": "practicals/practical_tracking/index.html#sec-feature-visualization",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "Basic Feature Visualization",
    "text": "Basic Feature Visualization\n\nFinally, let‚Äôs visualize the computed features that might be interesting or useful\nIn the bdv window press File &gt; Preferences to open the feature color coding visualization parameters\n\n\n\nOn Settings &gt; Feature Color Modes click on Duplicate (it‚Äôll generate a Number of links (2)) and then Rename. Rename it to Velocity\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Coloring Spots change Read spot color from to Incoming link and change Feature to Link velocity. Then click on autoscale in the range\nOn the Coloring Links change Read link color from to Link and change Feature to Link velocity. Then also click on autoscale\nClick Apply (nothing will happen) then OK\n\n\n\nOn the bdv window press View &gt; Coloring &gt; Velocity\n\n\n\nThe spots and links in the BigDataViewer window will change colors\n\n\n\nDo the same for the other bdv window and the TrackScheme\n\n\n\nThis gives a visual representation of cells which have a high displacement per frame. These might be artifacts in linking unrelated spots or, in a good processed lineage, reveal some biological process like cell migration",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-graph-plotting",
    "href": "practicals/practical_tracking/index.html#sec-graph-plotting",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "Graph Plotting",
    "text": "Graph Plotting\n\nTo finalize, a simple example of plotting\nClick on grapher in the main Mastodon window, a plot window will open\n\n\n\nPress the lock 1 to lock the windows, select Link velocity - outgoing for X axis and Detection quality for Y axis and press Plot\n\n\n\nFind out if the spots with the highest link velocity are properly linked or if it is an artifact",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_tracking/index.html#sec-references",
    "href": "practicals/practical_tracking/index.html#sec-references",
    "title": "Cell tracking using Mastodon in Fiji",
    "section": "References",
    "text": "References\n\n\nGirstmair, Johannes, Tobias Pietzsch, Vladimir Ulman, Stefan Hahmann, Matthias Arzt, Mette Handberg-Thorsager, Ko Sugawara, et al. 2025. ‚ÄúMastodon: The Command Center for Large-Scale Lineage-Tracing Microscopy Datasets.‚Äù bioRxiv, December, 2025.12.10.693416. https://doi.org/10.64898/2025.12.10.693416.\n\n\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. ‚ÄúFiji: An Open-Source Platform for Biological-Image Analysis.‚Äù Nat. Methods 9 (June): 676‚Äì82. https://doi.org/10.1038/nmeth.2019.",
    "crumbs": [
      "Practicals",
      "Cell tracking using Mastodon in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html",
    "href": "practicals/practical_multiview/index.html",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "",
    "text": "Multiview reconstruction is the process of registering and fusing microscopy data acquired from multiple angles into a single, isotropic stack.\nThis is a tutorial on how to register and fuse multiview lightsheet microscopy datasets using the plugin BigStitcher (Preibisch et al. 2010; H√∂rl et al. 2019) in Fiji (Schindelin et al. 2012).\nWe will cover how to convert the raw data for visualization in the BigDataViewer (Pietzsch et al. 2015), how to best detect interests points for registration, the different approaches to register views, and how to fuse the views to reconstruct an isotropic dataset.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-summary",
    "href": "practicals/practical_multiview/index.html#sec-summary",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "",
    "text": "Multiview reconstruction is the process of registering and fusing microscopy data acquired from multiple angles into a single, isotropic stack.\nThis is a tutorial on how to register and fuse multiview lightsheet microscopy datasets using the plugin BigStitcher (Preibisch et al. 2010; H√∂rl et al. 2019) in Fiji (Schindelin et al. 2012).\nWe will cover how to convert the raw data for visualization in the BigDataViewer (Pietzsch et al. 2015), how to best detect interests points for registration, the different approaches to register views, and how to fuse the views to reconstruct an isotropic dataset.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-requirements",
    "href": "practicals/practical_multiview/index.html#sec-requirements",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Requirements",
    "text": "Requirements\n\nMultiview lightsheet dataset\nFiji/ImageJ\nBigStitcher plugin",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-setup",
    "href": "practicals/practical_multiview/index.html#sec-setup",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Setup",
    "text": "Setup\n\nDefine working directory\n\nCreate a directory in your computer to put the files needed for this tutorial.\n\n\n\nDownload dataset\n\nGo to Zenodo URL and download 5angles-beads dataset\nUnzip the file to your working directory\nYou should see a single CZI file named Dmel_Gap43-mCherry_5Angles_2Channels_1Timepoint_60Slices_Beads\nAs described in the file name, this single CZI file contains 5 different angles, each with 2 channels and a single timepoint with 60 z-slices\nThe XY resolution is 0.276 ¬µm and the Z resolution is 3 ¬µm\nThe dataset also has fluorescent beads around the sample, which we will use to register the views\nThe dataset is ready\n\n\n\nDownload Fiji and BigStitcher\n\nGo to https://fiji.sc\nChoose Distribution: Stable\nClick the big download button\nCopy fiji-stable-linux64-jdk.zip to working directory and unzip it\n\n\n\nOpen the new directory fiji-stable-linux64-jdk/Fiji.app/\nDouble-click on fiji-linux-x64 launcher\nFiji will open\n\n\n\nClick on Help &gt; Update‚Ä¶\n\n\n\nThe updater will run and open open and say if Fiji is up-to-date\nClick Manage Update Sites\n\n\n\nA window will open with a list of plugins available to install in Fiji\n\n\n\nFind BigStitcher in the list and click on the checkbox\nClick Apply and Close\n\n\n\nThen Apply Changes\n\n\n\nWait‚Ä¶ until the downloads are finished. Then, click OK\n\n\n\nRestart Fiji (close window and double-click the launcher)\nCheck if BigStitcher is installed under Plugins &gt; BigStitcher\n\n\n\nYou are ready!",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-inspect-dataset",
    "href": "practicals/practical_multiview/index.html#sec-inspect-dataset",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Inspect dataset",
    "text": "Inspect dataset\n\nNow let‚Äôs first inspect the dataset in Fiji\n\n\nOpen file\n\nDrag and drop the CZI file in Fiji‚Äôs main window\n\n\n\nA Bio-Formats Import Options should open\nThat‚Äôs the default importer for proprietary file formats\nThere are many options but for now simply click OK\n\n\n\nA Bio-Formats Series Options window will open\nBio-Formats recognized that this file contains more than one view (series) and is asking which ones do we want to open\nWe just want to inspect one view since they will be quite similar, so simply press OK\n\n\n\n\nAdjust contrast\n\nA big window with a black background will open\nCheck if the dimensions were correctly assigned (information line at the top and sliders at the bottom)\n\n\n\nTo see something, we first need to adjust the levels\nOpen the Brightness/Contrast (B&C) tool with Image &gt; Adjust &gt; Brightness/Contrast‚Ä¶ (or ctrl+shift+c) and the Channels Tool with Image &gt; Color &gt; Channels Tool‚Ä¶ (or ctrl+shift+z)\n\n\n\nPress Reset to adjust the levels of Channel 1 then slide the Z position to the middle of the sample and press reset again\n\n\n\n\n\n\n\n\n\n\n\n\nNow move the Channel slider to Channel 2 and press Reset\n\n\n\nIn the Channels window change the menu Color to Composite\n\n\n\nThe sample is ready to be visualized\n\n\n\nOpen orthogonal views\n\nTo get a sense of the data tridimentionality we want to look at the XY, XZ, and YZ optical sections\nClick on Image &gt; Stacks &gt; Orthogonal Views (or ctrl+shift+H)\nIt takes a moment. XZ and YZ panels will open\n\n\n\nResize the main window to fit the screen\nThe sample is a fly embryo which resembles a cylinder in 3D\nExplore the dataset by clicking and sliding the mouse pointer through the different images\n\n\n\nWhen done, please close the stack",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-define-dataset",
    "href": "practicals/practical_multiview/index.html#sec-define-dataset",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Define dataset",
    "text": "Define dataset\n\nBefore we begin, we need to define a multiview dataset and resave the dataset\nDefining multiview dataset will create an XML file where all the dataset metadata and information and data from the registration process will be stored\nGo to Plugins &gt; BigStitcher &gt; General &gt; Define Multi-View Dataset\n\n\n\nA window named Choose method to define dataset will open\nOn the Define Dataset using field choose Zeiss Lightsheet Z.1 Dataset Loader (Bioformats) from the dropdown menu, since our testing dataset is from Zeiss Lightsheet Z.1, then press OK\n\n\n\nClick browse in the next window, select the CZI file, and press OK\n\n\n\nBigStitcher will read the metadata of the CZI and show a dialog with the details\nCheck that five angles are present, that there are two channels and that the XYZ resolution matches the expected values (see above)\nThen press OK\n\n\n\nIf you look into the working directory, an XML file named dataset will have appeared there\n\n\n\nYou can open this file in a text editor to see the information stored there\nRight-click the file, select Open With‚Ä¶ and choose Text Editor\nThe file has the file name, the image dimensions, the XYZ resolution, etc\n\n\n\nNote that the XML only stores metadata from the dataset and not the actual image data",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-resave-dataset",
    "href": "practicals/practical_multiview/index.html#sec-resave-dataset",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Resave dataset",
    "text": "Resave dataset\n\nNext, we need to convert the actual image, which is still stored in the CZI, to a format that allow us to open and visualize this heavy dataset in an efficient and lightweight manner\nFor that, we will resave the data into HDF5 format\nGo to Plugins &gt; BigStitcher &gt; I/O &gt; Resave as HDF5 (local)\n\n\n\nThe new window Select dataset for Resaving as HDF5 will automatically load the last used XML file, in this case, our dataset.xml\nYou can choose whether you want to convert every angle, all channels, all timepoints, or only a subset of those\nWe want it all, press OK\n\n\n\nAnother window will appear with some resaving options\nLeave the options as is, but make sure that the Export path is pointing to the dataset.xml file (click on Browse and, if the file is not selected, navigate and select dataset.xml)\nPress OK and wait‚Ä¶\n\n\n\nResaving this dataset takes about 3 min. But consider that larger datasets will take significantly longer (with several timepoints, for example)\nThe Log window will show that it‚Äôs done\n\n\n\nNote that another XML file named dataset.xml~1 and a new HDF5 file named dataset.h5 were created\nEvery time the dataset file is saved, BigStitcher creates a backup copy. dataset.xml~1 was the original dataset.xml which was renamed after the resaving\nIf you inspect the new dataset.xml in a Text Editor you will see that it now points to the dataset.h5 file\n\n\n\nWhenever we refere to the multiview dataset we are referring to the XML/HDF5 pair",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-visualize-dataset",
    "href": "practicals/practical_multiview/index.html#sec-visualize-dataset",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Visualize dataset",
    "text": "Visualize dataset\n\nWe can finally open the main BigStitcher application\n\n\nStart BigStitcher\n\nGo to Plugins &gt; BigStitcher &gt; BigStitcher\n\n\n\nThe last dataset.xml file will be automatically loaded in the select dataset window, click OK\n\n\n\nThis will open two windows, the BigDataViewer and the Multiview Explorer\n\n\n\nThe Multiview Explorer shows a table with the individual views of the dataset. We have 5 views, each with 2 channels. Therefore, we have in total 10 views.\nClicking in a row will show the data in the BigDataViewer. You can select multiple rows freely. You can also sort the table by channel or angle for example\nSelect the five views from Channel 561 (channel 2)\n\n\n\nThe image is too bright, we need to adjust the contrast\nFor that, go to Settings &gt; Brightness & Color\nA new window will open\n\n\n\nChange the max value of channel 2 to 2500\n\n\n\nNow we can visualize the dataset in more detail\n\n\n\nLearn BigDataViewer\n\nIt is important to familiarize yourself with the BigDataViewer commands and shortcuts\nBigDataViewer is very intuitive to use but a quick look at the Help is important to not get lost\nSome of the most important commands are as follow:\nShift+X, Shift+Y, Shift+Z: That‚Äôs your compass. If you get lost, pressing one of these shortcuts will get you back to the original XY, YZ, ZX orientation\nIn this scope the rotation axis is Y\nTherefore, pressing shift+y will show you the separate angles\n\n\n\nHold left mouse button to rotate the data around the pointer\nHold right mouse button to drag the view\nCtrl+shift+scroll+up or down will zoom in/out fast (don‚Äôt use shift for normal speed)\nTip, select the Explorer window with the five views selected and press C. This will autocolor the views which is great for visualization\nTip, select the BigDataViewer and press i to activate interpolation for better visualization\n\n\n\n\n\n\n\n\n\n\n\n\nTake some time to explore the data, the different views, zoom in and out, find the beads, and get familiar with the BigDataViewer\nThen finish with the 5-views oriented as in the image",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-detect-points",
    "href": "practicals/practical_multiview/index.html#sec-detect-points",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Detect points",
    "text": "Detect points\n\nWe can now start the first processing step of the pipeline, detecting points of interest to be used for the registration\nThe Multiview Explorer is always the starting point\nSelect the five views of channel 561 (if not selected)\nIn this case, we want only the views of channel 2 to be selected since it is in this channel that the beads are visible\nThen, then right click the rows with the mouse\nA long menu will appear. This is the main menu of BigStitcher. Anything that we select will only be applied to the selected views.\nAfter right-clicking, find the section Processing and select Detect Interest Points‚Ä¶\n\n\n\nIn this first window we can choose the type of detection (Default: Difference of Gaussian) and a label to describe these interest points\nOne dataset can have multiple sets of interest points\nWe will keep the default options and press OK\n\n\n\nWe can set different parameters for the Difference of Gaussian approach\nWhat is important to us at this point is to make sure that Interactive‚Ä¶ is set for Interest point specification and that you change Downsample XY from Match Z Resolution (less downsampling) to 2x. The latter is not essential for all datasets but it works better for this dataset which has relatively high anisotropy (lower Z resolution compared to XY). Keep Downsample Z as 1x.\nPress OK\n\n\n\nIn case, we can select which view we will open the interactive window and if we will load the entire stack. Simply press OK to load the entire first view\n\n\n\nA stack will open with a rectangular ROI placed at the top left corner\nThe ROI shows a live view of detected points for the current parameters Sigma and Threshold present in the other window\n\n\n\nSigma is the size (radius) of the point and Threshold is the intensity-based cut value to discard low-quality detections\nThe default is to Find DoG maxima (red) or, in other words, bright spots. You can also find dark spots surrounded by bright areas (useful for some samples)\nThis is a normal ImageJ window, so you can zoom, adjust contrast, and if you lose the ROI you can simply draw a new rectangular ROI\nYou can drag the ROI around by clicking inside it and holding/dragging it\n\n\n\nWhat we want to do now is to adjust the Sigma and Threshold so that most of the beads outside the embryo are properly detect with the least of spurious detections\nThe best way to begin is to zoom in into a bead outside the embryo and check if the circle size is matching well the bead. Move the Sigma slider, aiming for a circle slightly larger than the bead\nRemember to also move through the Z slices of the stack to see how the detection behaves\nIf the size looks good, now increase the Threshold until real beads begin to not be detect. Once you reach this point roll the slider back until most beads are detected\nNote that, no matter how much you tweak these parameters, there‚Äôll always be many detections in the sample tissues. These non-bead detections will not have a strong influence on the registration given that you have enough detected real beads\nOnce satisfied, press Done\n\n\n\nBead detection takes some time\nOnce DONE, go to the Multiview Explorer window and press Save\n\n\n\nThis is important because the detections are initially saved in memory and will be only written to disk after pressing Save (detections are saved in a directory named interestpoints.n5)\nYou can notice that the column #InterestPoints in the Multiview Explorer now shows 1 for the five views of channel 561 (but not for channel 488)",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-register-views",
    "href": "practicals/practical_multiview/index.html#sec-register-views",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Register views",
    "text": "Register views\n\nNow we can try to register these views using the detected interest points\nWith the 5-views selected, right-click and run Register using Interest Points‚Ä¶\n\n\n\nWe can change different registration parameters like the algorithm to be used or the specific set of previously interest points\nWe want to go with the default Fast descriptor-based (rotation invariant) Registration algorithm as it works well for bead-based registration with Z.1 datasets\nSince our views are very further apart with almost no overlap, we want to change the option Registration in between views to Compare all views against each other\nLeave the other options as is making sure we are using the Interest points labeled as beads\nClick OK\n\n\n\nA novel window will open with several other parameters to tweak. Please refer to the BigStitcher documentation for the specific function of these\nFor us, it is important to note two. The option Fix views set to Fix first view means that all other views will be mapped to the first angle. And the Transformation model set to Affine means that the data will be transformed non-rigidly to fit the individual views. This is important since different portions of the stack might have a certain degree of distortion from the objective lenses and an affine transformation helps to fit the views better together\nThe other parameters we will only need to change if our registration fails\nPress OK\n\n\n\nThis small window with Regularization Parameters can be kept as is (Rigid and 0.10). Press OK\nSame for the interest point grouping options. Press OK\n\n\n\n\n\n\n\n\n\n\n\n\nThe registration will begin and be over in a few seconds. Don‚Äôt blink or you will miss it! If successful, you will see that the individual views will now have moved over (registered) the first view and they are all overlapping in the BigDataViewer window\n\n\n\n\n\n\n\n\n\n\n\n\nNow that the views are registered, explore the dataset to verify that the registration worked well. The best way to do this is visually\nOne of the first things that you can do is to press shift+y and zoom in into a bead close to the embryo‚Äôs surface\n\n\n\nYou will see the point spread function of one bead in each individual view forming a star with generally four views (as the fifth view is too further away).\nIf the sample is registered well, the center of the point spread functions of the different views should match in the middle of the star\n\n\n\nAnother thing that you can do is to find a structure you know well in the sample and check that the tissues are actually registered. It can happen that the beads are nicely registered, but the tissues themselves are a bit off\nOne way to do this is to select only two contiguous views and check them closely\nWhen done, make sure to Save the project again\n\n\n\nAfter saving, the #Registrations column should now show the number 3 for the selected views (if not deselect and select them again to update the counter)",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-set-bounding",
    "href": "practicals/practical_multiview/index.html#sec-set-bounding",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Set bounding box",
    "text": "Set bounding box\n\nOur dataset is registered, but before fusing the views it is important to set a bounding box around the sample. This reduces the final dimensions and file size of the fused data.\nFor that, right-click and select Define Bounding Box‚Ä¶\n\n\n\nWe want to define it interactively, so leave the Bounding Box option as isotropic\nYou can give the bounding box a custom name and define different bounding boxes for different purposes, but the default name is good enough for this tutorial. Click OK\n\n\n\nTwo windows will open: BigDataViewer with the sample and some purple shade and a bounding box window full of sliders\n\n\n\nFor defining the bounding box I follow a specific procedure, always in the same order, to avoid inadvertently leaving out a part of your sample when fusing\nFirst, press shift+x to orient the sample on XY\n\n\n\nGo through the sample (Z) top to bottom to get a sense of the entire volume and stop back at the middle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMove the x min slider to the right to cut out the region on the left of the sample (the dashed line is the reference edge). Get close to the sample, but leave a gap\n\n\n\nOnce the placed x min, go again top to bottom through Z to make sure nothing was cut out\nNow do the same of x max to cut out the region on the right side of the sample\n\n\n\nNext we want to cut a bit from the top and bottom regions\nMove the slider y min to cut from the top and y max to cut from the bottom. Remember to go through Z to make sure it is not cutting the tip off the embryo (it happens)\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, press shift+y to cut out the exceeding portions in the Z direction\n\n\n\nUse z min to cut from the top (in this orientation), always going through Y to check!\nThen adjust z max to cut from the bottom (in this orientation), also going through Y.\n\n\n\n\n\n\n\n\n\n\n\n\nWhen done, press OK in the bounding box window\n\n\n\nThe dimensions of the interactively defined bounding box and the estimated sized of the fused image will appear.\nPress OK and Save",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-fuse-single",
    "href": "practicals/practical_multiview/index.html#sec-fuse-single",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Fuse dataset (one channel)",
    "text": "Fuse dataset (one channel)\n\nFinally, let‚Äôs begin the fusing of registered views\nSelect the five registered views in the Multiview Explorer, right-click and press Image Fusion‚Ä¶\n\n\n\nThe window with fusion options will appear\n\n\n\nOur ‚ÄúMy Bounding Box‚Äù is automatically selected for the Bounding Box\nWe can choose to downsample the fused image. I highly recommend downsampling 2x, 4x, or even 8x, depending on the dataset, if you are fusing a dataset for the first time.\nFusing without downsampling (1x) can take a really long time for large samples (many hours), so it is good practice to downsample the fused image to make sure the fusing parameters are good for your dataset before fusing the whole thing.\nBut for this tutorial, you can leave at 1x\nFor Interpolation leave Linear interpolation as it gives better outputs\nFusion type is an important parameter to choose wisely\nThe simplest is Avg, which averages the signal of every view per pixel. This is quick but it is combining the good contrast of one view with the blurred side of another view and the resulting contrast will be suboptimal.\nAvg, Blending is the same as Avg, but it blends smoothly the edges of the different views giving a slightly better fusing than simple Avg\nAvg, Blending & Content Based improves the other two options by adding a step that checks and keeps only the best information for each coordinate (keep good contrast, discard blurred information). This option gives the best results. However, it is also the one that requires more memory and takes longer to finish (much longer)\nTherefore, I would start with 2x or 4x downsample using Avg, Blending before trying less downsampling and the content based fusion\nFor the Pixel type I often use 16-bit, but it depends on what is your goal with the fused image. If it is only to have a volume visualization, 8-bit might be enough. If further processing and analysis is expected, definitely go for 16-bit or, in special cases, 32-bit.\nBigStitcher also has an option for using the interest points information during the fusion step to obtain better results (Non-Rigid fusion). This is for advanced users and I have not tried it enough to have an opinion about it.\nGenerally, we want to have one fused image per timepoint per channel\nAnd I always Save as (compressed) TIFF stacks for the Fused image option. Choosing Display using ImageJ can be dangerous as the fused image will be large and your computer can run out of memory and crash. Writing to disk is safer.\nAfter pressing OK there‚Äôll be another window to define the min/max levels, but they are automatically detected. Press OK\n\n\n\nThe output directory will be the same where the dataset.xml is. You can add a Filename addition to distinguish different types of fusion and downsampling (useful when doing it multiple times)\nPress OK and fusion will start\n\n\n\nOnce done, drag and drop the fused dataset named avg_blend_1x_fused_tp_0_ch_1 in Fiji to open it and adjust the contrast with the Brightness/Contrast tool to see the data\n\n\n\n\n\n\n\n\n\n\n\n\nInspect the fusion result, checking for artifacts. If your sample has a membrane staining, for example, check for doubled membranes\nNote that this is an isotropic dataset",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-duplicate-transformation",
    "href": "practicals/practical_multiview/index.html#sec-duplicate-transformation",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Duplicate transformation",
    "text": "Duplicate transformation\n\nNow that we have successfully registered and fused the views of one channel, we can simply apply the series of transformations to the other channel without the need to detect interest points or register the channel independently\n\n\n\nYou can do so using the tool Duplicate Transformations from BigStitcher\nFirst, close the Multiview Explorer and the Select dataset window that pops-up\nThen go to Plugins &gt; BigStitcher &gt; General &gt; Tools &gt; Duplicate Transformations\n\n\n\nSelect the option One channel to other channels\n\n\n\nA Select dataset window will open with the last dataset.xml already opened. Press OK\n\n\n\nNow choose the source channel. Remember that we registered the Channel 561 (channel 2).\nThe Target channel(s) is All Channels (all the other channels except for the source one).\nThe last option, Duplicate which transformations is important. Generally, Replace all transformations work for most cases. However, I often prefer to use Add last transformation only. This will take the last transformation from the source channel and apply to the target channel.\nNote, however, that for this to work, the source channel can only be one transformation ahead of the target. If for instance, we ran two subsequent transformations for the source channel, then applying only the last would not duplicate all the transformations. Always check the #Registrations in the Multiview Explorer.\n\n\n\nOnce you press OK, the transformations will be applied in the XML file. It‚Äôs quick.\nNow open BigStitcher again and check if the 5 views of the other channel are registered (they should)",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#sec-fuse-all",
    "href": "practicals/practical_multiview/index.html#sec-fuse-all",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "Fuse dataset (all channels)",
    "text": "Fuse dataset (all channels)\n\nWe have now both channels registered, but only one fused\nTo fuse both channels select all the views in the Multiview Explorer\n\n\n\nThen set the desired parameters for fusion (optimized previously) and run the fusion again as described above\nThis time there will be two files as output: avg_blend_1x_fused_tp_0_ch_0.tif and avg_blend_1x_fused_tp_0_ch_1.tif\n\n\n\nOpen both files in Fiji and adjust their contrast\n\n\n\nThen go to Image &gt; Color &gt; Merge Channels‚Ä¶\n\n\n\nSelect ch_0 for C1 and ch_1 for C2 and press OK\n\n\n\nA red-green 2-channel stack will open\nAs red-green isn‚Äôt good, use the LUT tool to update the colors to green for C1 and magenta for C2.\n\n\n\n\n\n\n\n\n\n\n\n\nWe can even compare this fused dataset with one single view of the original dataset.\nDrag and drop the CZI file, select the first view only to import, and put the stacks side-by-side for a comparison slice by slice",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "practicals/practical_multiview/index.html#references",
    "href": "practicals/practical_multiview/index.html#references",
    "title": "Multiview reconstruction using BigStitcher in Fiji",
    "section": "References",
    "text": "References\n\n\nH√∂rl, David, Fabio Rojas Rusak, Friedrich Preusser, Paul Tillberg, Nadine Randel, Raghav K Chhetri, Albert Cardona, et al. 2019. ‚ÄúBigStitcher: Reconstructing High-Resolution Image Datasets of Cleared and Expanded Samples.‚Äù Nat. Methods 16 (September): 870‚Äì74. https://doi.org/10.1038/s41592-019-0501-0.\n\n\nPietzsch, Tobias, Stephan Saalfeld, Stephan Preibisch, and Pavel Tomancak. 2015. ‚ÄúBigDataViewer: Visualization and Processing for Large Image Data Sets.‚Äù Nat. Methods 12 (June): 481‚Äì83. https://doi.org/10.1038/nmeth.3392.\n\n\nPreibisch, Stephan, Stephan Saalfeld, Johannes Schindelin, and Pavel Tomancak. 2010. ‚ÄúSoftware for Bead-Based Registration of Selective Plane Illumination Microscopy Data.‚Äù Nat. Methods 7 (June): 418‚Äì19. https://doi.org/10.1038/nmeth0610-418.\n\n\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. ‚ÄúFiji: An Open-Source Platform for Biological-Image Analysis.‚Äù Nat. Methods 9 (June): 676‚Äì82. https://doi.org/10.1038/nmeth.2019.",
    "crumbs": [
      "Practicals",
      "Multiview reconstruction using BigStitcher in Fiji"
    ]
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Help",
    "section": "",
    "text": "Find answers to your questions about the course.\n\n\n\nInstitute\nTransportation stop\nTheory sessions: ‚Ä¶\nCoffee breaks: ‚Ä¶\nPoster sessions: ‚Ä¶\nPractical sessions: ‚Ä¶\nLunch breaks: ‚Ä¶\nDinner?\n\n\n\n\n\nBackground and base for practicals\n\n\n\n\n\nTwo per computer\nDetails of systems\n\n\n\n\n\nDefine achievable goal based on your needs and on what you want to learn.\nUse your own data or example data provided by organizers\n\n\n\n\n\n8 min + 2 min questions\n\n\n\n\n\nTime, location, etc",
    "crumbs": [
      "Help"
    ]
  },
  {
    "objectID": "help.html#locations",
    "href": "help.html#locations",
    "title": "Help",
    "section": "",
    "text": "Institute\nTransportation stop\nTheory sessions: ‚Ä¶\nCoffee breaks: ‚Ä¶\nPoster sessions: ‚Ä¶\nPractical sessions: ‚Ä¶\nLunch breaks: ‚Ä¶\nDinner?",
    "crumbs": [
      "Help"
    ]
  },
  {
    "objectID": "help.html#theory-sessions",
    "href": "help.html#theory-sessions",
    "title": "Help",
    "section": "",
    "text": "Background and base for practicals",
    "crumbs": [
      "Help"
    ]
  },
  {
    "objectID": "help.html#practical-sessions",
    "href": "help.html#practical-sessions",
    "title": "Help",
    "section": "",
    "text": "Two per computer\nDetails of systems",
    "crumbs": [
      "Help"
    ]
  },
  {
    "objectID": "help.html#project-work",
    "href": "help.html#project-work",
    "title": "Help",
    "section": "",
    "text": "Define achievable goal based on your needs and on what you want to learn.\nUse your own data or example data provided by organizers",
    "crumbs": [
      "Help"
    ]
  },
  {
    "objectID": "help.html#group-presentations",
    "href": "help.html#group-presentations",
    "title": "Help",
    "section": "",
    "text": "8 min + 2 min questions",
    "crumbs": [
      "Help"
    ]
  },
  {
    "objectID": "help.html#group-dinner",
    "href": "help.html#group-dinner",
    "title": "Help",
    "section": "",
    "text": "Time, location, etc",
    "crumbs": [
      "Help"
    ]
  },
  {
    "objectID": "internal/quarto.html",
    "href": "internal/quarto.html",
    "title": "Quarto reference guide",
    "section": "",
    "text": "Here are some of the markdown basics we can use with Quarto.\n\n\nitalics, bold, bold italics, superscript2 / subscript2, strikethrough, verbatim code\n\nunordered list\n\nsub-item 1\nsub-item 2\n\nsub-sub-item 1\n\n\nTask 1\nTask 2\n\n\nterm\n\ndefinition\n\n\n\nBlockquote\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: An Elephant\n\n\n\nThis is illustrated well by Figure¬†1.\n\n\n\n\n\n\nTable¬†1: My Caption\n\n\n\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\n\n\n\nSee Table¬†1.\n\n\n\nLife is good (Weigert et al. 2018). (bibkey should be in references.bib)\n\n\n\nProgram Resources\n\n\n\nHere is a footnote reference,1 and another.2.\nHere is an inline note.3\n\n\n\ninline math: \\(E = mc^{2}\\)\ndisplay math:\n\\[E = mc^{2}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote callout block.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTip callout block.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWarning callout block.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nCaution callout block.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nImportant callout block.",
    "crumbs": [
      "Internal",
      "Quarto reference guide"
    ]
  },
  {
    "objectID": "internal/quarto.html#text-formatting",
    "href": "internal/quarto.html#text-formatting",
    "title": "Quarto reference guide",
    "section": "",
    "text": "italics, bold, bold italics, superscript2 / subscript2, strikethrough, verbatim code\n\nunordered list\n\nsub-item 1\nsub-item 2\n\nsub-sub-item 1\n\n\nTask 1\nTask 2\n\n\nterm\n\ndefinition\n\n\n\nBlockquote",
    "crumbs": [
      "Internal",
      "Quarto reference guide"
    ]
  },
  {
    "objectID": "internal/quarto.html#figures",
    "href": "internal/quarto.html#figures",
    "title": "Quarto reference guide",
    "section": "",
    "text": "Figure¬†1: An Elephant\n\n\n\nThis is illustrated well by Figure¬†1.",
    "crumbs": [
      "Internal",
      "Quarto reference guide"
    ]
  },
  {
    "objectID": "internal/quarto.html#tables",
    "href": "internal/quarto.html#tables",
    "title": "Quarto reference guide",
    "section": "",
    "text": "Table¬†1: My Caption\n\n\n\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\n\n\n\nSee Table¬†1.",
    "crumbs": [
      "Internal",
      "Quarto reference guide"
    ]
  },
  {
    "objectID": "internal/quarto.html#citations",
    "href": "internal/quarto.html#citations",
    "title": "Quarto reference guide",
    "section": "",
    "text": "Life is good (Weigert et al. 2018). (bibkey should be in references.bib)",
    "crumbs": [
      "Internal",
      "Quarto reference guide"
    ]
  },
  {
    "objectID": "internal/quarto.html#buttons",
    "href": "internal/quarto.html#buttons",
    "title": "Quarto reference guide",
    "section": "",
    "text": "Program Resources",
    "crumbs": [
      "Internal",
      "Quarto reference guide"
    ]
  },
  {
    "objectID": "internal/quarto.html#footnotes",
    "href": "internal/quarto.html#footnotes",
    "title": "Quarto reference guide",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere is the footnote.‚Ü©Ô∏é\nHere‚Äôs one with multiple blocks.‚Ü©Ô∏é\nInlines notes are easier to write, since you don‚Äôt have to pick an identifier and move down to type the note.‚Ü©Ô∏é",
    "crumbs": [
      "Internal",
      "Quarto reference guide"
    ]
  },
  {
    "objectID": "internal/quarto.html#code-blocks-and-math",
    "href": "internal/quarto.html#code-blocks-and-math",
    "title": "Quarto reference guide",
    "section": "",
    "text": "inline math: \\(E = mc^{2}\\)\ndisplay math:\n\\[E = mc^{2}\\]",
    "crumbs": [
      "Internal",
      "Quarto reference guide"
    ]
  },
  {
    "objectID": "internal/quarto.html#callout-blocks",
    "href": "internal/quarto.html#callout-blocks",
    "title": "Quarto reference guide",
    "section": "",
    "text": "Note\n\n\n\nNote callout block.\n\n\n\n\n\n\n\n\nTip\n\n\n\nTip callout block.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWarning callout block.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nCaution callout block.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nImportant callout block.",
    "crumbs": [
      "Internal",
      "Quarto reference guide"
    ]
  },
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Program",
    "section": "",
    "text": "Note\n\n\n\nThis is the current program. Edit this page! The original draft program is here.\n\n\n\n\nMonday, 5 Jan 2026\n\n\n\n09:00‚Äì09:15 - Welcome session and course overview\n09:15‚Äì10:00 - Concepts in light-sheet microscopy (Marina)\n10:00‚Äì11:00 - Experimental design and sample mounting (Marina)\n\n\n\n\n\n\n\n\n11:45‚Äì12:30 - Visualization and processing of digital images (Agust√≠n)\n12:30‚Äì13:00 - Best practices in microscopy data management (Bruno)\n\n\n\n\n\n\n\n\n14:00‚Äì14:30 - [Talk from companies] (Company 1)\n14:30‚Äì15:30 - Visualization of 2D images (Agust√≠n)\n15:30‚Äì17:30 - Visualization of 3D images (Bruno)\n\n\n\n\n\nTuesday, 6 Jan 2026\n\n\n\n09:00‚Äì10:00 - Image processing and analysis (Agust√≠n)\n10:00‚Äì11:00 - Multiview reconstruction and tissue cartography (Bruno)\n\n\n\n\n\n\n\n\n\n11:45‚Äì13:00 - Multiview reconstruction (Bruno)\n\n\n\n\n\n\n\n\n\n14:00‚Äì14:30 - [Talk from companies] (Company 2-Galenica)\n14:30‚Äì16:00 - Pre-processing, denoising, segmentation (Marina)\n16:00‚Äì17:30 - ImageJ macro programming (Marina)\n\n\n\n\n\nWednesday, 7 Jan 2026\n\n\n\n\n09:00‚Äì10:30 - Deep learning (Agust√≠n/Bruno)\n\n\n\n\n\n\n\n\n11:00‚Äì12:30 - 3D segmentation using machine/deep learning (Marina/Agust√≠n)\n12:30‚Äì13:00 - [Talk from companies] (Company 3-Bruker)\n\n\n\n\n\n\n\n\n\n14:00‚Äì16:00 - Tissue cartography (Bruno)\n16:00‚Äì17:30 - Cell tracking (Bruno)\n\n\n\n\n\nThursday, 8 Jan 2026\n\n\n\n09:00‚Äì09:30 - Talk Bruno (Bruno)\n09:30‚Äì10:00 - Talk Marina (Marina)\n10:00‚Äì10:30 - Talk Charlotte (Charlotte)\n10:30‚Äì11:00 - Talk Leo (Leo)\n\n\n\n\n\n\n\n\n11:30‚Äì13:00 - Project work (Groups)\n\n\n\n\n\n14:00‚Äì17:30 - Project work (Groups)\n\n\n\n\n\nFriday, 9 Jan 2026\n\n\n\n09:00‚Äì11:00 - Project work (Groups)\n\n\n\n\n\n\n\n\n11:30‚Äì13:00 - Project work (Groups)\n\n\n\n\n\n14:00‚Äì16:30 - Group presentations (Groups)\n16:30‚Äì17:00 - Closing remarks (Organizers)\n17:00‚Äìonwards - Group dinner (Everyone)",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "program.html#day-1-light-sheet-principles-and-digital-images",
    "href": "program.html#day-1-light-sheet-principles-and-digital-images",
    "title": "Program",
    "section": "",
    "text": "Monday, 5 Jan 2026\n\n\n\n09:00‚Äì09:15 - Welcome session and course overview\n09:15‚Äì10:00 - Concepts in light-sheet microscopy (Marina)\n10:00‚Äì11:00 - Experimental design and sample mounting (Marina)\n\n\n\n\n\n\n\n\n11:45‚Äì12:30 - Visualization and processing of digital images (Agust√≠n)\n12:30‚Äì13:00 - Best practices in microscopy data management (Bruno)\n\n\n\n\n\n\n\n\n14:00‚Äì14:30 - [Talk from companies] (Company 1)\n14:30‚Äì15:30 - Visualization of 2D images (Agust√≠n)\n15:30‚Äì17:30 - Visualization of 3D images (Bruno)",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "program.html#day-2-image-processing-and-multiview-reconstruction",
    "href": "program.html#day-2-image-processing-and-multiview-reconstruction",
    "title": "Program",
    "section": "",
    "text": "Tuesday, 6 Jan 2026\n\n\n\n09:00‚Äì10:00 - Image processing and analysis (Agust√≠n)\n10:00‚Äì11:00 - Multiview reconstruction and tissue cartography (Bruno)\n\n\n\n\n\n\n\n\n\n11:45‚Äì13:00 - Multiview reconstruction (Bruno)\n\n\n\n\n\n\n\n\n\n14:00‚Äì14:30 - [Talk from companies] (Company 2-Galenica)\n14:30‚Äì16:00 - Pre-processing, denoising, segmentation (Marina)\n16:00‚Äì17:30 - ImageJ macro programming (Marina)",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "program.html#day-3-machine-learning-and-advanced-workflows",
    "href": "program.html#day-3-machine-learning-and-advanced-workflows",
    "title": "Program",
    "section": "",
    "text": "Wednesday, 7 Jan 2026\n\n\n\n\n09:00‚Äì10:30 - Deep learning (Agust√≠n/Bruno)\n\n\n\n\n\n\n\n\n11:00‚Äì12:30 - 3D segmentation using machine/deep learning (Marina/Agust√≠n)\n12:30‚Äì13:00 - [Talk from companies] (Company 3-Bruker)\n\n\n\n\n\n\n\n\n\n14:00‚Äì16:00 - Tissue cartography (Bruno)\n16:00‚Äì17:30 - Cell tracking (Bruno)",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "program.html#day-4-scientific-applications-and-project-work",
    "href": "program.html#day-4-scientific-applications-and-project-work",
    "title": "Program",
    "section": "",
    "text": "Thursday, 8 Jan 2026\n\n\n\n09:00‚Äì09:30 - Talk Bruno (Bruno)\n09:30‚Äì10:00 - Talk Marina (Marina)\n10:00‚Äì10:30 - Talk Charlotte (Charlotte)\n10:30‚Äì11:00 - Talk Leo (Leo)\n\n\n\n\n\n\n\n\n11:30‚Äì13:00 - Project work (Groups)\n\n\n\n\n\n14:00‚Äì17:30 - Project work (Groups)",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "program.html#day-5-project-work-and-presentations",
    "href": "program.html#day-5-project-work-and-presentations",
    "title": "Program",
    "section": "",
    "text": "Friday, 9 Jan 2026\n\n\n\n09:00‚Äì11:00 - Project work (Groups)\n\n\n\n\n\n\n\n\n11:30‚Äì13:00 - Project work (Groups)\n\n\n\n\n\n14:00‚Äì16:30 - Group presentations (Groups)\n16:30‚Äì17:00 - Closing remarks (Organizers)\n17:00‚Äìonwards - Group dinner (Everyone)",
    "crumbs": [
      "Program"
    ]
  },
  {
    "objectID": "internal/draft_program.html",
    "href": "internal/draft_program.html",
    "title": "Original draft program",
    "section": "",
    "text": "Caution\n\n\n\nPlease don‚Äôt edit this page. The current program is here.\n\n\nInitial draft of the program written by Marina. Keeping it here as a reference.\n\n\n\nüß† Morning Session\n- Welcome & Course Overview\n- Principles of Light-Sheet Microscopy\n- Optical setup & acquisition strategies\n- Experiment planning and sample mounting\n- Common image artifacts and how to avoid/fix them\n- Introduction to light-sheet datasets: file types, metadata, scale, volume/time complexity\nüñ•Ô∏è Afternoon Workshop\n- Multiview reconstruction in Fiji\n- Hands-on with 3D visualization in Fiji and Napari\n\n\n\nüß† Morning Session\n- Fundamentals of BioImage Analysis\n- Image types and bit depth\n- Noise, background correction, filters\n- Thresholding, segmentation basics (pixel vs object)\n- ROI handling and masks\n- Overview of available open-source tools\nüñ•Ô∏è Afternoon Workshop\n- Segmentation workflows in Fiji and Napari (2D/3D)\n- Intro to cell tracking methods\n\n\n\nüß† Morning Session\n- Introduction to AI and deep learning in microscopy\n- When and why to use DL models\n- Overview of StarDist, CellPose\nüñ•Ô∏è Afternoon Workshop\n- Using BioImage Model Zoo models with Napari\n- Hands-on with Labkit and ilastik (interactive annotation & classification)\n- Deploying pre-trained models on user data\n- Optional: Train your own simple model (time permitting)\n\n\n\nüß† Morning Session\n- Research applications of BioImage Analysis\n- Talks or case studies using light-sheet imaging\n- Highlight multi-disciplinary projects (e.g.¬†developmental biology, neuroscience, plant science)\n- Tips for designing an effective analysis pipeline\nüñ•Ô∏è Afternoon Workshop\n- Begin working on personal projects or example datasets\n- Q&A with instructors (project clinic)\n- Begin structuring project presentations (suggested format below)\n\n\n\nüß† Morning Session\n- Continue work on projects\n- Finalize analysis and prepare presentations\n- Instructor support for polishing visuals and conclusions\nüñ•Ô∏è Afternoon Session\n- Student Presentations (10 min each + 5 min Q&A)\n- Suggested format:\n1. Dataset and biological question\n2. Analysis goal\n3. Method and tools used\n4. Results\n5. Challenges & next steps\n- Group Discussion and Feedback\n- Closing Remarks & Certificate Distribution\n- Group Dinner üéâ",
    "crumbs": [
      "Internal",
      "Original draft program"
    ]
  },
  {
    "objectID": "internal/draft_program.html#day-1-introduction-to-light-sheet-and-3d-visualization",
    "href": "internal/draft_program.html#day-1-introduction-to-light-sheet-and-3d-visualization",
    "title": "Original draft program",
    "section": "",
    "text": "üß† Morning Session\n- Welcome & Course Overview\n- Principles of Light-Sheet Microscopy\n- Optical setup & acquisition strategies\n- Experiment planning and sample mounting\n- Common image artifacts and how to avoid/fix them\n- Introduction to light-sheet datasets: file types, metadata, scale, volume/time complexity\nüñ•Ô∏è Afternoon Workshop\n- Multiview reconstruction in Fiji\n- Hands-on with 3D visualization in Fiji and Napari",
    "crumbs": [
      "Internal",
      "Original draft program"
    ]
  },
  {
    "objectID": "internal/draft_program.html#day-2-image-analysis-fundamentals",
    "href": "internal/draft_program.html#day-2-image-analysis-fundamentals",
    "title": "Original draft program",
    "section": "",
    "text": "üß† Morning Session\n- Fundamentals of BioImage Analysis\n- Image types and bit depth\n- Noise, background correction, filters\n- Thresholding, segmentation basics (pixel vs object)\n- ROI handling and masks\n- Overview of available open-source tools\nüñ•Ô∏è Afternoon Workshop\n- Segmentation workflows in Fiji and Napari (2D/3D)\n- Intro to cell tracking methods",
    "crumbs": [
      "Internal",
      "Original draft program"
    ]
  },
  {
    "objectID": "internal/draft_program.html#day-3-machine-learning-and-ai-for-image-analysis",
    "href": "internal/draft_program.html#day-3-machine-learning-and-ai-for-image-analysis",
    "title": "Original draft program",
    "section": "",
    "text": "üß† Morning Session\n- Introduction to AI and deep learning in microscopy\n- When and why to use DL models\n- Overview of StarDist, CellPose\nüñ•Ô∏è Afternoon Workshop\n- Using BioImage Model Zoo models with Napari\n- Hands-on with Labkit and ilastik (interactive annotation & classification)\n- Deploying pre-trained models on user data\n- Optional: Train your own simple model (time permitting)",
    "crumbs": [
      "Internal",
      "Original draft program"
    ]
  },
  {
    "objectID": "internal/draft_program.html#day-4-applications-independent-project-work",
    "href": "internal/draft_program.html#day-4-applications-independent-project-work",
    "title": "Original draft program",
    "section": "",
    "text": "üß† Morning Session\n- Research applications of BioImage Analysis\n- Talks or case studies using light-sheet imaging\n- Highlight multi-disciplinary projects (e.g.¬†developmental biology, neuroscience, plant science)\n- Tips for designing an effective analysis pipeline\nüñ•Ô∏è Afternoon Workshop\n- Begin working on personal projects or example datasets\n- Q&A with instructors (project clinic)\n- Begin structuring project presentations (suggested format below)",
    "crumbs": [
      "Internal",
      "Original draft program"
    ]
  },
  {
    "objectID": "internal/draft_program.html#day-5-final-projects-presentations",
    "href": "internal/draft_program.html#day-5-final-projects-presentations",
    "title": "Original draft program",
    "section": "",
    "text": "üß† Morning Session\n- Continue work on projects\n- Finalize analysis and prepare presentations\n- Instructor support for polishing visuals and conclusions\nüñ•Ô∏è Afternoon Session\n- Student Presentations (10 min each + 5 min Q&A)\n- Suggested format:\n1. Dataset and biological question\n2. Analysis goal\n3. Method and tools used\n4. Results\n5. Challenges & next steps\n- Group Discussion and Feedback\n- Closing Remarks & Certificate Distribution\n- Group Dinner üéâ",
    "crumbs": [
      "Internal",
      "Original draft program"
    ]
  },
  {
    "objectID": "internal/presentation.html#getting-up",
    "href": "internal/presentation.html#getting-up",
    "title": "Example presentation",
    "section": "Getting up",
    "text": "Getting up\n\nTurn off alarm\nGet out of bed",
    "crumbs": [
      "Internal",
      "Example presentation"
    ]
  },
  {
    "objectID": "internal/presentation.html#breakfast",
    "href": "internal/presentation.html#breakfast",
    "title": "Example presentation",
    "section": "Breakfast",
    "text": "Breakfast\n\nEat eggs\nDrink coffee",
    "crumbs": [
      "Internal",
      "Example presentation"
    ]
  },
  {
    "objectID": "internal/presentation.html#dinner",
    "href": "internal/presentation.html#dinner",
    "title": "Example presentation",
    "section": "Dinner",
    "text": "Dinner\n\nEat spaghetti\nDrink wine",
    "crumbs": [
      "Internal",
      "Example presentation"
    ]
  },
  {
    "objectID": "internal/presentation.html#going-to-sleep",
    "href": "internal/presentation.html#going-to-sleep",
    "title": "Example presentation",
    "section": "Going to sleep",
    "text": "Going to sleep\n\nGet in bed\nCount sheep",
    "crumbs": [
      "Internal",
      "Example presentation"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Compilation of tools and resources for image analysis focused on what we will use.\nTODO: Add more links for tools and resources here.\n\n\nDesktop software for image analysis.\n\nhttps://fiji.sc/\nhttps://napari.org/\n\n\n\n\nSpaces to interact and discuss about image analysis.\n\nhttps://forum.image.sc/\nhttps://imagesc.zulipchat.com/\n\n\n\n\nOther curated lists of tools and resources for scientific image analysis.\n\nhttps://github.com/epfl-center-for-imaging/awesome-scientific-image-analysis/\nhttps://github.com/hallvaaw/awesome-biological-image-analysis/",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#general",
    "href": "resources.html#general",
    "title": "Resources",
    "section": "",
    "text": "Desktop software for image analysis.\n\nhttps://fiji.sc/\nhttps://napari.org/",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#community",
    "href": "resources.html#community",
    "title": "Resources",
    "section": "",
    "text": "Spaces to interact and discuss about image analysis.\n\nhttps://forum.image.sc/\nhttps://imagesc.zulipchat.com/",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#compilations",
    "href": "resources.html#compilations",
    "title": "Resources",
    "section": "",
    "text": "Other curated lists of tools and resources for scientific image analysis.\n\nhttps://github.com/epfl-center-for-imaging/awesome-scientific-image-analysis/\nhttps://github.com/hallvaaw/awesome-biological-image-analysis/",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "practicals/practical_macros/index.html",
    "href": "practicals/practical_macros/index.html",
    "title": "ImageJ macros",
    "section": "",
    "text": "ImageJ macros\nMarina could do this with no prob. https://f1000research.com/slides/11-973 - Why? - Types of macros - How to record",
    "crumbs": [
      "Practicals",
      "ImageJ macros"
    ]
  },
  {
    "objectID": "practicals/practical_3d/index.html",
    "href": "practicals/practical_3d/index.html",
    "title": "Visualization of 3D images",
    "section": "",
    "text": "Visualization of 3D images\n\nMax projections\n\nOrthogonal views\n\nReslicing\n\nBigData viewer\n\n3D script",
    "crumbs": [
      "Practicals",
      "Visualization of 3D images"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html",
    "href": "practicals/practical_cartography/index.html",
    "title": "Tissue cartography using Blender",
    "section": "",
    "text": "Tissue cartography is the art of projecting a tridimensional surface, usually of a biological sample, into a 2D surface. This is useful to visualize and analyze complex 3D microscopy data.\nThe state-of-the-art approach to generate cartographic projections was the MATLAB-based ImSAnE toolbox (Heemskerk and Streichan 2015). However, more recently, a Blender-based tool was released known as Blender Tissue Cartography (Claussen et al. 2025). This tool has an extensive documentation and several in-depth tutorials that I highly recommend anyone interested in tissue cartography to read.\nThe tutorial below is a simplified version of the Blender Tissue Cartography tutorials, focusing on one of the approaches. It shows how to generate cartographic projections from 3D lightsheet microscopy data using Fiji to inspect data, ilastik to segment the tissues, and Blender with the Blender Tissue Cartography add-on to create the projections.\nIf you want to generate cartographic projections using the MATLAB-based ImSAnE toolbox (Heemskerk and Streichan 2015), there is another tutorial explaining how to set up and run the pipeline (Vellutini 2022).\nUV Editing - Go to UV - Remove cube - Orient sample vertically - Deselect bounding box - Change mode to face select (3) - Select all vertices/edges - UV &gt; Cylinder projection",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-summary",
    "href": "practicals/practical_cartography/index.html#sec-summary",
    "title": "Tissue cartography using Blender",
    "section": "",
    "text": "Tissue cartography is the art of projecting a tridimensional surface, usually of a biological sample, into a 2D surface. This is useful to visualize and analyze complex 3D microscopy data.\nThe state-of-the-art approach to generate cartographic projections was the MATLAB-based ImSAnE toolbox (Heemskerk and Streichan 2015). However, more recently, a Blender-based tool was released known as Blender Tissue Cartography (Claussen et al. 2025). This tool has an extensive documentation and several in-depth tutorials that I highly recommend anyone interested in tissue cartography to read.\nThe tutorial below is a simplified version of the Blender Tissue Cartography tutorials, focusing on one of the approaches. It shows how to generate cartographic projections from 3D lightsheet microscopy data using Fiji to inspect data, ilastik to segment the tissues, and Blender with the Blender Tissue Cartography add-on to create the projections.\nIf you want to generate cartographic projections using the MATLAB-based ImSAnE toolbox (Heemskerk and Streichan 2015), there is another tutorial explaining how to set up and run the pipeline (Vellutini 2022).\nUV Editing - Go to UV - Remove cube - Orient sample vertically - Deselect bounding box - Change mode to face select (3) - Select all vertices/edges - UV &gt; Cylinder projection",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-requirements",
    "href": "practicals/practical_cartography/index.html#sec-requirements",
    "title": "Tissue cartography using Blender",
    "section": "Requirements",
    "text": "Requirements\n\nFiji/ImageJ\nilastik 1.4.1.post1\nBlender 4.2.9\nBlender Tissue Cartography (Blender add-on)\nDrosophila_CAAX-mCherry.tif dataset from Blender Tissue Cartography (available here)",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-setup",
    "href": "practicals/practical_cartography/index.html#sec-setup",
    "title": "Tissue cartography using Blender",
    "section": "Setup",
    "text": "Setup\nNote! If you are following this during the course, the dataset has already been downloaded.\n\nDownload Blender Tissue Cartography\n\nGo to https://github.com/nikolas-claussen/blender-tissue-cartography\nPress the green button named Code &gt; Download ZIP to begin the download (or press here)\nUnzip the contents in your working directory\nYou should see a new directory named blender-tissue-cartography\nCopy the file Drosophila_CAAX-mCherry.tif located at blender-tissue-cartography/nbs/Tutorials/drosophila_example/ to your working directory\n\n\n\nDownload Blender\n\nGot to https://download.blender.org/release/Blender4.2/\nDownload Blender 4.2.9 (direct link for linux)\nUnzip the file into your working directory\nYou should see a new directory named blender-4.2.9-linux-x64\n\n\n\nInstall Blender Tissue Cartography\n\nOpen the directory blender-4.2.9-linux-x64\nDouble-click the file blender to open the program\nGo to Edit &gt; Preferences &gt; Add-ons &gt; Add-ons Settings (down arrow) &gt; Install from Disk...\nSelect the file blender_tissue_cartography-1.0.0-linux_x64.zip located in the directory blender-tissue-cartography/blender_addon/\nClose Blender\n\n\n\nDownload ilastik\n\nGo to https://www.ilastik.org/download\nDownload ilastik 1.4.1.post1 (direct link for linux)\nUnzip the file into your working directory\nYou should see a new directory named ilastik-1.4.1.post1-Linux\n\n\n\nDownload Fiji\n\nGo to https://fiji.sc\nChoose Distribution: Stable then click the big download button\nUnzip the file into your working directory\nYou should see a new directory named fiji-stable-linux64-jdk",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-inspect-dataset",
    "href": "practicals/practical_cartography/index.html#sec-inspect-dataset",
    "title": "Tissue cartography using Blender",
    "section": "Inspect dataset in Fiji",
    "text": "Inspect dataset in Fiji\n\nBefore starting, let‚Äôs inspect the Drosophila_CAAX-mCherry.tif dataset in Fiji\nOpening the directory fiji-stable-linux64-jdk/Fiji.app/ and double-click the fiji-linux-x64 launcher\n\n\n\nDrag and drop Drosophila_CAAX-mCherry.tif in the Fiji window to open it\nScroll through the Z slices of the stack\n\n\n\nTo get more information, activate the orthogonal views with Image &gt; Stacks &gt; Orthogonal Views (or ctrl+shift+h\n\n\n\nExplore the sample to understand well its shape\nTry to understand which side is dorsal, which is ventral, and what is left/right\nAlso notice what are the characteristics of the tissue and background\nThink about the potential issues we might encounter with this dataset\n\n\n\nOnce done, close the orthogonal views and stack (leave Fiji open)",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-segment-tissues",
    "href": "practicals/practical_cartography/index.html#sec-segment-tissues",
    "title": "Tissue cartography using Blender",
    "section": "Segment tissues in ilastik",
    "text": "Segment tissues in ilastik\n\nThe first step we need is to segment the stack in 3D to distinguish what is tissue and what is background\nThis is required to create a 3D mesh in Blender that has the shape of the sample\nTo accomplish that, we will use ilastik\nOpen the directory ilastik-1.4.1.post1-Linux, right-click the file run_ilastik.sh, and select Run as a Program to open ilastik\n\n\n\nCreate project\n\nMaximize the interface (we will need it)\nUnder Create New Project, click on Pixel Classification\n\n\n\nThe window Create Ilastik Project will open\nNavigate to your working directory and click save\nA file named MyProject.ilp will be created\n\n\n\n\nInput Data\n\nThe ilastik interface is ready to define our input data\nUnder the Raw Data tab, click on Add New... &gt; Add separate Image(s)...\n\n\n\nThen select the file Drosophila_CAAX-mCherry.tif\n\n\n\nilastik will open the dataset in three orthogonal views\nXY (blue), XZ (green), YZ (red)\n\n\n\nNote, however, that the images are too dark; let‚Äôs fix this\nRight-click the dataset row and select Edit properties...\nChange the value of Normalize Display to True and set the Range maximum value to 10000\nPress OK\n\n\n\n\n\n\n\n\n\n\n\n\nThe contrast will be much better now\n\n\n\n\nFeature Selection\n\nNext we need to select the image features to take into account for the segmentation\nOn the left column press 2. Feature Selection; the interface will update\nThen press Select Features... to open the Features window\nSelect all the features and press OK\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining\n\nOn the left column, click 3. Training\nA new toolbox will open underneath showing two labels, Label 1 (yellow) and Label 2 (blue), buttons for paint or eraser modes, a size drop-down menu, and a Live Update button\n\n\n\nNow is a good time to get familiar with the basic ilastik commands\nscroll+forward: go down through the slices of the orthogonal dimension\nscroll+backward: go up through the slices of the orthogonal dimension\nctrl+scroll+forward: zoom in\nctrl+scroll+backward: zoom out\nmiddle-click+hold: drag view around\nLearn how to zoom in/out, go through the slices, and drag the view\nIf you can zoom in significantly, and reach the top or bottom of a view by dragging, you are ready for painting\n\n\n\nOur goal is to paint tissues in yellow and background in blue\nBut to accomplish that, we only need a few strokes at the right regions of the image\nBegin by zooming in at the top region of the XZ (green) view\nSelect Label 2 (blue), change the size to 7, and paint a line right above the tissue\nNow select Label 1 (yellow) and paint the tissue immediately below the blue line\nThese two simple lines are indicating to ilastik that all the image features in this region very close to the tissue corresponds to ‚Äúbackground‚Äù, and that the image features of the tissue below correspond to ‚Äúforeground‚Äù. Putting the two linnes adjacent to each other also help ilastik to understand where the boundaries are\n\n\n\nSince we want that all of the tissue is segmented (and not just the surface), paint a line until the center of the embryo\nThis is enough to get started. Based on these simple strokes, ilastik will learn and apply this to the entire dataset\nTo start the training, press the button Live Update\nilastik will overlay the current segmentation model over the image\nYou should see that most of the tissue regions are yellow and that the region around the embryo are more blue\nThe more vivid the color, the more confident the model is about that specific region\n\n\n\nNow start painting with simple strokes areas which are wrong or pale\nFor example the corners of the images are background and should be blue; any area inside the sample, should be yellow; use different brush sizes if needed; or the eraser\nThis sample also has giant, super bright beads; they are not tissue, we want them blue\nNote that the segmentation model and overlay colors update upon each stroke, so you can see if what you did improved or worsen the segmentation. If it got worse, you can always erase the annotation\n\n\n\nBe meticulous and pay special attention to the edges of the image, we do not want tissue (yellow) to be touching the border, because this will create a hole in the segmentation\nThe better the segmentation is, the better will be our visualization and cartographic projection\nThat said, there are many ways that you can fix issues and improve the segmentation after converting it to a mesh\nIn this image, it is important to take care of the tip of the very top part of the sample because it is touching the edge\nUse a size 1 brush to place a couple of blue lines there\n\n\n\nGo through the slices in each of the orthogonal views to fix any left over segmentation uncertainties\nThe segmentation overlay should be showing a clearly separated yellow and blue regions that match the embryo and background\n\n\n\n\nPrediction Export\n\nWe can now export the segmentation prediction\nOn the left column, click on 4. Prediction Export\nUnder Export Settings keep the Source value as Probabilities\n\n\n\nThen press Choose Export Image Settings... to open the Image Export Options window\nThere are two options that we need to change\nUnder Cutout Subregion uncheck the row c (for channels) and change the stop value to 1\nSince this image has only one channel, changing this option avoids loading a duplicated channel into Blender\nUnder Output File Info change the value of Format to multipage tiff\nThis is required to be able to load the segmentation into Blender\nPress OK close the window\nThen press the Export All button and wait‚Ä¶\n\n\n\nWhen the prediction is done, you will find a new file in your working directory named Drosophila_CAAX-mCherry_Probabilities.tiff\n\n\n\nTry opening this file in Fiji to see how it looks before our next step in the tutorial\n\n\n\nNow close the file and let‚Äôs start with the actual projection",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-import-blender",
    "href": "practicals/practical_cartography/index.html#sec-import-blender",
    "title": "Tissue cartography using Blender",
    "section": "Import data to Blender",
    "text": "Import data to Blender\n\nWe can now import the image stack and segmentation probabilities into Blender\n\n\nOpen Blender\n\nEnter the directory blender-4.2.9-linux-x64 and double-click the file blender\n\n\n\nBlender will open with a nice splash screen at the center, click anywhere to close it\n\n\n\nNotice at the top left region that we are in the Layout tab (important for later)\nYou‚Äôll see a gray cube at the center, we want to get rid of it\nIn the top right panel under Scene Collection &gt; Collection, right-click the Cube line and select Delete\n\n\n\nGreat. Let‚Äôs focus now at the bottom right corner, it is busy, full of icons and menus. Don‚Äôt get overwhelmed, we only need to select and use one of the modes\nIf not yet selected, click on the Scene icon (it is the white triangle with two circles) to activate this panel\nThen locate the tab named Tissue Cartography at the bottom\n\n\n\nScroll down and make the side panels wider to be able to read the options of Tissue Cartography\nThis is the main interface of the Blender Tissue Cartography add-on\nIt is through here that we will control most of the steps of this pipeline\n\n\n\n\nLoad sample\n\nThe first thing we need to do is to load the sample\nClick on the folder icon in the Tissue Cartography &gt; File Path, navigate to your working directory, and select the file Drosophila_CAAX-mCherry.tiff\nTip: bookmark the directory for easy access in the future\nClick Accept\n\n\n\n\n\n\n\n\n\n\n\n\nThen press Load .tiff file to load it into Blender\nA new row will appear at the top right panel under Scene Collection &gt; Collection named Drosophila_CAAX-mCherry_BoundingBox and the bounding box of the image stack will be visible in the main window as orange lines\n\n\n\nOnly a portion of the bounding box is visible, but we want to see the whole thing\nThe controls to navigate the 3D space are at the top right corner of the main window\nYou have XYZ handles (red, green, blue), a zoom tool (magnifier lens), and a move tool (open hand)\nClick, hold, and drag any of these to move around\nClicking on the X, Y, or Z will reorient your sample along these axes (very useful)\nTake some time to practice and finish by placing the bounding box at the center of the main window as in the image below\n\n\n\n\nLoad probabilities\n\nNow let‚Äôs load the probabilities files\nClick in the folder icon of the Tissue Cartography &gt; Segmentation File Path, navigate to your working directory, and select the file Drosophila_CAAX-mCherry_Probabilities.tiff\nClick Accept\n\n\n\nThe file name will appear in the field, but before loading we want to adjust one parameter\nBlender will take the segmentation probabilities and convert it into a tridimensional mesh\nGenerally, the raw segmentation is full of sharp angles, which will not look very nice when we map the image information onto the mesh for visualization\nTherefore, it is generally a good idea to apply a degree of smoothing upon importing the segmentation\nYou can control the smoothing in the small field below and to the right of Segmentation File Path named S... 0.00. It should read Smoothing (¬µm) but the panel is too narrow to show the full name\nClick on it and set it to 1.0\n\n\n\n\n\n\n\n\n\n\n\n\nNow click on Get mesh(es) from binary segmentation .tiff file(s) to generate the mesh\nIt takes a second\nA gray mesh shaped like our sample will appear inside the bounding box in the main window\nAlso notice that a new row appeared at the top right panel under Scene Collection &gt; Collection named Drosophila_CAAX-mCherry_Probabilities_c0\n\n\n\nCongratulations! You have successfully generated a 3D mesh of your sample. That‚Äôs already a powerful visualization method\nCelebrate by exploring your sample. Rotate all around, zoom in to see details, and check how good the mesh is. Are there any holes or other artifacts?\n\n\n\nApply shading\n\nThe mesh is nice, but it would even better to see the actual image data overlaid on the mesh\nWe can accomplish that using the shading view and function of Blender\nFirst we need to activate the shading viewport at the top right corner of the main window, an icon that looks like a pie chart\n\n\n\nOnce clicked, the mesh will become almost white\n\n\n\nBefore applying the shading, there‚Äôs one important parameter to set: Vertex ... 0.00 or, in full, Vertex Shader Normal Offset (¬µm)\n\n\n\nWhen this parameter is 0, the image data that corresponds to the limits of the segmentation is applied onto the mesh. In this case, this is the surface of the sample which, in this case, does not have much information. The fluorescent signal of the tissue, in this case, is a few microns deeper. Therefore, we can use the offset parameter to adjust the exact layer to be applied to the mesh as shading\nIn this case, a value of 5 works well\n\n\n\nNow, every time we want to apply or refresh the shading, we need to select the bounding box and probabilities entries in the Scene Collection &gt; Collection top right panel\nYou can do so by clicking on one and ctrl+clicking on the other\n\n\n\nFinally, to apply the shading press Initialize/refresh vertex shading and wait\nAfter a few seconds, you should see cell membranes overlaid onto the mesh\n\n\n\nTake the chance to explore the sample again, now with some biological information projected into 3D!",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-generate-projection",
    "href": "practicals/practical_cartography/index.html#sec-generate-projection",
    "title": "Tissue cartography using Blender",
    "section": "Generate projection",
    "text": "Generate projection\n\nWe are ready to generate our first cartographic projection\nThis is done in the UV Editing workspace, click on the tab to activate it\n\n\n\nSet up UV Editing\n\nBefore starting, we need to make sure that the correct options are enabled\nFirst, zoom out the right side window for the entire sample to be visible\n\n\n\nThen, we want to disable the bounding box entry in the top right panel under Scene Collection &gt; Collection\nYou can do so by ctrl+clicking on the square nodes symbol on the left of the bounding box entry as shown below\n\n\n\n\n\n\n\n\n\n\n\n\nNext we want to change the Select Mode from Vertex to Face\n\n\n\n\n\n\n\n\n\n\n\n\nIn the right hand workspace, we should see the sample (not the bounding box) highlighted in light orange\n\n\n\nIf you accidentaly clicked somewhere and now the mesh is visible in black, worry not! Simply press A or click on Select &gt; All\n\n\n\nTo finish the setup, click on the Y axis handles until the sample is oriented vertically with the pointy side upward\n\n\n\nWe are now ready to project the mesh\n\n\n\nProject mesh\n\nWhile the right side shows your sample, the left side shows how the projected mesh looks like; it is initially empty\nTo make the first projection, go to UV &gt; Cylinder Projection to project the mesh over the curved wall of a cylinder\n\n\n\nA crazy, palisade wall will appear\nDon‚Äôt despair and click on the tiny menu named Cylinder Projection that appeared at the bottom of the workspace\n\n\n\nThe options for the cylinder projection will appear\nThere you can define the orientation of the axes and other options to change how the 3D mesh is transformed into a 2D surface\nWhat we need for now is to activate the checkbox Scale to Bounds\nThis will nicely contain the projection into the squared bounding area\n\n\n\nAnd‚Ä¶ that‚Äôs it. We have our first projected mesh\nHow good is it? Ideally, the mesh should occupy the entire area\nOur projected mesh has a couple of portions slightly bulging outside the area, and we have an empty vertical portion on the right side\nThis could be fixed with some editing\nHowever, for now, it looks good enough for a first try\n\n\n\nProject data\n\nWe need to project the actual image data onto this surface\nChange back to the Layout workspace, select both Drosophila_CAAX-mCherry_BoundingBox and Drosophila_CAAX-mCherry_Probabilities_c0 under Scene Collection &gt; Collection, and change the option Normal Offs... (Normal Offsets (¬µm)) to 5 to match the Vertex Shader Normal Offset (¬µm) options\nNote: that Normal Offsets (¬µm) accepts a comma-separated lists of values. We can put 0,1,2,3,4,5 to generate a projection with 6 slices representing onion layers deeper into the tissues\nThe click on Create Projection\n\n\n\nWait‚Ä¶ the interface might become unresponsive. If a dialog appears, click on Wait\nWhen done, the shading over the sample will blink. It should look very similar to how it was before (if it doesn‚Äôt, it is a sign that something went wrong)\n\n\n\nCheck projection\n\nTo visually inspect the projected data we need to change back to the UV Editing workspace\n\n\n\nZoom out on the left side window to see the entire projection\nThen click anywhere outside the sample or projection to unselect the mesh\nThe sample mesh will become visible in black, and the cartographic projection should appear on the left side window\nThe orientation of the sample will match that of the sample (if the sample is upside down when you project the mesh, the projected mesh will also be upside down)\n\n\n\nSo what happened here? We projected the sample mesh to 2D using the cylinder approach. Then the add-on Blender Tissue Cartography used this projected mesh to create a projection of the image data from the original stack. Quite nice!\nEvery time you create a new projection, the projected data is stored as an image which is available for Blender to display as an image data-block\nYou can see them by clicking on the picture icon in the top menu\nThis first projection is named Channel_0_Layer_0. The next will be named Channel_0_Layer_0.001, Channel_0_Layer_0.002, and so on\n\n\n\n\nSave projection\n\nThe projection now exists in Blender, but we need to export it to file\nFor that, go back to the Layout workspace, select both Drosophila_CAAX-mCherry_BoundingBox and Drosophila_CAAX-mCherry_Probabilities_c0 under Scene Collection &gt; Collection again, then click on Save Projection\n\n\n\nA Blender File View window will open\nNavigate to your working directory\nIn the file name field put the name of the file with _cylinder appended to it, to read Drosophila_CAAX-mCherry_cylinder.tif\nThen press Save Projection\nThree new files will appear on your working directory with BakedData, BakedNormals, and BakedPositions suffixes appended to the dataset filename\n\n\n\nBakedData shows the original image data projected on the surface\nBakedPositions shows the original XYZ positions projected on the surface in RGB\nBakedNormals shows the XYZ surface directions perpendicular to each point in RGB\n\n\n\nOpen projection\n\nLet‚Äôs open the files in Fiji for inspection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplore them in more detail, check with Image &gt; Color &gt; Channels Tool... (ctrl+shift+z) how the individual channels look like\nThese files provide important information to reconstruct back the 3D information from the projected surface in downstream analyses",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-optimize-projection",
    "href": "practicals/practical_cartography/index.html#sec-optimize-projection",
    "title": "Tissue cartography using Blender",
    "section": "Optimize projection",
    "text": "Optimize projection\n\nOur initial projection is satisfactory, but there are many ways to optimize it to your specific needs\nOne immediate thing is to try a Sphere Projection instead of Cylinder Projection. They are quite similar, but I found the sphere projection to be more consistent and predictable and might work better for more spherical samples\nBut another common use case is to be able to determine where the mesh will unwrap. This might be important for downstream analyses\nIn our sample, for example, the projection put the dorsal side on the left side (where a clump of germ cells are visible at the bottom) and the ventral side on the right side. However, let‚Äôs say that I need for my analyses the dorsal side at the center of the projection\nTo accomplish that we can manually mark a seam on our mesh to define the unwrapping position\n\n\nMark seam on mesh\n\nGo to the UV Editing workspace and change the Select Mode to Edge select\n\n\n\nIn this mode, when you click on the mesh an edge is selected and when you subsequently ctrl+click on another edge, the shortest path between the two edges will be selected. Like this you can quickly select a line along your sample to mark the unwrapping position\nWhat we want is to trace a line through the ventral side of the sample. This is the region opposite to the germ cell clump\nUsing the handle buttons, reorient the sample sideways with the ventral side facing you\n\n\n\nWe need to start by selecting an edge at one of the poles\nTurn the sample to show one of the poles and zoom in to see the edges clearly\nThen click on one at the center of the pole to select it\n\n\n\nZoom out slightly, turn the sample, and ctrl+click on another edge further from the pole\nA yellow line will appear connecting the pole with the current edge\n\n\n\nContinue ctrl+clicking edges along the sample until the opposite pole\nDon‚Äôt worry if the line is not perfectly straight\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow press ctrl+e to open a menu with edge options and select Mark Seam\n\n\n\nThe seam will be marked in red (below it‚Äôs mixed with yellow line from the edge selection)\n\n\n\n\nProject mesh with seam\n\nNow reorient the sample vertically again with the narrow tip up and change the Select Mode back to Face select\n\n\n\nPress A to select all the mesh (they will turn orange)\nGo to UV &gt; Cylinder Projection\nThen check Preserve Seams in the option box\n\n\n\nAs you can see, the projection changed\nIt is squeezed at the center of the projection area due to the very long protruding mesh at the top left and bottom right regions\nLet‚Äôs evaluate how good it is by projecting the data\n\n\n\nProject data with seam\n\nGo to the Layout workspace, select both the bounding box and probabilities under Scene Collection, and press Create Projection; then, wait‚Ä¶\n\n\n\nWhen done, go back to the UV Editing workspace and click anywhere outside the sample to unselect the mesh\nThe new data projection should be visible on the left side\nIf not, click on the image data-block picture icon and select Channel_0_Layer_0.001\n\n\n\nWe have successfully changed the position of the unwrapping using the seam\nThe clump of germ cells is now at the center of the projection\nThis projection is OK, but the contents are squeezed and it is not occupying the full area\nWe can improve this by editing the projection mesh\n\n\n\nEdit projected mesh\n\nPress A to select the entire mesh\nSelect the tool Transform on the left side menu\n\n\n\nHover the mouse on the right side edge to reveal the scale handle\n\n\n\nDrag it to the right to extend the mesh until the edge of the projection area\nThen drag the other size and make the final scaling adjustments so that the projected mesh is covering most of the projection area\n\n\n\n\n\n\n\n\n\n\n\n\nNow you can use the Pinch or Grab tools to edit the mesh at the corners, so that they are not clipped out (you can also leave them as is, they will not appear in the projected data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck edited projection\n\nTo check the new projection with the edited mesh, we need to re-create the data projection\nGo to the Layout workspace, select both the bounding box and probabilities under Scene Collection, and press Create Projection; then, wait‚Ä¶\nWhen done, switch to the UV Editing workspace and deselect the mesh and check the new projection (select the latest image data-block, likely Channel_0_Layer_0.002)\nDespite the unevenness of the corners (next time we can do better), this edited projection is better than the first one\n\n\n\nLet‚Äôs save the projection to disk by going to the Layout workspace, selecting both the bounding box and probabilities entries under Scene Collection &gt; Collection, and clicking on Save Projection\nThen navigate to your working directory and give the file a different suffix\n\n\n\nFinally, open the newly generated files in Fiji to investigate",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_cartography/index.html#sec-references",
    "href": "practicals/practical_cartography/index.html#sec-references",
    "title": "Tissue cartography using Blender",
    "section": "References",
    "text": "References\n\n\nClaussen, Nikolas, C√©cile Regis, Susan Wopat, and Sebastian Streichan. 2025. ‚ÄúBlender Tissue Cartography: An Intuitive Tool for the Analysis of Dynamic 3D Microscopy Data.‚Äù bioRxiv, July, 2025.02.04.636523. https://doi.org/10.1101/2025.02.04.636523.\n\n\nHeemskerk, Idse, and Sebastian J Streichan. 2015. ‚ÄúTissue Cartography: Compressing Bio-Image Data by Dimensional Reduction.‚Äù Nat. Methods 12 (December): 1139‚Äì42. https://doi.org/10.1038/nmeth.3648.\n\n\nVellutini, Bruno C. 2022. How to Make Cartographic Projections Using ImSAnE. Zenodo. https://doi.org/10.5281/zenodo.7628299.",
    "crumbs": [
      "Practicals",
      "Tissue cartography using Blender"
    ]
  },
  {
    "objectID": "practicals/practical_deep_learning/object-classification/index.html",
    "href": "practicals/practical_deep_learning/object-classification/index.html",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "In this exercise we will:\n\nTrain an object classifier with Napari\nFilter labels\nExtract morphological and intensity based parameters\nPlot parameters in 2D\nApply a dimensionality reduction method\nVisualize the results\nCluster subgroups of objects using K-means\n\nThese steps form a complete workflow: raw image ‚Üí segmentation ‚Üí cleanup ‚Üí labeling ‚Üí measurement ‚Üí data viz\nWe‚Äôll use Lund.tif as the example image https://zenodo.org/records/17986091\nRequirements: - Everything you need is in the toml file in the Pixi/napari-devbio folder https://github.com/cuenca-mb/pixi-napari-devbio\n\n\nIn the terminal, go to the directory Pixi/napari-devbio and run:\npixi run napari\n\n\n\nDrag and drop the file or\nFile ‚Üí Open File\n\n\n\nLoad image and generate labels layer\n\n\nWe will be able to see and explore the stack, and we can change to a 3D rendering with the option Toogle 2D/3D view in the lower left button pannel. We will create a labels layer to anotate some ground truth for our object classifier.\nIn the left pannel, select the brush tool and make some background anotations with the brush. Then change to label 2 and make some object annotations (cells).\n\n\n\nDraw the labels\n\n\n\n\n\nGo to\nTools ‚Üí Segmentation / labeling ‚Üí Object segmentation (APOC)\n\n\n\nBrightness/Contrast histogram\n\n\nSelect the raw data for training and the labels layer for ground truth. Then click Train. Observe the results in 3D. For better visualization you can hide the labels layer.\n\n\n\nTrain\n\n\n\n\n\nObserve labels result\n\n\nFrom this first round of training, very likely the result is not perfect. This is because our ground truth anotation was done on a single z slice, so the classifier will perform well only around that depth. We can do several rounds of training.\nSelect again the labels image, and in the 2D mode go to a different z. Perform new anotations, preserving the label orders for background and object. Then just click Train again and observe the new results. Repeat as needed. Note that a classifier file will be produced and overwritten after each training. This can be used for batch segmentation of data from the same experiment/microscope.\n\n\n\nModify labels layer\n\n\n\n\n\nRe-train\n\n\n\n\n\nNow there might be some small objects that we want to filter. Go to\nTools ‚Üí Segmentation post-processing ‚Üí Exclude small labels\nThen select the layer Result of ObjectSegmentation and impose a minimum size of 20 (it will say maximum size in napari, but it is a mistake from the developers). Run it and observe the final labels (hide the old labels for better visualization.\n\n\n\nExclude small objects\n\n\n\n\n\nObserve filtered labels\n\n\nNow that we have a nice set of labels in 3D, we will quantify intensity and morphology based parameters for further analysis.\nGo to\nTools ‚Üí Measurement tables ‚Üí Regionprops\nSelect all parameters except moments and click run.\n\n\n\nRegionprops\n\n\n\n\n\nTable\n\n\nWe can export this table or visualize some of the data using the cluster plotter.\n\n\n\nGo to\nPlugins ‚Üí napari-clusters-plotter ‚Üí Plotter widget\nIn here you can select any measurement (column) from the region props table and make scatter plots.\n\n\n\nPlotter widget\n\n\n\n\n\nScatter plot\n\n\nA very useful tool is that we can draw in the plot closed curves and the objects falling inside will be automatically colorcoded and visualized in the 3D rendered image. I recommend hiding all the other layers except the raw image.\nBy pressing shift, we can draw further subgroups.\n\n\n\nLabel IDs\n\n\n\nThis manual selection generates a new column in the regionprops table with the identifier for us to export and further analyse in other software.\n\n\n\nGo to\nPlugins ‚Üí napari-clusters-plotter ‚Üí Dimensionality Reduction Widget\nThis is a very useful tool in which we can apply a variety of dimensionality reduction methods to our list of parameters. In this example I used UMAPS. We can select how many components we want and after hitting run they will be added to our regionprops table.\n\n\n\nDimensionality Reduction\n\n\n\n\n\nUMAP\n\n\nAfter doing this, we can come back to the Plotter Widget and plot this two components of the UMAP and generate more complex clustering of the data manually.\n\n\n\nUMAP viz\n\n\n\n\n\nManual Cluster ID\n\n\n\n\n\nBut what if we don‚Äôt want to be biased when clustering the data in subgroups? We can use unsupervised methods includded in this plugin. Go to\nPlugins ‚Üí napari-clusters-plotter ‚Üí Clustering Widget\nSelect the K-MEANS method, impose 3 clusters and run. This will generate a new column with the cluster ID for each object.\n\n\n\nClustering methods\n\n\n\n\n\nK-MEANS\n\n\nNow going back to the Plotter Widget, we can select the UMAP axis AND the K-MEAN cluster IDs to automatically colorcode the objects belonging to different classes.\n\n\n\nAutomatic clustering",
    "crumbs": [
      "Practicals",
      "Practical Deep Learning",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_deep_learning/object-classification/index.html#open-the-image-and-inspect-the-histogram",
    "href": "practicals/practical_deep_learning/object-classification/index.html#open-the-image-and-inspect-the-histogram",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "In the terminal, go to the directory Pixi/napari-devbio and run:\npixi run napari",
    "crumbs": [
      "Practicals",
      "Practical Deep Learning",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_deep_learning/object-classification/index.html#open-the-image-and-create-manual-anotations",
    "href": "practicals/practical_deep_learning/object-classification/index.html#open-the-image-and-create-manual-anotations",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "Drag and drop the file or\nFile ‚Üí Open File\n\n\n\nLoad image and generate labels layer\n\n\nWe will be able to see and explore the stack, and we can change to a 3D rendering with the option Toogle 2D/3D view in the lower left button pannel. We will create a labels layer to anotate some ground truth for our object classifier.\nIn the left pannel, select the brush tool and make some background anotations with the brush. Then change to label 2 and make some object annotations (cells).\n\n\n\nDraw the labels",
    "crumbs": [
      "Practicals",
      "Practical Deep Learning",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_deep_learning/object-classification/index.html#train-the-object-classifier",
    "href": "practicals/practical_deep_learning/object-classification/index.html#train-the-object-classifier",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "Go to\nTools ‚Üí Segmentation / labeling ‚Üí Object segmentation (APOC)\n\n\n\nBrightness/Contrast histogram\n\n\nSelect the raw data for training and the labels layer for ground truth. Then click Train. Observe the results in 3D. For better visualization you can hide the labels layer.\n\n\n\nTrain\n\n\n\n\n\nObserve labels result\n\n\nFrom this first round of training, very likely the result is not perfect. This is because our ground truth anotation was done on a single z slice, so the classifier will perform well only around that depth. We can do several rounds of training.\nSelect again the labels image, and in the 2D mode go to a different z. Perform new anotations, preserving the label orders for background and object. Then just click Train again and observe the new results. Repeat as needed. Note that a classifier file will be produced and overwritten after each training. This can be used for batch segmentation of data from the same experiment/microscope.\n\n\n\nModify labels layer\n\n\n\n\n\nRe-train",
    "crumbs": [
      "Practicals",
      "Practical Deep Learning",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_deep_learning/object-classification/index.html#filter-small-objects-and-quantify-parameters",
    "href": "practicals/practical_deep_learning/object-classification/index.html#filter-small-objects-and-quantify-parameters",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "Now there might be some small objects that we want to filter. Go to\nTools ‚Üí Segmentation post-processing ‚Üí Exclude small labels\nThen select the layer Result of ObjectSegmentation and impose a minimum size of 20 (it will say maximum size in napari, but it is a mistake from the developers). Run it and observe the final labels (hide the old labels for better visualization.\n\n\n\nExclude small objects\n\n\n\n\n\nObserve filtered labels\n\n\nNow that we have a nice set of labels in 3D, we will quantify intensity and morphology based parameters for further analysis.\nGo to\nTools ‚Üí Measurement tables ‚Üí Regionprops\nSelect all parameters except moments and click run.\n\n\n\nRegionprops\n\n\n\n\n\nTable\n\n\nWe can export this table or visualize some of the data using the cluster plotter.",
    "crumbs": [
      "Practicals",
      "Practical Deep Learning",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_deep_learning/object-classification/index.html#data-visualization-with-clusters-plotter",
    "href": "practicals/practical_deep_learning/object-classification/index.html#data-visualization-with-clusters-plotter",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "Go to\nPlugins ‚Üí napari-clusters-plotter ‚Üí Plotter widget\nIn here you can select any measurement (column) from the region props table and make scatter plots.\n\n\n\nPlotter widget\n\n\n\n\n\nScatter plot\n\n\nA very useful tool is that we can draw in the plot closed curves and the objects falling inside will be automatically colorcoded and visualized in the 3D rendered image. I recommend hiding all the other layers except the raw image.\nBy pressing shift, we can draw further subgroups.\n\n\n\nLabel IDs\n\n\n\nThis manual selection generates a new column in the regionprops table with the identifier for us to export and further analyse in other software.",
    "crumbs": [
      "Practicals",
      "Practical Deep Learning",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_deep_learning/object-classification/index.html#dimensionality-reduction",
    "href": "practicals/practical_deep_learning/object-classification/index.html#dimensionality-reduction",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "Go to\nPlugins ‚Üí napari-clusters-plotter ‚Üí Dimensionality Reduction Widget\nThis is a very useful tool in which we can apply a variety of dimensionality reduction methods to our list of parameters. In this example I used UMAPS. We can select how many components we want and after hitting run they will be added to our regionprops table.\n\n\n\nDimensionality Reduction\n\n\n\n\n\nUMAP\n\n\nAfter doing this, we can come back to the Plotter Widget and plot this two components of the UMAP and generate more complex clustering of the data manually.\n\n\n\nUMAP viz\n\n\n\n\n\nManual Cluster ID",
    "crumbs": [
      "Practicals",
      "Practical Deep Learning",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "practicals/practical_deep_learning/object-classification/index.html#unsupervised-clustering",
    "href": "practicals/practical_deep_learning/object-classification/index.html#unsupervised-clustering",
    "title": "Object-classification, dimensionality reduction and clustering",
    "section": "",
    "text": "But what if we don‚Äôt want to be biased when clustering the data in subgroups? We can use unsupervised methods includded in this plugin. Go to\nPlugins ‚Üí napari-clusters-plotter ‚Üí Clustering Widget\nSelect the K-MEANS method, impose 3 clusters and run. This will generate a new column with the cluster ID for each object.\n\n\n\nClustering methods\n\n\n\n\n\nK-MEANS\n\n\nNow going back to the Plotter Widget, we can select the UMAP axis AND the K-MEAN cluster IDs to automatically colorcode the objects belonging to different classes.\n\n\n\nAutomatic clustering",
    "crumbs": [
      "Practicals",
      "Practical Deep Learning",
      "Object-classification, dimensionality reduction and clustering"
    ]
  },
  {
    "objectID": "talks/talk_multiview_cartography.html",
    "href": "talks/talk_multiview_cartography.html",
    "title": "Multiview reconstruction and tissue cartography",
    "section": "",
    "text": "Multiview reconstruction and tissue cartography\n\nPrinciples of multiview acquisition\nOptimal settings\nExperimental optimization\nBigStitcher\nRegistration\nFusion/Deconvolution\nTissue cartography\nMaps\nSoftware tools",
    "crumbs": [
      "Talks",
      "Multiview reconstruction and tissue cartography"
    ]
  },
  {
    "objectID": "talks/talk_data_management.html",
    "href": "talks/talk_data_management.html",
    "title": "Best practices in microscopy data management",
    "section": "",
    "text": "Best practices in microscopy data management\n\nNumbered directories\nFile names\nREADMEs",
    "crumbs": [
      "Talks",
      "Best practices in microscopy data management"
    ]
  },
  {
    "objectID": "talks/talk_image_processing.html",
    "href": "talks/talk_image_processing.html",
    "title": "Image processing and analysis",
    "section": "",
    "text": "Image processing and analysis\n\nWhat is Bioimage Analysis?\nWhat is a workflow or a pipeline?\nDenoising\nBackground subtraction\nTheory of Macro programming and Napari assistant (mention checklist)\nSegmentation (only classical algorithms) and labeling (MC Found this presentation https://imagej.net/media/arganda-carreras-segmentation-bioimage-course-mdc-berlin-2016.pdf)\nMorphological operations?\nBIA ecosystem: open source tools, licensed tools, image.sc\nhttps://zenodo.org/records/16920518",
    "crumbs": [
      "Talks",
      "Image processing and analysis"
    ]
  },
  {
    "objectID": "talks/talk_lightsheet_concepts.html",
    "href": "talks/talk_lightsheet_concepts.html",
    "title": "Concepts in light-sheet microscopy",
    "section": "",
    "text": "Concepts in light-sheet microscopy\n\nConcept behind LSM in contrast with other imaging methods (Talk adapted from Sebastian Bundschuh, to be uploaded soon)",
    "crumbs": [
      "Talks",
      "Concepts in light-sheet microscopy"
    ]
  },
  {
    "objectID": "talks/talk_deep_learning.html",
    "href": "talks/talk_deep_learning.html",
    "title": "Deep learning",
    "section": "",
    "text": "Deep learning\n\nDeep Learning Theory\n\nCellpose (? Maybe AC) (Check trackmate for 3D CellPose)\n\nStardist\n\nPlantSeg (MC)\n\nBioImage Zoo (AC)\n\nCARE - noise2void (BV)",
    "crumbs": [
      "Talks",
      "Deep learning"
    ]
  }
]